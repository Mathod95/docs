---
title: "Mon √©t√© avec Cilium et EKS (Partie 1)"
date: 2026-01-18
categories:
  - "uncategorized"
tags:
  - "untagged"
source: "https://medium.com/@littel.jo/mon-%C3%A9t%C3%A9-avec-cilium-et-eks-partie-1-99a66ed6671f"
author:
  - "[[Joseph Ligier]]"
---
<!-- more -->

[Sitemap](https://medium.com/sitemap/sitemap.xml)

## Introduction

N ous allons voir Cilium sous toutes ces facettes avec EKS, comment il s‚Äôint√®gre √† l‚Äôenvironnement d‚ÄôAWS. Pour cette premi√®re partie, on va voir ce qu‚Äôest Cilium et EKS et comment installer rapidement Cilium sur un cluster EKS.

![](https://miro.medium.com/v2/resize:fit:640/format:webp/1*NaWuw7RC617aWuGrxmcC_g.jpeg)

Un √©t√© studieux

## Qu‚Äôest-ce qu‚ÄôAWS EKS et Cilium?

A WS EKS est un service g√©r√© par AWS. Il permet de cr√©er des clusters Kubernetes. Kubernetes est un orchestrateur de containers. J‚Äôai choisi ce service car je le connais bien et que √ßa permet de confronter cilium √† un ‚Äúvrai‚Äù cluster et non √† des clusters de dev comme kind ou minikube.

Cilium est un plugin r√©seau (ou CNI: Container Network Interface) pour Kubernetes. Ce projet est g√©r√© par la soci√©t√© Isovalent. Contrairement √† la plupart des autres CNI qui utilisent iptables/netfilter pour g√©rer les modifications r√©seaux, Cilium utilise l‚ÄôeBPF. Pour faire simple, l‚ÄôeBPF a beaucoup d‚Äôavantages sur iptables. Par exemple, la performance est meilleure, l‚Äôidentification des trames r√©seaux est plus simple qui permet ainsi d‚Äôavoir une meilleure observabilit√© du r√©seau. Ainsi Isovalent a cr√©√© un outil qui s‚Äôappelle Hubble permettant de visualiser les flux r√©seaux dans le r√©seau cilium √† l‚Äôinstar de tcpdump dans un mod√®le traditionnel.

Par d√©faut, AWS EKS utilise un autre plugin r√©seau AWS VPC CNI. Pourquoi utiliser Cilium plut√¥t que le plugin par d√©faut? Nous allons le voir pendant tout cet √©t√© mais la premi√®re chose qui me vient en t√™te est l‚Äôabsence de Network Policies, des r√®gles qui permettent de dire je veux que tel pod acc√®de √† tel pod via tel port. Cela n‚Äôest pas possible par exemple avec le CNI par d√©faut. Nous allons aussi explorer des limitations au niveau de Cilium. Tout n‚Äôest pas rose non plus.

Pour ce premier √©pisode, nous allons faire (tr√®s) simple: on va d√©ployer un cluster eks et installer cilium.

## Pr√©-requis

- un compte AWS avec des access keys
- un peu d‚Äôargent (Pour une heure: 0.1 $ pour le cluster eks, environ 0.08 $ pour deux t3.medium et 0.05 $ pour la nat gateway). Dur√©e minimale: environ 30 minutes
- eksctl: outil pour d√©ployer des clusters eks
- aws iam authenticator: outil pour s‚Äôauthentifier aupr√®s du cluster eks
- aws cli: outil pour communiquer avec l‚ÄôAPI d‚ÄôAWS
- kubectl: outil pour controler le cluster kubernetes
- cilium cli: outil pour installer cilium

Comme AWS EKS est un service payant, il est conseill√© d‚Äôavoir bien install√© les outils avant de cr√©er le cluster.

## D√©ploiement d‚Äôun cluster AWS EKS

Nous allons voir comment d√©ployer rapidement un cluster AWS EKS.

On va exporter les access keys:

```c
export AWS_DEFAULT_REGION=ch-ange-1
export AWS_ACCESS_KEY_ID="CHANGEME"
export AWS_SECRET_ACCESS_KEY="CHANGEME"
```

On va cr√©er un fichier yaml pour eksctl:

```c
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: basic-cilium
  region: us-east-1
  version: "1.27"

managedNodeGroups:
- name: ng-1
  instanceType: t3.medium
  # taint nodes so that application pods are
  # not scheduled/executed until Cilium is deployed.
  # Alternatively, see the note above regarding taint effects.
  taints:
   - key: "node.cilium.io/agent-not-ready"
     value: "true"
     effect: "NoExecute"
```

Je d√©ploie sur us-east-1 mais si vous pr√©f√©rez utilisez une autre r√©gion, n‚Äôh√©sitez pas √† changer.

On va lancer la commande suivante:

```c
eksctl create cluster -f ./files/eks-cilium.yaml
```

Cela va prendre un temps important (de l‚Äôordre de 15 minutes).

Voici ce que cette commande va cr√©er:

![](https://miro.medium.com/v2/resize:fit:640/format:webp/1*LvxOrlOsaNdSWvTJIXRGyw.png)

Un sch√©ma pour vous faire patienter

Une fois que c‚Äôest fini, on va pouvoir lancer des commandes kubectl:

```c
kubectl get node
NAME STATUS ROLES AGE VERSION
ip-192‚Äì168‚Äì11‚Äì135.ec2.internal Ready <none> 4m18s v1.27.1-eks-2f008fe
ip-192‚Äì168‚Äì56‚Äì129.ec2.internal Ready <none> 4m22s v1.27.1-eks-2f008fe
```

## Installation de Cilium

Rien de plus simple avec la cli cilium:

```c
cilium install
üîÆ Auto-detected Kubernetes kind: EKS
‚ÑπÔ∏è  Using Cilium version 1.13.3
üîÆ Auto-detected cluster name: basic-cilium-us-east-1-eksctl-io
üîÆ Auto-detected datapath mode: aws-eni
üîÆ Auto-detected kube-proxy has been installed
üî• Patching the "aws-node" DaemonSet to evict its pods...
‚ÑπÔ∏è  helm template --namespace kube-system cilium cilium/cilium --version 1.13.3 --set cluster.id=0,cluster.name=basic-cilium-us-east-1-eksctl-io,egressMasqueradeInterfaces=eth0,encryption.nodeEncryption=false,eni.enabled=true,ipam.mode=eni,kubeProxyReplacement=disabled,operator.replicas=1,serviceAccounts.cilium.name=cilium,serviceAccounts.operator.name=cilium-operator,tunnel=disabled
‚ÑπÔ∏è  Storing helm values file in kube-system/cilium-cli-helm-values Secret
üîë Created CA in secret cilium-ca
üîë Generating certificates for Hubble...
üöÄ Creating Service accounts...
üöÄ Creating Cluster roles...
üöÄ Creating ConfigMap for Cilium version 1.13.3...
üöÄ Creating Agent DaemonSet...
üöÄ Creating Operator Deployment...
‚åõ Waiting for Cilium to be installed and ready...
‚úÖ Cilium was successfully installed! Run 'cilium status' to view installation health
```

On voit que cilium d√©tecte pas mal de chose automatiquement, par exemple qu‚Äôil va installer sur EKS. Il va installer la version 1.13.3. Il va donc supprimer les pods qui composent l‚Äôaws vpc cni. Il va ensuite g√©n√©rer toutes les d√©pendances permettant d‚Äôinstaller cilium sur le cluster.

La commande *cilium status* va permettre d‚Äôavoir un aper√ßu si cela a bien √©t√© install√©:

```c
cilium status --wait
    /¬Ø¬Ø\
 /¬Ø¬Ø\__/¬Ø¬Ø\    Cilium:             OK
 \__/¬Ø¬Ø\__/    Operator:           OK
 /¬Ø¬Ø\__/¬Ø¬Ø\    Envoy DaemonSet:    disabled (using embedded mode)
 \__/¬Ø¬Ø\__/    Hubble Relay:       disabled
    \__/       ClusterMesh:        disabled

Deployment        cilium-operator    Desired: 1, Ready: 1/1, Available: 1/1
DaemonSet         cilium             Desired: 2, Ready: 2/2, Available: 2/2
Containers:       cilium-operator    Running: 1
                  cilium             Running: 2
Cluster Pods:     2/2 managed by Cilium
Image versions    cilium             quay.io/cilium/cilium:v1.13.3@sha256:77176464a1e11ea7e89e984ac7db365e7af39851507e94f137dcf56c87746314: 2
                  cilium-operator    quay.io/cilium/operator-aws:v1.13.3@sha256:394c40d156235d3c2004f77bb73402457092351cc6debdbc5727ba36fbd863ae: 1
```

On rajoute l‚Äôoption wait pour attendre que l‚Äôinstallation soit bien finie.

V√©rifions maintenant que l‚Äôinstallation s‚Äôest bien pass√©e:

```c
cilium connectivity test
```

La commande va faire plein de tests r√©seaux. Cela va donc prendre un temps important.

En exclusivit√© voici le r√©sum√© final du test:

```c
üìã Test Report
‚ùå 4/42 tests failed (20/300 actions), 12 tests skipped, 1 scenarios skipped:
Test [no-policies]:
  ‚ùå no-policies/pod-to-host/ping-ipv4-1: cilium-test/client-6965d549d5-hwmbt (192.168.61.62) -> 54.243.4.228 (54.243.4.228:0)
  ‚ùå no-policies/pod-to-host/ping-ipv4-3: cilium-test/client-6965d549d5-hwmbt (192.168.61.62) -> 54.158.35.146 (54.158.35.146:0)
  ‚ùå no-policies/pod-to-host/ping-ipv4-5: cilium-test/client2-76f4d7c5bc-5h4xr (192.168.63.171) -> 54.158.35.146 (54.158.35.146:0)
  ‚ùå no-policies/pod-to-host/ping-ipv4-7: cilium-test/client2-76f4d7c5bc-5h4xr (192.168.63.171) -> 54.243.4.228 (54.243.4.228:0)
Test [no-policies-extra]:
  ‚ùå no-policies-extra/pod-to-remote-nodeport/curl-0: cilium-test/client-6965d549d5-hwmbt (192.168.61.62) -> cilium-test/echo-same-node (echo-same-node:8080)
  ‚ùå no-policies-extra/pod-to-remote-nodeport/curl-1: cilium-test/client-6965d549d5-hwmbt (192.168.61.62) -> cilium-test/echo-other-node (echo-other-node:8080)
  ‚ùå no-policies-extra/pod-to-remote-nodeport/curl-2: cilium-test/client2-76f4d7c5bc-5h4xr (192.168.63.171) -> cilium-test/echo-other-node (echo-other-node:8080)
  ‚ùå no-policies-extra/pod-to-remote-nodeport/curl-3: cilium-test/client2-76f4d7c5bc-5h4xr (192.168.63.171) -> cilium-test/echo-same-node (echo-same-node:8080)
  ‚ùå no-policies-extra/pod-to-local-nodeport/curl-0: cilium-test/client-6965d549d5-hwmbt (192.168.61.62) -> cilium-test/echo-other-node (echo-other-node:8080)
  ‚ùå no-policies-extra/pod-to-local-nodeport/curl-1: cilium-test/client-6965d549d5-hwmbt (192.168.61.62) -> cilium-test/echo-same-node (echo-same-node:8080)
  ‚ùå no-policies-extra/pod-to-local-nodeport/curl-2: cilium-test/client2-76f4d7c5bc-5h4xr (192.168.63.171) -> cilium-test/echo-same-node (echo-same-node:8080)
  ‚ùå no-policies-extra/pod-to-local-nodeport/curl-3: cilium-test/client2-76f4d7c5bc-5h4xr (192.168.63.171) -> cilium-test/echo-other-node (echo-other-node:8080)
Test [allow-all-except-world]:
  ‚ùå allow-all-except-world/pod-to-host/ping-ipv4-1: cilium-test/client-6965d549d5-hwmbt (192.168.61.62) -> 54.158.35.146 (54.158.35.146:0)
  ‚ùå allow-all-except-world/pod-to-host/ping-ipv4-3: cilium-test/client-6965d549d5-hwmbt (192.168.61.62) -> 54.243.4.228 (54.243.4.228:0)
  ‚ùå allow-all-except-world/pod-to-host/ping-ipv4-5: cilium-test/client2-76f4d7c5bc-5h4xr (192.168.63.171) -> 54.158.35.146 (54.158.35.146:0)
  ‚ùå allow-all-except-world/pod-to-host/ping-ipv4-7: cilium-test/client2-76f4d7c5bc-5h4xr (192.168.63.171) -> 54.243.4.228 (54.243.4.228:0)
Test [host-entity]:
  ‚ùå host-entity/pod-to-host/ping-ipv4-1: cilium-test/client-6965d549d5-hwmbt (192.168.61.62) -> 54.158.35.146 (54.158.35.146:0)
  ‚ùå host-entity/pod-to-host/ping-ipv4-3: cilium-test/client-6965d549d5-hwmbt (192.168.61.62) -> 54.243.4.228 (54.243.4.228:0)
  ‚ùå host-entity/pod-to-host/ping-ipv4-5: cilium-test/client2-76f4d7c5bc-5h4xr (192.168.63.171) -> 54.158.35.146 (54.158.35.146:0)
  ‚ùå host-entity/pod-to-host/ping-ipv4-7: cilium-test/client2-76f4d7c5bc-5h4xr (192.168.63.171) -> 54.243.4.228 (54.243.4.228:0)
```

On voit d√©j√† que:

- le ping des pods vers les hosts ne fonctionne pas.
- le test des nodeports ne fonctionne pas

C‚Äôest d√©j√† pas mal, on n‚Äôa pas forc√©ment besoin du ping ni des nodeports.

Si on veut r√©soudre le probl√®me, il suffit d‚Äôouvrir le ping et les ports tcp des nodeports (30000‚Äì32767) au niveau du security group qui concernent les ec2. On aura alors:

```c
‚úÖ All 42 tests (300 actions) successful, 12 tests skipped, 1 scenarios skipped.
```

La premi√®re partie est finie. Vous pouvez voir cette installation r√©sum√©e ici: [https://github.com/littlejo/cilium-eks-cookbook/blob/main/install-cilium-eks.md](https://github.com/littlejo/cilium-eks-cookbook/blob/main/install-cilium-eks.md)

Dans la [prochaine partie](https://medium.com/@littel.jo/mon-%C3%A9t%C3%A9-avec-cilium-et-eks-partie-2-ea8ba7a9dcae), nous verrons ce qui est ‚Äúcach√©‚Äù dans l‚Äôinstallation avec la cli cilium avec l‚Äôinstallation avec helm.

## More from Joseph Ligier

## Recommended from Medium

[

See more recommendations

](https://medium.com/?source=post_page---read_next_recirc--99a66ed6671f---------------------------------------)