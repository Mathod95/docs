{"config":{"separator":"[\\s\\-_,:!=\\[\\]()\\\\\"`/]+|\\.(?!\\d)"},"items":[{"location":"","level":1,"title":"HOME","text":"<p>{%     include-markdown \"../README.md\"     start=\"\"     end=\"\" %}</p>","path":["HOME"],"tags":[]},{"location":"awesome/","level":1,"title":"","text":"<p>Uptime Kuma est un outil open source pour monitorer la disponibilit√© de sites et services. Auto-h√©berg√© et simple, il remplace facilement les services payants comme Uptime Robot. #Monitoring #OpenSource #DevOps</p> <p> </p> <p>Eza est un utilitaire CLI qui am√©liore la commande ls en affichant les fichiers et dossiers avec des couleurs et une mise en forme claire, pour une navigation terminal plus agr√©able. #CLI #Terminal #DevTools</p> <p> </p> <p>Backstage, cr√©√© par Spotify, est une plateforme open source centralisant documentation, outils et services pour simplifier la gestion des infrastructures et le travail des √©quipes dev √† grande √©chelle. #DevPlatform #OpenSource #DevOps</p> <p> </p> <p>Komoplane est une WebUI open source bas√©e sur Crossplane qui facilite la gestion multi-cloud via Kubernetes. Elle simplifie le d√©ploiement, la s√©curit√© et l‚Äôobservabilit√© des ressources cloud via une interface web intuitive. #Kubernetes #Cloud #OpenSource</p> <p> </p> <p>Flood est une interface web moderne pour contr√¥ler rTorrent, qBittorrent, Deluge ou Transmission. √âl√©gant, auto-h√©bergeable, et con√ßu pour g√©rer √† distance vos torrents. #Torrent #WebUI #OpenSource</p> <p> </p> <p>Rollout Extension est un plugin open source pour Argo CD qui facilite la gestion des d√©ploiements progressifs et des strat√©gies de mise √† jour avanc√©es, am√©liorant la fiabilit√© des livraisons continues. #Kubernetes #GitOps #DevOps</p> <p> </p> <p>ArgoCD Extension Installer est un outil open source qui simplifie l'installation et la gestion des extensions dans Argo CD, rendant la plateforme plus modulaire et personnalisable. #GitOps #DevOps #Kubernetes</p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p>","path":[""],"tags":[]},{"location":"awesome/#_2","level":2,"title":"","text":"","path":[""],"tags":[]},{"location":"contact/","level":1,"title":"ABOUT ME","text":"","path":["ABOUT ME"],"tags":[]},{"location":"glossaire/","level":1,"title":"Glossaire","text":"","path":["Glossaire"],"tags":[]},{"location":"glossaire/#a","level":2,"title":"A","text":"","path":["Glossaire"],"tags":[]},{"location":"glossaire/#alerting","level":3,"title":"<code>Alerting</code>","text":"L‚Äôalerting consiste √† configurer des notifications automatiques lorsqu‚Äôun √©v√©nement critique ou anormal survient dans le syst√®me. Cela permet aux √©quipes DevOps de r√©agir rapidement aux incidents et de maintenir la disponibilit√© et la performance des applications.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#argo","level":3,"title":"<code>Argo</code>","text":"Argo est un √©cosyst√®me d‚Äôoutils cloud-native pour Kubernetes, con√ßu pour automatiser les workflows, les d√©ploiements et la gestion op√©rationnelle. Il comprend Argo CD pour le GitOps, Argo Workflows pour orchestrer des pipelines, Argo Rollouts pour les d√©ploiements progressifs et Argo Events pour l‚Äôautomatisation pilot√©e par √©v√©nements. Argo s‚Äôint√®gre de mani√®re native avec Kubernetes, facilitant la mise en place de pipelines d√©claratifs et reproductibles.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#artifact","level":3,"title":"<code>Artifact</code>","text":"Un artifact est un fichier g√©n√©r√© par un pipeline CI/CD, tel qu‚Äôun binaire, une image Docker ou un bundle applicatif. Il repr√©sente une version stable du code qui peut √™tre d√©ploy√©e de mani√®re reproductible dans diff√©rents environnements.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#autoscaling-hpa-vpa","level":3,"title":"<code>Autoscaling (HPA / VPA)</code>","text":"L‚Äôautoscaling ajuste automatiquement le nombre de pods ou les ressources allou√©es en fonction de la charge du syst√®me. Le HPA (Horizontal Pod Autoscaler) ajuste le nombre de pods, tandis que le VPA (Vertical Pod Autoscaler) adapte les ressources CPU/m√©moire des pods existants pour optimiser la performance et l‚Äôutilisation des ressources.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#b","level":2,"title":"B","text":"","path":["Glossaire"],"tags":[]},{"location":"glossaire/#blue-green-deployment","level":3,"title":"<code>Blue-Green Deployment</code>","text":"Le blue-green deployment consiste √† maintenir deux environnements identiques, l‚Äôun actif et l‚Äôautre inactif. Lorsqu‚Äôune nouvelle version est pr√™te, le trafic est bascul√© vers l‚Äôenvironnement secondaire, permettant une mise √† jour sans interruption et une reprise rapide en cas de probl√®me.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#build","level":3,"title":"<code>Build</code>","text":"La phase de build transforme le code source en un livrable ex√©cutable ou d√©ployable. Elle inclut la compilation, la gestion des d√©pendances, les tests unitaires et l‚Äôanalyse statique, garantissant que l‚Äôartefact produit est stable et pr√™t pour les √©tapes suivantes du pipeline CI/CD.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#c","level":2,"title":"C","text":"","path":["Glossaire"],"tags":[]},{"location":"glossaire/#canary-release","level":3,"title":"<code>Canary Release</code>","text":"Un canary release d√©ploie progressivement une nouvelle version √† un sous-ensemble de trafic pour d√©tecter d‚Äô√©ventuels probl√®mes avant un d√©ploiement global. Cette strat√©gie r√©duit les risques pour les utilisateurs finaux et permet de valider les changements en production.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#cd-continuous-delivery-continuous-deployment","level":3,"title":"<code>CD (Continuous Delivery / Continuous Deployment)</code>","text":"La CD automatise le d√©ploiement du code vers la pr√©-production ou la production. Elle garantit que les modifications valid√©es dans le pipeline CI peuvent √™tre mises en production rapidement et de mani√®re fiable, r√©duisant le temps entre d√©veloppement et livraison.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#ci-continuous-integration","level":3,"title":"<code>CI (Continuous Integration)</code>","text":"La CI consiste √† int√©grer et tester automatiquement le code √† chaque commit. Elle permet de d√©tecter rapidement les erreurs et de maintenir un code fonctionnel √† tout moment dans le pipeline.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#cli","level":3,"title":"<code>CLI</code>","text":"La CLI (Command Line Interface) permet d‚Äôinteragir avec un logiciel ou un syst√®me via des commandes textuelles. Elle est essentielle pour l‚Äôautomatisation, la configuration et le contr√¥le pr√©cis des op√©rations dans un environnement DevOps.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#cni","level":3,"title":"<code>CNI</code>","text":"La Container Network Interface (CNI) est un standard permettant aux conteneurs de se connecter √† un r√©seau. Kubernetes utilise des plugins CNI comme Cilium, Calico ou Flannel pour g√©rer la communication entre les pods et assurer un r√©seau s√©curis√© et fiable.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#configmap","level":3,"title":"<code>ConfigMap</code>","text":"Un ConfigMap est un objet Kubernetes qui stocke des donn√©es de configuration non sensibles (fichiers, variables d‚Äôenvironnement, param√®tres). Il permet de s√©parer la configuration du code et de rendre les applications plus modulaires et flexibles.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#container-registry","level":3,"title":"<code>Container registry</code>","text":"Un container registry est une plateforme pour stocker, g√©rer et distribuer des images de conteneurs. Des exemples incluent Docker Hub, GHCR, ECR, GCR et Harbor, utilis√©s pour versionner et partager des images entre environnements et clusters.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#controller-manager","level":3,"title":"<code>Controller Manager</code>","text":"Le Controller Manager est un composant Kubernetes qui supervise diff√©rents contr√¥leurs. Il veille √† ce que l‚Äô√©tat r√©el du cluster corresponde √† l‚Äô√©tat d√©sir√© d√©fini par les manifests, en effectuant les ajustements n√©cessaires sur les ressources.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#cronjob","level":3,"title":"<code>CronJob</code>","text":"Un CronJob permet d‚Äôex√©cuter des t√¢ches planifi√©es automatiquement selon une syntaxe cron. Il est utilis√© pour des op√©rations r√©currentes telles que les sauvegardes, les rapports ou les traitements batch.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#crd","level":3,"title":"<code>CRD</code>","text":"Une Custom Resource Definition (CRD) permet d‚Äôajouter de nouveaux types de ressources personnalis√©es dans Kubernetes. Les CRD sont √† la base des op√©rateurs et permettent d‚Äô√©tendre les fonctionnalit√©s du cluster sans modifier son c≈ìur.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#d","level":2,"title":"D","text":"","path":["Glossaire"],"tags":[]},{"location":"glossaire/#daemonset","level":3,"title":"<code>DaemonSet</code>","text":"Un DaemonSet assure qu‚Äôun pod sp√©cifique est ex√©cut√© sur chaque n≈ìud du cluster. Il est utilis√© pour d√©ployer des agents syst√®me, des collecteurs de logs ou des plugins r√©seau sur tous les n≈ìuds.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#deployment","level":3,"title":"<code>Deployment</code>","text":"Un Deployment g√®re le cycle de vie des pods en orchestrant les ReplicaSets et en permettant les mises √† jour progressives. Il facilite les d√©ploiements, les rollbacks et le maintien de l‚Äô√©tat souhait√© d‚Äôune application.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#e","level":2,"title":"E","text":"","path":["Glossaire"],"tags":[]},{"location":"glossaire/#entrypoint-cmd","level":3,"title":"<code>Entrypoint / CMD</code>","text":"L‚ÄôEntrypoint ou CMD d√©finit la commande principale ex√©cut√©e automatiquement lorsqu‚Äôun conteneur d√©marre. Il permet de contr√¥ler le comportement initial d‚Äôune application conteneuris√©e.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#f","level":2,"title":"F","text":"","path":["Glossaire"],"tags":[]},{"location":"glossaire/#failover","level":3,"title":"<code>Failover</code>","text":"Le failover bascule automatiquement vers un syst√®me de secours lorsqu‚Äôun composant tombe en panne. Cela assure la continuit√© de service et r√©duit l‚Äôimpact des incidents sur les utilisateurs.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#flannel","level":3,"title":"<code>Flannel</code>","text":"Flannel est un plugin CNI qui fournit un r√©seau overlay pour les pods Kubernetes. Il est simple √† configurer et convient aux clusters de petite taille ou aux environnements de test.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#fluxcd","level":3,"title":"<code>FluxCD</code>","text":"FluxCD est un outil GitOps open source qui synchronise automatiquement l‚Äô√©tat d‚Äôun cluster Kubernetes avec un d√©p√¥t Git. Il applique les changements de mani√®re d√©clarative et assure une gestion coh√©rente de l‚Äôinfrastructure.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#g","level":2,"title":"G","text":"","path":["Glossaire"],"tags":[]},{"location":"glossaire/#gitops","level":3,"title":"<code>GitOps</code>","text":"GitOps est une approche o√π l‚Äô√©tat de l‚Äôinfrastructure est d√©fini dans Git et appliqu√© automatiquement aux clusters. Cette m√©thode permet un d√©ploiement fiable, tra√ßable et reproductible des applications et des configurations.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#h","level":2,"title":"H","text":"","path":["Glossaire"],"tags":[]},{"location":"glossaire/#helm","level":3,"title":"<code>Helm</code>","text":"Helm est le gestionnaire de packages pour Kubernetes, utilisant des charts regroupant mod√®les YAML, valeurs par d√©faut et d√©pendances. Il simplifie le d√©ploiement, la configuration et la mise √† jour d‚Äôapplications complexes, tout en facilitant la gestion multi-environnements et le versioning des d√©ploiements.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#high-availability-ha","level":3,"title":"<code>High Availability (HA)</code>","text":"L‚ÄôHA d√©signe une architecture con√ßue pour garantir la disponibilit√© maximale d‚Äôun service et la tol√©rance aux pannes. Elle repose sur la redondance des composants, la r√©plication des donn√©es et des m√©canismes de basculement automatique.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#i","level":2,"title":"I","text":"","path":["Glossaire"],"tags":[]},{"location":"glossaire/#immutable-infrastructure","level":3,"title":"<code>Immutable Infrastructure</code>","text":"L‚Äôinfrastructure immuable est une approche o√π les serveurs ou conteneurs existants ne sont jamais modifi√©s. Toute mise √† jour consiste √† d√©ployer une nouvelle version, garantissant la coh√©rence et la pr√©visibilit√© du syst√®me.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#ingress","level":3,"title":"<code>Ingress</code>","text":"Un Ingress d√©finit les r√®gles de routage HTTP/HTTPS pour exposer des services Kubernetes √† l‚Äôext√©rieur. Il centralise le contr√¥le du trafic et peut inclure des fonctionnalit√©s de r√©√©criture d‚ÄôURL, SSL et redirections.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#ingress-controller","level":3,"title":"<code>Ingress Controller</code>","text":"L‚ÄôIngress Controller applique les r√®gles d‚ÄôIngress dans le cluster. Des solutions comme NGINX, Traefik ou Kong assurent la gestion du trafic entrant et la s√©curit√© des services expos√©s.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#j","level":2,"title":"J","text":"","path":["Glossaire"],"tags":[]},{"location":"glossaire/#job","level":3,"title":"<code>Job</code>","text":"Un Job ex√©cute une t√¢che unique ou r√©p√©t√©e jusqu‚Äô√† sa compl√©tion. Il est utilis√© pour les migrations, traitements batch ou autres t√¢ches ponctuelles n√©cessitant un contr√¥le pr√©cis.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#k","level":2,"title":"K","text":"","path":["Glossaire"],"tags":[]},{"location":"glossaire/#kind","level":3,"title":"<code>Kind</code>","text":"Kind (Kubernetes IN Docker) permet de cr√©er des clusters Kubernetes locaux dans des conteneurs Docker. Il est particuli√®rement utile pour le d√©veloppement, les tests CI/CD et l‚Äôexp√©rimentation sans d√©ployer de cluster complet sur le cloud.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#l","level":2,"title":"L","text":"","path":["Glossaire"],"tags":[]},{"location":"glossaire/#liveness-probe","level":3,"title":"<code>Liveness Probe</code>","text":"La Liveness Probe v√©rifie p√©riodiquement qu‚Äôun conteneur est en bon √©tat de fonctionnement. Si le contr√¥le √©choue, Kubernetes red√©marre le pod pour restaurer un √©tat sain.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#load-balancer","level":3,"title":"<code>Load balancer</code>","text":"Un load balancer r√©partit le trafic r√©seau entre plusieurs instances d‚Äôune application. Il am√©liore la performance, la disponibilit√© et la tol√©rance aux pannes.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#load-balancing","level":3,"title":"<code>Load Balancing</code>","text":"Le load balancing consiste √† distribuer le trafic ou les demandes utilisateur sur plusieurs serveurs pour optimiser les performances et √©viter les surcharges.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#logging","level":3,"title":"<code>Logging</code>","text":"Le logging centralise la collecte et le stockage des journaux applicatifs et syst√®mes. Il permet d‚Äôanalyser le comportement du syst√®me, d√©tecter les erreurs et assurer la tra√ßabilit√© des √©v√©nements.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#m","level":2,"title":"M","text":"","path":["Glossaire"],"tags":[]},{"location":"glossaire/#metrics","level":3,"title":"<code>Metrics</code>","text":"Les metrics sont des mesures quantitatives sur les ressources et performances d‚Äôun syst√®me (CPU, RAM, latence, erreurs). Elles servent √† surveiller, diagnostiquer et optimiser les applications et l‚Äôinfrastructure.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#n","level":2,"title":"N","text":"","path":["Glossaire"],"tags":[]},{"location":"glossaire/#n8n","level":3,"title":"<code>N8N</code>","text":"N8N est un outil open source d‚Äôautomatisation de workflows permettant de connecter des applications et services. Il offre une interface visuelle pour cr√©er des pipelines et automatiser des processus sans √©crire beaucoup de code.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#namespace","level":3,"title":"<code>Namespace</code>","text":"Un namespace isole les ressources au sein d‚Äôun cluster Kubernetes. Il facilite l‚Äôorganisation, la gestion des permissions, l‚Äôapplication de quotas et la s√©paration des environnements ou projets.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#network-policy","level":3,"title":"<code>Network Policy</code>","text":"Les Network Policies d√©finissent les r√®gles de communication entre pods dans un cluster Kubernetes. Elles permettent de s√©curiser le r√©seau interne en contr√¥lant quel pod peut parler √† quel service.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#node","level":3,"title":"<code>Node</code>","text":"Un node est une machine physique ou virtuelle ex√©cutant des pods Kubernetes. Chaque node fournit les ressources n√©cessaires pour faire tourner les conteneurs et communiquer avec le cluster.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#o","level":2,"title":"O","text":"","path":["Glossaire"],"tags":[]},{"location":"glossaire/#observability","level":3,"title":"<code>Observability</code>","text":"L‚Äôobservabilit√© regroupe les pratiques permettant de comprendre l‚Äô√©tat interne d‚Äôun syst√®me √† partir des logs, metrics et traces distribu√©es. Elle est essentielle pour diagnostiquer les probl√®mes et optimiser les performances.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#oci-open-container-initiative","level":3,"title":"<code>OCI (Open Container Initiative)</code>","text":"OCI d√©finit des standards pour les formats d‚Äôimages et les runtimes de conteneurs, assurant l‚Äôinterop√©rabilit√© entre outils comme Docker, containerd ou Podman.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#operator","level":3,"title":"<code>Operator</code>","text":"Un Operator est un composant Kubernetes qui automatise la gestion d‚Äôapplications complexes √† l‚Äôaide de CRD et de contr√¥leurs personnalis√©s, permettant de g√©rer le cycle de vie complet d‚Äôune application.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#p","level":2,"title":"P","text":"","path":["Glossaire"],"tags":[]},{"location":"glossaire/#persistentvolume-pv","level":3,"title":"<code>PersistentVolume (PV)</code>","text":"Un PV est une ressource de stockage durable dans Kubernetes, ind√©pendante du cycle de vie des pods, utilis√©e pour conserver les donn√©es de mani√®re persistante.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#persistentvolumeclaim-pvc","level":3,"title":"<code>PersistentVolumeClaim (PVC)</code>","text":"Un PVC est une requ√™te faite par un pod pour utiliser un volume persistant. Il permet de lier dynamiquement ou statiquement un stockage √† un pod selon ses besoins.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#pipeline-cicd","level":3,"title":"<code>Pipeline CI/CD</code>","text":"Un pipeline CI/CD est une suite d‚Äô√©tapes automatis√©es incluant tests, build, analyse et d√©ploiement, permettant de livrer du code de mani√®re fiable et reproductible.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#pod","level":3,"title":"<code>Pod</code>","text":"Un pod est l‚Äôunit√© de base d‚Äôex√©cution dans Kubernetes, pouvant contenir un ou plusieurs conteneurs partageant r√©seau et stockage. Les pods sont g√©r√©s par des contr√¥leurs tels que Deployment ou StatefulSet.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#q","level":2,"title":"Q","text":"","path":["Glossaire"],"tags":[]},{"location":"glossaire/#qos-quality-of-service-class","level":3,"title":"<code>QoS (Quality of Service Class)</code>","text":"QoS classe les pods selon leur priorit√© et leurs demandes/limites de ressources (Guaranteed, Burstable, BestEffort), influen√ßant la planification et la tol√©rance aux ressources du cluster.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#quota","level":3,"title":"<code>Quota</code>","text":"Un quota limite l‚Äôusage des ressources (CPU, m√©moire, stockage, objets) dans un namespace, garantissant un partage √©quitable et la stabilit√© du cluster.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#r","level":2,"title":"R","text":"","path":["Glossaire"],"tags":[]},{"location":"glossaire/#readiness-probe","level":3,"title":"<code>Readiness Probe</code>","text":"La Readiness Probe indique si un pod est pr√™t √† recevoir du trafic. Si le pod n‚Äôest pas pr√™t, il est exclu des services expos√©s jusqu‚Äô√† ce qu‚Äôil le devienne.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#replicaset","level":3,"title":"<code>ReplicaSet</code>","text":"Un ReplicaSet garantit qu‚Äôun nombre d√©fini de pods identiques est toujours en fonctionnement. Il est souvent g√©r√© indirectement via des Deployments pour les mises √† jour progressives.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#rolling-update","level":3,"title":"<code>Rolling Update</code>","text":"Le rolling update met √† jour les pods progressivement, un par un, pour limiter les interruptions et maintenir la disponibilit√© du service pendant la mise √† jour.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#s","level":2,"title":"S","text":"","path":["Glossaire"],"tags":[]},{"location":"glossaire/#secret","level":3,"title":"<code>Secret</code>","text":"Un Secret stocke des donn√©es sensibles encod√©es, comme des tokens ou mots de passe, permettant de s√©curiser la configuration et l‚Äôacc√®s aux services.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#service-clusterip-nodeport-loadbalancer","level":3,"title":"<code>Service (ClusterIP / NodePort / LoadBalancer)</code>","text":"Un Service expose un ou plusieurs pods via une IP stable ou vers l‚Äôext√©rieur, offrant un point d‚Äôacc√®s unique pour la communication r√©seau.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#service-mesh","level":3,"title":"<code>Service Mesh</code>","text":"Un service mesh g√®re la communication entre services dans un cluster, offrant s√©curit√©, routage, observabilit√© et r√©silience, avec des outils comme Istio ou Linkerd.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#sailorsh","level":3,"title":"<code>sailor.sh</code>","text":"Sailor.sh est un simulateur en ligne pour pratiquer les examens Kubernetes (CKA, CKAD, CKS), avec des laboratoires et sc√©narios r√©alistes pour s‚Äôentra√Æner aux comp√©tences pratiques.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#scheduler","level":3,"title":"<code>Scheduler</code>","text":"Le scheduler d√©cide sur quel node chaque pod doit √™tre plac√©, en fonction des ressources disponibles, des contraintes et des politiques d√©finies.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#sli","level":3,"title":"<code>SLI</code>","text":"Un SLI (Service Level Indicator) mesure la performance r√©elle d‚Äôun service par rapport √† un objectif d√©fini, comme le temps de r√©ponse ou le taux d‚Äôerreur.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#slo","level":3,"title":"<code>SLO</code>","text":"Un SLO (Service Level Objective) d√©finit un objectif de performance mesurable pour un service, utilis√© pour √©valuer le respect d‚Äôun SLA.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#sla","level":3,"title":"<code>SLA</code>","text":"Un SLA (Service Level Agreement) est un contrat d√©finissant les niveaux de service attendus entre un fournisseur et un client.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#statefulset","level":3,"title":"<code>StatefulSet</code>","text":"Un StatefulSet g√®re les pods n√©cessitant une identit√© stable, du stockage persistant ou un ordre de d√©ploiement sp√©cifique.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#storageclass","level":3,"title":"<code>StorageClass</code>","text":"Une StorageClass d√©finit la mani√®re dont un PersistentVolume doit √™tre provisionn√©, y compris le type de disque et les performances attendues.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#t","level":2,"title":"T","text":"","path":["Glossaire"],"tags":[]},{"location":"glossaire/#tracing-distributed-tracing","level":3,"title":"<code>Tracing / Distributed Tracing</code>","text":"Le tracing distribu√© suit les requ√™tes √† travers diff√©rents services et composants, permettant de comprendre les interactions et identifier les goulots d‚Äô√©tranglement.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#u","level":2,"title":"U","text":"","path":["Glossaire"],"tags":[]},{"location":"glossaire/#upstream","level":3,"title":"<code>Upstream</code>","text":"L‚Äôupstream d√©signe la source principale d‚Äôun projet open source, souvent utilis√©e pour distinguer les d√©p√¥ts officiels des forks ou d√©riv√©s.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#undeploy","level":3,"title":"<code>Undeploy</code>","text":"Undeploy consiste √† retirer proprement une application d√©ploy√©e, en supprimant ses ressources Kubernetes et l‚Äôinfrastructure associ√©e.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#v","level":2,"title":"V","text":"","path":["Glossaire"],"tags":[]},{"location":"glossaire/#volume","level":3,"title":"<code>Volume</code>","text":"Un volume est un espace de stockage attach√© √† un pod pour conserver les donn√©es, √©ph√©m√®res ou persistantes, selon le type de volume.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#w","level":2,"title":"W","text":"","path":["Glossaire"],"tags":[]},{"location":"glossaire/#webhook","level":3,"title":"<code>Webhook</code>","text":"Un webhook est un m√©canisme qui permet √† un service d‚Äôenvoyer automatiquement une requ√™te HTTP lorsqu‚Äôun √©v√©nement survient, par exemple pour d√©clencher un pipeline CI/CD.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#workload","level":3,"title":"<code>Workload</code>","text":"Un workload d√©signe toute ressource responsable d‚Äôex√©cuter des applications dans Kubernetes, comme Deployment, Job, DaemonSet ou StatefulSet.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#wasm-webassembly","level":3,"title":"<code>WASM (WebAssembly)</code>","text":"Le WebAssembly est un format bytecode performant permettant d‚Äôex√©cuter des applications sandbox√©es. Il commence √† √™tre utilis√© dans Kubernetes pour des workloads l√©gers et s√©curis√©s.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#x","level":2,"title":"X","text":"","path":["Glossaire"],"tags":[]},{"location":"glossaire/#x509","level":3,"title":"<code>X.509</code>","text":"X.509 est un standard de certificats utilis√©s pour chiffrer et authentifier les communications, notamment pour TLS/mTLS et les API Kubernetes.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#xp-extreme-programming","level":3,"title":"<code>XP (Extreme Programming)</code>","text":"XP est une m√©thodologie Agile centr√©e sur l‚Äôam√©lioration continue, la qualit√© du code et la r√©duction du cycle de feedback gr√¢ce √† des pratiques comme le TDD ou le pair programming.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#y","level":2,"title":"Y","text":"","path":["Glossaire"],"tags":[]},{"location":"glossaire/#yaml","level":3,"title":"<code>YAML</code>","text":"YAML est un format de fichier structur√© tr√®s utilis√© dans Kubernetes pour d√©clarer les ressources de mani√®re hi√©rarchique et lisible par l‚Äôhomme.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#ytt-yaml-templating-tool","level":3,"title":"<code>YTT (YAML Templating Tool)</code>","text":"YTT est un outil de templating pour YAML qui permet de g√©n√©rer ou modifier dynamiquement des manifests Kubernetes, facilitant la r√©utilisation et la personnalisation des configurations.","path":["Glossaire"],"tags":[]},{"location":"glossaire/#z","level":2,"title":"Z","text":"","path":["Glossaire"],"tags":[]},{"location":"glossaire/#zsh","level":3,"title":"<code>ZSH</code>","text":"ZSH est un shell Unix puissant et personnalisable, alternatif √† Bash, souvent utilis√© avec des frameworks comme Oh My Zsh pour am√©liorer la productivit√© et la configuration du terminal.","path":["Glossaire"],"tags":[]},{"location":"syntax/","level":1,"title":"Syntax","text":"","path":["Syntax"],"tags":[]},{"location":"syntax/#header","level":2,"title":"Header","text":"<p>Example</p> Code<pre><code>---\ntitle: Titre de ton article\ndescription: \"Courte description pour l‚Äôaper√ßu et le SEO\"\ndate: 2025-02-10\ncategories:\n  - Documentation\ntags:\n  - template\nstatus:\nsources:\nhide:\n  - tags\n---\n</code></pre>","path":["Syntax"],"tags":[]},{"location":"syntax/#code-annotate","level":2,"title":"Code Annotate","text":"<p>Example</p> Visual<pre><code>theme:\n  features:\n    - content.code.annotate # (1)\n</code></pre> <ol> <li>:man_raising_hand: I'm a code annotation! I can contain <code>code</code>, formatted     text, images, ... basically anything that can be written in Markdown.</li> </ol> Code<pre><code>```yaml\ntheme:\n  features:\n    - content.code.annotate # (1)\n```\n\n1.  :man_raising_hand: I'm a code annotation! I can contain `code`, __formatted\n    text__, images, ... basically anything that can be written in Markdown.\n</code></pre>","path":["Syntax"],"tags":[]},{"location":"syntax/#footnotes","level":2,"title":"Footnotes","text":"<p>Example</p> <p>https://zensical.org/docs/authoring/footnotes/</p> <pre><code>[^1]: Lorem ipsum dolor sit amet, consectetur adipiscing elit.\n[^2]:\n    Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod\n    nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor\n    massa, nec semper lorem quam in massa.\n</code></pre>","path":["Syntax"],"tags":[]},{"location":"syntax/#emotes","level":2,"title":"Emotes","text":"<p>Example</p> <pre><code>üìã Liste\n‚úÖ Succ√®s/OK\n‚ùå Erreur/√âchec\n‚ö†Ô∏è Attention\n‚ÑπÔ∏è Information\n‚è≠Ô∏è Ignor√©/Skip\nüîÑ En cours\n‚è∏Ô∏è Pause\n‚ñ∂Ô∏è Play/Start\n‚ûï Ajouter\n‚ûñ Retirer\nüîß Configuration\nüî® Build\nüöÄ D√©ployer\nüì¶ Package\nüóëÔ∏è Supprimer\nüìù Documenter\nüìä Stats\nüìà Augmentation\nüìâ Diminution\nüéØ Objectif\nüí° Id√©e\nüîç Recherche\n</code></pre>","path":["Syntax"],"tags":[]},{"location":"syntax/#admonitions","level":2,"title":"Admonitions","text":"<p>Example</p> <p>Note</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <p>Abstract</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <p>Info</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <p>Tip</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <p>Success</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <p>Question</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <p>Warning</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <p>Failure</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <p>Danger</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <p>Bug</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <p>Example</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <p>Quote</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p>","path":["Syntax"],"tags":[]},{"location":"syntax/#tabbed-admonition","level":2,"title":"Tabbed Admonition","text":"<p>Example</p> Tab1Tab2 <p>Note for Python</p> <p>This is a note inside the Python tab.</p> <p>Warning for JavaScript</p> <p>This is a warning inside the JavaScript tab.</p> <p>Parent Admonition</p> <p>This is the outer admonition content.</p> Tab 1Tab 2 <p>Tip inside Tab 1</p> <p>This is the first tab's inner admonition.</p> <p>Warning inside Tab 2</p> <p>This is the second tab's inner admonition.</p> <pre><code>=== \"Tab1\"\n\n    !!! note \"Note for Python\"\n        This is a note inside the Python tab.\n\n=== \"Tab2\"\n\n    !!! warning \"Warning for JavaScript\"\n        This is a warning inside the JavaScript tab.\n\n!!! note \"Parent Admonition\"\n\n    This is the outer admonition content.\n\n    === \"Tab 1\"\n\n        !!! tip \"Tip inside Tab 1\"\n            This is the first tab's inner admonition.\n\n    === \"Tab 2\"\n\n        !!! warning \"Warning inside Tab 2\"\n            This is the second tab's inner admonition.\n</code></pre>","path":["Syntax"],"tags":[]},{"location":"syntax/#variables","level":2,"title":"Variables","text":"<pre><code>---\nuser-defined-values:\n  - YOUR_APPNAME\n  - YOUR_ENV\n---\n</code></pre> <p>Example</p> <p>{{{user-defined-values}}}</p> <pre><code>---\napiVersion: v1 \nkind: Pod \nmetadata: \n  name: APPNAME-pod \n  namespace: NAMESPACE\n  labels: \n    app: APPNAME\n    env: ENVIRONMENT\n  annotations: \n    aws-account: \"ACCOUNT_ID\"\n    maintainer: \"dev@mathod.io\"\nspec: \n  containers: \n  - name: nginx-container\n    image: nginx\n</code></pre> File Path on Saltbox Host <p>Never Edit These Files</p> <p>Updates will overwrite your changes. Use the inventory system instead.</p> <pre><code>/srv/git/saltbox/roles/&lt;role_name&gt;/defaults/main.yml\n</code></pre> <pre><code>/opt/sandbox/roles/&lt;role_name&gt;/defaults/main.yml\n</code></pre> <pre><code>/srv/git/saltbox/inventories/group_vars/all.yml\n</code></pre> <p>Never Edit These Files</p> <p>Updates will banane your changes. Use the inventory system instead.</p> <p>To determine which apps are included in Authelia by default, you can run this command or a similar one:</p> <pre><code>grep -Ril '_traefik_sso_middleware: \"{{ traefik_default_sso_middleware }}\"' /srv/git/saltbox/roles /opt/sandbox/roles | awk 'BEGIN{RS=\"roles/\"; FS=\"/defaults\"}NF&gt;1{print $1}' | sort -u\n</code></pre>","path":["Syntax"],"tags":[]},{"location":"tags/","level":1,"title":"Tags","text":"<p>Following is a list of relevant tags:</p>","path":["Tags"],"tags":[]},{"location":"argo/","level":1,"title":"KODEKLOUD","text":"","path":["ARGO","Introduction to Argo"],"tags":["Argo","Argo CD","Argo Workflows","Argo Events","Argo Rollouts"]},{"location":"argo/#gitops","level":2,"title":"GITOPS","text":"","path":["ARGO","Introduction to Argo"],"tags":["Argo","Argo CD","Argo Workflows","Argo Events","Argo Rollouts"]},{"location":"argo/#introduction","level":3,"title":"Introduction:","text":"<ul> <li>What is GitOps</li> <li>GitOps Workflow</li> <li>GitOps Principles</li> <li>DevOps vs GitOps</li> <li>Push vs Pull</li> <li>GitOps Feature Set</li> <li>GitOps Benefits Drawbacks</li> <li>GitOps Projects Tools</li> </ul>","path":["ARGO","Introduction to Argo"],"tags":["Argo","Argo CD","Argo Workflows","Argo Events","Argo Rollouts"]},{"location":"argo/#argo","level":2,"title":"ARGO","text":"","path":["ARGO","Introduction to Argo"],"tags":["Argo","Argo CD","Argo Workflows","Argo Events","Argo Rollouts"]},{"location":"argo/#argocd-basics","level":3,"title":"ArgoCD Basics:","text":"<ul> <li>WhatWhyHow ArgoCD</li> <li>ConceptsTerminology</li> <li>Features</li> <li>Architecture</li> <li>Installation Options</li> <li>ArgoCD Installation</li> <li>ArgoCD App projects</li> <li>Create Application using UI</li> <li>Create Application using CLI</li> <li>Create ArgoCD Project</li> </ul>","path":["ARGO","Introduction to Argo"],"tags":["Argo","Argo CD","Argo Workflows","Argo Events","Argo Rollouts"]},{"location":"argo/#argocd-intermediate","level":3,"title":"ArgoCD Intermediate:","text":"<ul> <li>Reconciliation loop</li> <li>Git Webhook Configuration</li> <li>Application health</li> <li>Application Custom Health Check</li> <li>Types of Sync Strategies</li> <li>Application Synchronization Options</li> <li>Declarative Setup</li> <li>Declarative Setup Mono Application</li> <li>App of Apps</li> <li>Declarative Setup App of Apps</li> <li>Deploy apps using HELM Chart</li> <li>Deploy apps using HELM Chart</li> <li>Multi cluster application deployment</li> <li>Multi cluster application deployment</li> </ul>","path":["ARGO","Introduction to Argo"],"tags":["Argo","Argo CD","Argo Workflows","Argo Events","Argo Rollouts"]},{"location":"argo/#argocd-advancedadmin","level":3,"title":"ArgoCD AdvancedAdmin:","text":"<ul> <li>User Management RBAC ArgoCD</li> <li>ArgoCD User Management RBAC</li> <li>Dex Okta Connector</li> <li>Dex Okta Connector</li> <li>Bitnami Sealed Secrets</li> <li>Bitnami Sealed Secrets</li> <li>Hashicorp Vault</li> <li>Hashicorp Vault 2</li> <li>ArgoCD Vault Plugin CLI</li> <li>ArgoCD Vault Plugin with ArgoCD</li> <li>ArgoCD Metrics Monitoring</li> <li>ArgoCD Metrics Monitoring 2</li> <li>Monitoring through Prometheus Grafana</li> <li>Raise Alert using AlertManager</li> <li>ArgoCD Notifications</li> <li>ArgoCD Notifications with Slack</li> </ul>","path":["ARGO","Introduction to Argo"],"tags":["Argo","Argo CD","Argo Workflows","Argo Events","Argo Rollouts"]},{"location":"argo/#argocd-with-jenkins-ci-pipeline","level":3,"title":"ArgoCD with Jenkins CI Pipeline:","text":"<ul> <li>CICD with GitOps</li> <li>Git Reposiories Dockerfile and Application Walkthrough</li> <li>Jenkinsfile Walkthrough</li> <li>CICD Pipeline Demo</li> </ul>","path":["ARGO","Introduction to Argo"],"tags":["Argo","Argo CD","Argo Workflows","Argo Events","Argo Rollouts"]},{"location":"argo/#cncf","level":1,"title":"CNCF","text":"","path":["ARGO","Introduction to Argo"],"tags":["Argo","Argo CD","Argo Workflows","Argo Events","Argo Rollouts"]},{"location":"argo/#argo_1","level":2,"title":"ARGO","text":"","path":["ARGO","Introduction to Argo"],"tags":["Argo","Argo CD","Argo Workflows","Argo Events","Argo Rollouts"]},{"location":"argo/#introduction_1","level":3,"title":"Introduction","text":"<ul> <li>Chapter Overview and Objectives</li> </ul>","path":["ARGO","Introduction to Argo"],"tags":["Argo","Argo CD","Argo Workflows","Argo Events","Argo Rollouts"]},{"location":"argo/#essential-concepts-for-argo","level":3,"title":"Essential Concepts for Argo","text":"<ul> <li>What Is GitOps?</li> </ul>","path":["ARGO","Introduction to Argo"],"tags":["Argo","Argo CD","Argo Workflows","Argo Events","Argo Rollouts"]},{"location":"argo/#argo-overview","level":3,"title":"Argo Overview","text":"<ul> <li>What Is Argo?</li> <li>Argo Continuous Delivery (CD)</li> <li>Argo Workflows</li> <li>Argo Events</li> <li>Argo Rollouts</li> <li>Benefits of Using Argo</li> </ul>","path":["ARGO","Introduction to Argo"],"tags":["Argo","Argo CD","Argo Workflows","Argo Events","Argo Rollouts"]},{"location":"argo/#benefits-of-using-argo","level":3,"title":"Benefits of Using Argo","text":"<ul> <li>Step-by-Step: Deploying Kubernetes for Argo</li> <li>Installing Docker</li> <li>Installing kubectl</li> <li>Installing kind and Creating a Cluster</li> </ul>","path":["ARGO","Introduction to Argo"],"tags":["Argo","Argo CD","Argo Workflows","Argo Events","Argo Rollouts"]},{"location":"argo/#argo-cd","level":2,"title":"ARGO CD","text":"","path":["ARGO","Introduction to Argo"],"tags":["Argo","Argo CD","Argo Workflows","Argo Events","Argo Rollouts"]},{"location":"argo/#introduction_2","level":3,"title":"Introduction","text":"<ul> <li>Chapter Overview and Objectives</li> </ul>","path":["ARGO","Introduction to Argo"],"tags":["Argo","Argo CD","Argo Workflows","Argo Events","Argo Rollouts"]},{"location":"argo/#argo-cd-overview","level":3,"title":"Argo CD Overview","text":"<ul> <li>What Is Argo CD?</li> </ul>","path":["ARGO","Introduction to Argo"],"tags":["Argo","Argo CD","Argo Workflows","Argo Events","Argo Rollouts"]},{"location":"argo/#the-architecture-of-argo-cd","level":2,"title":"The Architecture of Argo CD","text":"<ul> <li>Vocabulary</li> </ul>","path":["ARGO","Introduction to Argo"],"tags":["Argo","Argo CD","Argo Workflows","Argo Events","Argo Rollouts"]},{"location":"argo/#core-components","level":3,"title":"Core Components","text":"<ul> <li>Controllers</li> <li>API Server</li> <li>Repository Server</li> <li>Application Controller</li> <li></li> </ul>","path":["ARGO","Introduction to Argo"],"tags":["Argo","Argo CD","Argo Workflows","Argo Events","Argo Rollouts"]},{"location":"argo/#understanding-reconciliation-and-synchronization-control","level":3,"title":"Understanding Reconciliation and Synchronization Control","text":"<ul> <li>How Does the Argo CD Reconciliation Loop Work?</li> <li>Synchronization Principles</li> </ul>","path":["ARGO","Introduction to Argo"],"tags":["Argo","Argo CD","Argo Workflows","Argo Events","Argo Rollouts"]},{"location":"argo/#objects-resources","level":3,"title":"Objects &amp; Resources","text":"<ul> <li>Simplifying Application Management</li> </ul>","path":["ARGO","Introduction to Argo"],"tags":["Argo","Argo CD","Argo Workflows","Argo Events","Argo Rollouts"]},{"location":"argo/#argo-cd-extensions-integrations","level":3,"title":"Argo CD Extensions &amp; Integrations","text":"<ul> <li>Plugins</li> <li>Understanding Plugins in Argo CD</li> <li>Configuring Plugins with ConfigMaps</li> <li>How Plugins Work in Argo CD</li> <li>Plugins in Action: Notifications and Beyond</li> </ul>","path":["ARGO","Introduction to Argo"],"tags":["Argo","Argo CD","Argo Workflows","Argo Events","Argo Rollouts"]},{"location":"argo/#best-practices","level":3,"title":"Best Practices","text":"<ul> <li>Securing Argo CD</li> </ul>","path":["ARGO","Introduction to Argo"],"tags":["Argo","Argo CD","Argo Workflows","Argo Events","Argo Rollouts"]},{"location":"argo/#note-on-helm-and-kustomize","level":3,"title":"Note on Helm and Kustomize","text":"<ul> <li>Enhancing Deployment Efficiency with Helm and Kustomize</li> </ul>","path":["ARGO","Introduction to Argo"],"tags":["Argo","Argo CD","Argo Workflows","Argo Events","Argo Rollouts"]},{"location":"argo/#lab-exercises","level":3,"title":"Lab Exercises","text":"<ul> <li>Lab 3.1. Installing Argo CD</li> <li>Lab 3.2. Managing Applications with Argo CD</li> <li>Lab 3.3. Argo CD Security and RBAC</li> </ul>","path":["ARGO","Introduction to Argo"],"tags":["Argo","Argo CD","Argo Workflows","Argo Events","Argo Rollouts"]},{"location":"argo/#argo-workflows","level":2,"title":"ARGO WORKFLOWS","text":"","path":["ARGO","Introduction to Argo"],"tags":["Argo","Argo CD","Argo Workflows","Argo Events","Argo Rollouts"]},{"location":"argo/#introduction_3","level":3,"title":"Introduction","text":"<ul> <li>Chapter Overview and Objectives</li> </ul>","path":["ARGO","Introduction to Argo"],"tags":["Argo","Argo CD","Argo Workflows","Argo Events","Argo Rollouts"]},{"location":"argo/#argo-workflows-core-concepts","level":3,"title":"Argo Workflows Core Concepts","text":"<ul> <li>Workflow</li> <li>Template Types</li> <li>Outputs</li> <li>WorkflowTemplate</li> </ul>","path":["ARGO","Introduction to Argo"],"tags":["Argo","Argo CD","Argo Workflows","Argo Events","Argo Rollouts"]},{"location":"argo/#argo-workflows-architecture","level":3,"title":"Argo Workflows Architecture","text":"<ul> <li>Defining Argo Workflows and Its Components</li> <li>Argo Workflow Overview</li> </ul>","path":["ARGO","Introduction to Argo"],"tags":["Argo","Argo CD","Argo Workflows","Argo Events","Argo Rollouts"]},{"location":"argo/#use-cases-for-argo-workflow","level":3,"title":"Use Cases for Argo Workflow","text":"<ul> <li>Examples</li> </ul>","path":["ARGO","Introduction to Argo"],"tags":["Argo","Argo CD","Argo Workflows","Argo Events","Argo Rollouts"]},{"location":"argo/#lab-exercises_1","level":3,"title":"Lab Exercises","text":"<ul> <li>Lab 4.1. Installing Argo Workflows</li> <li>Lab 4.2. A Simple DAG Workflow</li> <li>Lab 4.3. CI/CD Using Argo Workflows</li> </ul>","path":["ARGO","Introduction to Argo"],"tags":["Argo","Argo CD","Argo Workflows","Argo Events","Argo Rollouts"]},{"location":"argo/#argo-rollouts","level":2,"title":"ARGO ROLLOUTS","text":"","path":["ARGO","Introduction to Argo"],"tags":["Argo","Argo CD","Argo Workflows","Argo Events","Argo Rollouts"]},{"location":"argo/#introduction_4","level":3,"title":"Introduction","text":"<ul> <li>Chapter Overview and Objectives</li> </ul>","path":["ARGO","Introduction to Argo"],"tags":["Argo","Argo CD","Argo Workflows","Argo Events","Argo Rollouts"]},{"location":"argo/#a-primer-on-progressive-delivery","level":3,"title":"A Primer on Progressive Delivery","text":"<ul> <li>Essentials of CI/CD and Progressive Delivery in Software Development</li> <li>Continuous Integration</li> <li>Continuous Delivery</li> <li>Progressive Delivery</li> <li>Deployment Strategies</li> <li>Recreate/Fixed Deployment</li> <li>Rolling Update</li> <li>Blue-Green Deployment</li> <li>Canary Deployment</li> <li>Strategies for Smooth and Reliable Releases</li> </ul>","path":["ARGO","Introduction to Argo"],"tags":["Argo","Argo CD","Argo Workflows","Argo Events","Argo Rollouts"]},{"location":"argo/#argo-rollouts-architecture-and-core-components","level":3,"title":"Argo Rollouts Architecture and Core Components","text":"<ul> <li>Building Blocks of Argo Rollouts</li> <li>A Refresher: The Kubernetes Replica Set</li> <li>Argo Rollouts</li> <li>Key Features of Argo Rollouts</li> <li>Migrating Existing Deployments to Rollouts</li> <li>Discussion: Create Rollouts or Reference Deployments from Rollouts?</li> <li>Ingress and Service Resources</li> <li>Rollout Analysis &amp; Experiments</li> <li>Experiments</li> </ul>","path":["ARGO","Introduction to Argo"],"tags":["Argo","Argo CD","Argo Workflows","Argo Events","Argo Rollouts"]},{"location":"argo/#lab-exercises_2","level":3,"title":"Lab Exercises","text":"<ul> <li>Lab 5.1. Installing Argo Rollouts</li> <li>Lab 5.2. Argo Rollouts Blue-Green</li> <li>Lab 5.3. Migrating an Existing Deployment to Argo Rollouts</li> </ul>","path":["ARGO","Introduction to Argo"],"tags":["Argo","Argo CD","Argo Workflows","Argo Events","Argo Rollouts"]},{"location":"argo/#argo-events","level":2,"title":"ARGO EVENTS","text":"<p>Introduction</p> <ul> <li>Chapter Overview and Objectives</li> </ul> <p>The Main Components</p> <ul> <li>Event-Driven Architecture</li> </ul> <p>Lab Exercises</p> <ul> <li>Lab 6.1. Setting Up Event Triggers with Argo</li> <li>Lab 6.2. Integrating Argo Events with External Systems</li> </ul> <ul> <li>https://argoproj.github.io/</li> <li>https://github.com/akuity/awesome-argo</li> <li>https://argo-workflows.readthedocs.io/en/latest/</li> <li>https://argo-cd.readthedocs.io/en/stable/</li> <li>https://argoproj.github.io/argo-rollouts/</li> <li>https://argoproj.github.io/argo-events/</li> <li>https://training.linuxfoundation.org/certification/certified-argo-project-associate-capa/</li> </ul>","path":["ARGO","Introduction to Argo"],"tags":["Argo","Argo CD","Argo Workflows","Argo Events","Argo Rollouts"]},{"location":"argo/draft/","level":1,"title":"Draft","text":"<p>Argo Rollouts Features Architecture Argo Rollouts Components Key Features of Argo Rollouts Installation Options Standard Cluster-Wide Installation Deploy Argo Rollouts Restrictedn Namespace-Scoped Installation Deploy Argo Rollouts Namespace Scoped CLI Installation</p> <p>Deployment Strategies / Deyployment and Release pattern Direct Conversion Method workloadRef Method Strategies for Smooth and Reliable Releases Benefits of Introducing Deployment Strategies Common Use Cases for Each Strategy Rollout Analysis &amp; Experiments Experiments Lab exercices Lab - Installing Argo Rollouts Objective Prerequisites Install Cluster and Argo Rollouts</p> <p>Objective Prerequisites Transitioning to Argo Rollouts Dashboard Solutions √† essayer dans l'ordre : 1. V√©rifier la version d'Argo Rollouts 2. Utiliser le dashboard d√©ploy√© dans le cluster (recommand√©) 3. Cr√©er un Rollout minimal 4. Essayer avec des options diff√©rentes 5. Utiliser l'extension ArgoCD (si tu as ArgoCD) 6. Debug avanc√© Solution temporaire si urgent 1. Installation d'ArgoCD via Helm 2. Installation d'Argo Rollouts (n√©cessaire pour utiliser l'extension) 3. Acc√®s √† l'interface ArgoCD 4. Test de l'extension avec un Rollout exemple Alternative : Si tu pr√©f√®res configurer directement avec les volumes Blue/Green - Initial Version Specification Demo blue green deployment Canary</p>","path":["ARGO","DRAFT","Draft"],"tags":[]},{"location":"argo/draft/#argo-rollouts-features","level":2,"title":"Argo Rollouts Features","text":"<p>Progressive Delivery: D√©ployer progressivement de nouvelles versions de logiciels √† un sous ensemble d'utilisateurs pour minimiser le risque Rollout CRD: Rollout Controller est un CRD Kubernetes qui remplace l'objet de d√©ploiement standard pour permettre ces strat√©gies avances Canary Release: Expose une nouvelle version √† un petit groupe d'utilistaur pour des test en direct avant un d√©ploiement complet Blue-Green Deployment: Fonctionner deux environement blue est stable et green est la nouvelle version. Switch traffic une fois que green est d√©ployer green deviens blue Analysis (metric providers): Int√©roge des fourniseur de metriques comme Prometheus pour verfiiser si la nouvelle version est saine avant de continuer Traffic Shaping: % pourcentage d'utilisateur qui bascule sur la nouvelle version (ingress controllers ou service mesh) Automated rollbacks: Scrap des donn√©e de performance si l'analyse √©choue le d√©ploiement est automatiquement r√©tablie a la version stable pr√©c√©dente gitops integration:</p> <p>Argo Rollouts Here, we will explore the Argo Rollouts resource, which is the central element in Argo Rollouts, enabling advanced deployment strategies. A Rollout, in essence, is a Kubernetes resource that closely mirrors the functionality of a Kubernetes Deployment object. However, it steps in as a more advanced substitute for Deployment objects, particularly in scenarios demanding intricate deployment of progressive delivery techniques.</p>","path":["ARGO","DRAFT","Draft"],"tags":[]},{"location":"argo/draft/#installation-options","level":2,"title":"Installation Options","text":"","path":["ARGO","DRAFT","Draft"],"tags":[]},{"location":"argo/draft/#standard-cluster-wide-installation","level":3,"title":"Standard Cluster-Wide Installation","text":"","path":["ARGO","DRAFT","Draft"],"tags":[]},{"location":"argo/draft/#deploy-argo-rollouts","level":4,"title":"Deploy Argo Rollouts","text":"<p>Create a namespace for Argo Rollouts using the following command:</p> <pre><code>kubectl create namespace argo-rollouts\n</code></pre> <p>Deploy Argo Rollouts using the quick start manifest:</p> <pre><code>kubectl apply -n argo-rollouts -f https://github.com/argoproj/argo-rollouts/releases/download/v1.8.3/install.yaml\n</code></pre> <p>This will install custom resource definitions as well as the Argo Rollouts controller. During this course we use Argo Rollouts in version 1.8.3. We recommend using the same version to ensure consistent results.  </p> <p>Verify that Argo Rollouts is installed by running the following command:</p> <pre><code>kubectl get pods -n argo-rollouts\n</code></pre>","path":["ARGO","DRAFT","Draft"],"tags":[]},{"location":"argo/draft/#restrictedn-namespace-scoped-installation","level":3,"title":"Restrictedn Namespace-Scoped Installation","text":"<p>Limit√© √† un namespace</p>","path":["ARGO","DRAFT","Draft"],"tags":[]},{"location":"argo/draft/#deploy-argo-rollouts-namespace-scoped","level":4,"title":"Deploy Argo Rollouts Namespace Scoped","text":"<p>Create a namespace for Argo Rollouts using the following command:</p> <pre><code>kubectl create namespace argo-rollouts\n</code></pre> <p>Deploy Argo Rollouts using the quick start manifest:</p> <pre><code>kubectl apply -n argo-rollouts -f https://github.com/argoproj/argo-rollouts/../v1.8.3/download/namespace-install.yaml\n</code></pre> <p>Admin de cluster requi pour installer les CRDs de rollout s√©parement</p> <pre><code>kubectl -n argo-rollouts get all\nNAME                                 READY   STATUS    RESTARTS   AGE\npod/argo-rollouts-7858b65d86-bcbhq   1/1     Running   0          8m55s\n\nNAME                            TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)    AGE\nservice/argo-rollouts-metrics   ClusterIP   10.96.68.1   &lt;none&gt;        8090/TCP   8m55s\n\nNAME                            READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/argo-rollouts   1/1     1            1           8m55s\n\nNAME                                       DESIRED   CURRENT   READY   AGE\nreplicaset.apps/argo-rollouts-7858b65d86   1         1         1       8m55s\n</code></pre>","path":["ARGO","DRAFT","Draft"],"tags":[]},{"location":"argo/draft/#rolling-update","level":4,"title":"Rolling Update","text":"<p>Contexte</p> <p>Vous devez d√©ployer une nouvelle version de votre application sans pouvoir tol√©rer la moindre interruption de service. Cependant, lancer simultan√©ment l‚Äôensemble des nouveaux pods n‚Äôest pas envisageable, car cela risquerait de surcharger le cluster.</p> <p>Il est donc n√©cessaire d‚Äôadopter une strat√©gie de d√©ploiement progressive permettant de remplacer les anciennes versions par les nouvelles tout en maintenant la disponibilit√© et la stabilit√© du syst√®me.</p> <p> </p> <p>Cette approche garantit une perturbation minimale et une disponibilit√© continue de l‚Äôapplication. En cas de probl√®me avec un nouveau pod, le rollout peut √™tre mis en pause ou annul√©.</p> <p>Note</p> <p>Rolling Update et la strat√©gie par d√©faut de l‚Äôobjet <code>Deployment</code> de Kubernetes. Kubernetes utilise les valeurs par d√©faut (25%)</p> <p>Il n'est pas n√©c√©ssaire de rajouter une strategy dans l'objet <code>Deployment</code> par d√©fault il utiliseras la configuration ci-dessous: Tu n‚Äôes pas oblig√© de rajouter √ßa dans ton Deployment si tu veux juste le comportement par d√©faut.</p> <pre><code>  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1 \n      maxUnavailable: 1 \n</code></pre> <p>Voici pourquoi :</p> <p>1Ô∏è‚É£ Comportement par d√©faut</p> <p>Pour un Deployment Kubernetes, si tu ne pr√©cises pas de strategy :</p> <p>strategy:   type: RollingUpdate</p> <p>C‚Äôest exactement le rolling update qui va se passer.</p> <p>Les valeurs par d√©faut pour maxSurge et maxUnavailable sont :</p> <p>maxSurge: 25%       # jusqu‚Äô√† 25% de pods en plus maxUnavailable: 25% # jusqu‚Äô√† 25% de pods indisponibles</p> <p>Donc pour ton Deployment avec replicas: 3 :</p> <p>maxSurge: 25% ‚Üí arrondi √† 1 pod suppl√©mentaire au maximum</p> <p>maxUnavailable: 25% ‚Üí arrondi √† 0 ou 1 pod indisponible (Kubernetes arrondit √† l‚Äôentier sup√©rieur pour maxUnavailable)</p> <p>2Ô∏è‚É£ Quand tu voudrais le mettre explicitement</p> <p>Tu utiliserais :</p> <p>strategy:   type: RollingUpdate   rollingUpdate:     maxSurge: 1     maxUnavailable: 1</p> <p>Ici tu contr√¥les exactement qu‚Äôau maximum 1 pod suppl√©mentaire sera cr√©√© et qu‚Äôau maximum 1 pod peut √™tre indisponible √† la fois.</p> <p>C‚Äôest utile si tu veux contr√¥ler tr√®s finement le trafic ou si tu as des pods qui consomment beaucoup de ressources.</p> <p>‚úÖ Conclusion</p> <p>Si tu es d‚Äôaccord avec le comportement par d√©faut (Kubernetes fait un rolling update avec au maximum 1 pod sur 3 indisponible √† la fois), tu n‚Äôas pas besoin de l‚Äôajouter.</p> <p>Ajouter cette section est juste pour plus de pr√©cision et contr√¥le, pas pour que √ßa fonctionne.</p> <p>Si tu veux, je peux te montrer un petit tableau qui compare le comportement par d√©faut vs maxSurge/maxUnavailable explicites, √ßa rend tout super clair pour 3 pods. Veux que je fasse √ßa‚ÄØ?</p> <p>maxSurge et maxUnavailable sont des pourcentage obligatoire ?</p> <p>Non, ils ne sont pas obligatoirement des pourcentages, tu peux les sp√©cifier en nombre absolu de pods. Kubernetes accepte les deux formats :</p> <p>1Ô∏è‚É£ En pourcentage maxSurge: 25%       # 25% des replicas, arrondi √† l'entier sup√©rieur maxUnavailable: 25% # idem</p> <p>Pour ton Deployment de 3 pods :</p> <p>25% ‚Üí 0,75 ‚Üí arrondi √† 1 pod.</p> <p>Avantage : √ßa s‚Äôadapte automatiquement si tu changes le nombre de replicas.</p> <p>2Ô∏è‚É£ En nombre absolu maxSurge: 1 maxUnavailable: 1</p> <p>Ici, Kubernetes prend exactement 1 pod, peu importe le nombre total de replicas.</p> <p>Avantage : tu as un contr√¥le strict, utile pour des d√©ploiements sensibles ou des petits clusters.</p> <p>3Ô∏è‚É£ Important</p> <p>Tu dois toujours mettre une valeur pour maxSurge et maxUnavailable si tu d√©finis strategy: RollingUpdate.</p> <p>Mais si tu ne d√©finis pas strategy du tout, Kubernetes utilise les valeurs par d√©faut (25%), donc tu n‚Äôas rien √† mettre.</p> <p>GitOps: Le dev modifie dans le manifest de deployment la version de 1.0 √† 2.0 et le push  argo prendra en consid√©ration la modification et d√©ploieras la nouvelle app selon les indication fournis dans la strategy</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: example-app\n  namespace: default\n  labels:\n    app: example-app\nspec:\n  replicas: 3\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1 # (1)! \n      maxUnavailable: 1 # (2)! \n  selector:\n    matchLabels:\n      app: example-app\n  template:\n    metadata:\n      labels:\n        app: example-app\n    spec:\n      containers:\n      - name: app\n        image: nginx:1.24\n        ports:\n        - containerPort: 80\n          name: http\n        resources:\n          requests:\n            memory: \"64Mi\"\n            cpu: \"50m\"\n          limits:\n            memory: \"128Mi\"\n            cpu: \"100m\"\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 80\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 80\n          initialDelaySeconds: 15\n          periodSeconds: 20\n</code></pre> <ol> <li>maxSurge     Nombre max (Pourcentage ou Absolue) de pods en plus pendant l'update</li> <li>maxUnavailable     Nombre max (Pourcentage ou Absolue) de pods indisponibles pendant l'update</li> </ol> <p>Pour d√©clencher un Rolling Update, tu modifies simplement l'image dans le Deployment :</p> <pre><code># Via kubectl\nkubectl set image deployment/example-app app=nginx:1.25\n\n# Ou en √©ditant le manifest et en appliquant\nkubectl apply -f deployment.yaml\n\n# Suivre le status du rollout\nkubectl rollout status deployment/example-app\n\n# Voir l'historique\nkubectl rollout history deployment/example-app\n\n# Rollback si n√©cessaire\nkubectl rollout undo deployment/example-app\n</code></pre> <p>Les points cl√©s pour un Rolling Update efficace:</p> <ul> <li>Strategy configuration : Les param√®tres maxSurge et maxUnavailable contr√¥lent la vitesse et la disponibilit√© pendant l'update  </li> <li>Readiness/Liveness probes : Essentielles pour que Kubernetes sache quand un pod est pr√™t √† recevoir du trafic  </li> <li>Resources limits : Permet au scheduler de placer correctement les pods pendant l'update</li> </ul>","path":["ARGO","DRAFT","Draft"],"tags":[]},{"location":"argo/draft/#direct-conversion-method","level":2,"title":"Direct Conversion Method","text":"<p>Migrating Existing Deployments to Rollouts The similarity of Deployments and Rollouts spec makes it easier to convert from one to the other resource type. Argo Rollouts supports a great way to migrate existing Deployment resources to Rollouts.</p> <p>By providing a spec.workloadRef instead of spec.template a Rollout can refer to a Deployments template:</p> <p>apiVersion: argoproj.io/v1alpha1 kind: Rollout metadata:   name: nginx-rollout spec:   replicas: 3   selector:     matchLabels:       app: nginx   workloadRef:     apiVersion: apps/v1     kind: Deployment     name: nginx-deployment [...]</p> <p>The Rollout will fetch the template information from the Deployment (in our example named nginx-deployment) and start the in the Rollout specified number of pods.</p> <p>Please note, that lifecycles of Deployment and Rollouts are distinct and managed by their respective controllers. This means that the Kubernetes Deployment controller will not start to manage Pods created by the Rollout. Also, the Rollout will not start to manage pods that are controlled by the Deployment.</p> <p>This enables a zero-downtime introduction of Argo Rollouts to your existing cluster. It furthermore makes experimentation with multiple deployment scenarios possible.</p> <p>deployment.yaml Que fait Kubernetes exactement ?</p> <ol> <li>Kubernetes compare la nouvelle spec avec l‚Äôancienne.</li> <li>Il voit que l‚Äôimage a chang√©, donc il doit cr√©er de nouveaux pods.</li> <li> <p>Le rolling update va fonctionner comme suit:b</p> <ul> <li>Kubernetes va cr√©er un nouveau pod avec l‚Äôimage green.</li> <li>Quand ce pod est Ready (liveness et readiness passent), il supprime un pod blue existant.</li> <li>Il r√©p√®te ce processus jusqu‚Äô√† ce que tous les pods soient remplac√©s par des pods green.</li> </ul> </li> </ol> <p>Donc le cycle est :</p> <ul> <li>3 pods blue</li> <li>3 pods blue ‚Üí 1 pod green</li> <li>2 pods blue ‚Üí 2 pods green</li> <li>1 pod blue ‚Üí 3 pods green</li> <li>3 pod green</li> </ul> <p>D√©tails sur les pods:</p> <ul> <li>Les pods blue sont supprim√©s progressivement.</li> <li>Les pods green sont cr√©√©s progressivement.</li> <li>Tu n‚Äôas jamais de downtime complet si tes pods sont correctement configur√©s avec readiness probes, car le Service continuera √† router le trafic vers les pods Ready.</li> </ul> <p>Deployment and Argo Rollout Resource in Comparison</p> deployment.yaml<pre><code>apiVersion: {==apps/v1==}\nkind: {==Deployment==}\nmetadata:\n  name: blue-green-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: blue-green\n  template:\n    metadata:\n      labels:\n        app: blue-green\n    spec:\n      containers:\n        - name: blue-green-container\n          image: siddharth67/app:blue\n  strategy:\n    {==type: RollingUpdate==}\n    rollingUpdate:\n      maxSurge: 1 \n      maxUnavailable: 1 \n</code></pre> rollout.yaml<pre><code>apiVersion: {==argoproj.io/v1alpha1==}\nkind: {==Rollout==}\nmetadata:\n  name: blue-green-rollout\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: blue-green\n  template:\n    metadata:\n      labels:\n        app: blue-green\n    spec:\n      containers:\n        - name: blue-green-container\n          image: siddharth67/app:blue\n  strategy:\n    blueGreen:\n      activeService: blue-green-active-svc\n      previewService: blue-green-preview-svc\n      autoPromotionEnabled: false\n</code></pre> <p>Pros:</p> <ul> <li>Rollout adds advanced strategies like blue-green and canary</li> <li>Keeps the same pod template and replicas</li> <li>Traffic can be controlled via active/preview services</li> <li>Auto-promotion can be manual ou automatic</li> </ul>","path":["ARGO","DRAFT","Draft"],"tags":[]},{"location":"argo/draft/#workloadref-method","level":2,"title":"workloadRef Method","text":"<p>https://learn.kodekloud.com/user/courses/certified-argo-project-associate-capa/module/959dfde0-9415-4fc2-bcad-fe9e4bf84cc7/lesson/d363f5a0-40da-4442-9a84-91862257155e</p> deployment.yaml<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: blue-green-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: blue-green\n  template:\n    metadata:\n      labels:\n        app: blue-green\n    spec:\n      containers:\n        - name: blue-green-container\n          image: siddharth67/app:blue\n</code></pre> rollout.yaml<pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Rollout\nmetadata:\n  name: blue-green-rollout\nspec:\n  workloadRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: blue-green-deployment\n    scaleDown: never | onsuccess | progressively\n  strategy:\n    blueGreen:\n      activeService: blue-green-active-svc\n      previewService: blue-green-preview-svc\n      autoPromotionEnabled: false\n</code></pre> <p>https://argo-rollouts.readthedocs.io/en/stable/features/specification/</p>","path":["ARGO","DRAFT","Draft"],"tags":[]},{"location":"argo/draft/#dashboard","level":2,"title":"Dashboard","text":"<p>L'erreur <code>ERR_EMPTY_RESPONSE</code> avec le message <code>grpc: addrConn.createTransport failed</code> est un probl√®me connu. Voici comment le r√©soudre :</p>","path":["ARGO","DRAFT","Draft"],"tags":[]},{"location":"argo/draft/#solutions-a-essayer-dans-lordre","level":2,"title":"Solutions √† essayer dans l'ordre :","text":"","path":["ARGO","DRAFT","Draft"],"tags":[]},{"location":"argo/draft/#1-verifier-la-version-dargo-rollouts","level":3,"title":"1. V√©rifier la version d'Argo Rollouts","text":"<p>D'abord, v√©rifie ta version : <pre><code>kubectl argo rollouts version\n</code></pre></p> <p>Si tu es en v1.8.0, c'est probablement la cause - cette version a un bug critique qui casse le dashboard.</p>","path":["ARGO","DRAFT","Draft"],"tags":[]},{"location":"argo/draft/#2-utiliser-le-dashboard-deploye-dans-le-cluster-recommande","level":3,"title":"2. Utiliser le dashboard d√©ploy√© dans le cluster (recommand√©)","text":"<p>Au lieu d'utiliser <code>kubectl argo rollouts dashboard</code>, d√©ploie le dashboard comme service dans Kubernetes :</p> <pre><code># Si tu utilises Helm\nhelm upgrade --install argo-rollouts argo/argo-rollouts \\\n  --namespace argo-rollouts \\\n  --set dashboard.enabled=true\n\n# Ou avec les manifests officiels\nkubectl apply -n argo-rollouts -f https://github.com/argoproj/argo-rollouts/releases/latest/download/dashboard-install.yaml\n</code></pre> <p>Puis acc√®de-y via port-forward : <pre><code>kubectl port-forward -n argo-rollouts svc/argo-rollouts-dashboard 3100:3100\n</code></pre></p>","path":["ARGO","DRAFT","Draft"],"tags":[]},{"location":"argo/draft/#3-creer-un-rollout-minimal","level":3,"title":"3. Cr√©er un Rollout minimal","text":"<p>Le dashboard a besoin d'au moins un Rollout pour fonctionner correctement :</p> <pre><code># test-rollout.yaml\napiVersion: argoproj.io/v1alpha1\nkind: Rollout\nmetadata:\n  name: test-rollout\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: test\n  template:\n    metadata:\n      labels:\n        app: test\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:alpine\n        ports:\n        - containerPort: 80\n</code></pre> <pre><code>kubectl apply -f test-rollout.yaml\n</code></pre>","path":["ARGO","DRAFT","Draft"],"tags":[]},{"location":"argo/draft/#4-essayer-avec-des-options-differentes","level":3,"title":"4. Essayer avec des options diff√©rentes","text":"<pre><code># Sp√©cifier le namespace explicitement\nkubectl argo rollouts dashboard -n default\n\n# Ou essayer un autre port\nkubectl argo rollouts dashboard --port 8080\n</code></pre>","path":["ARGO","DRAFT","Draft"],"tags":[]},{"location":"argo/draft/#5-utiliser-lextension-argocd-si-tu-as-argocd","level":3,"title":"5. Utiliser l'extension ArgoCD (si tu as ArgoCD)","text":"<p>C'est l'alternative la plus stable actuellement : <pre><code># Installer l'extension dans ArgoCD\nkubectl apply -n argocd -f https://github.com/argoproj-labs/rollout-extension/releases/latest/download/install.yaml\n</code></pre></p> <p>L'extension ajoute un onglet \"Rollouts\" directement dans l'interface ArgoCD pour chaque application.</p>","path":["ARGO","DRAFT","Draft"],"tags":[]},{"location":"argo/draft/#6-debug-avance","level":3,"title":"6. Debug avanc√©","text":"<p>Si rien ne fonctionne, v√©rifie les logs du controller : <pre><code># Logs du controller\nkubectl logs -n argo-rollouts deployment/argo-rollouts\n\n# V√©rifier si les CRDs sont install√©s\nkubectl get crd rollouts.argoproj.io\n</code></pre></p>","path":["ARGO","DRAFT","Draft"],"tags":[]},{"location":"argo/draft/#solution-temporaire-si-urgent","level":3,"title":"Solution temporaire si urgent","text":"<p>En attendant de r√©soudre le probl√®me du dashboard, tu peux utiliser le CLI pour visualiser tes rollouts : <pre><code># Voir l'√©tat d'un rollout\nkubectl argo rollouts get rollout &lt;nom-rollout&gt; --watch\n\n# Lister tous les rollouts\nkubectl argo rollouts list rollouts -A\n</code></pre></p> <p>Le probl√®me le plus probable dans ton cas est soit : 1. La v1.8.0 qui est cass√©e 2. Aucun Rollout n'existe dans le cluster 3. Le dashboard n'est pas d√©ploy√© comme service dans le cluster</p> <p>Tu peux me dire quelle version tu utilises et si tu as d√©j√† des Rollouts ? Je pourrai t'aider plus pr√©cis√©ment.</p> <p>D'accord ! Je vais te pr√©parer un process complet pour installer ArgoCD via Helm et ensuite ajouter l'extension Rollout selon la documentation du repo.</p>","path":["ARGO","DRAFT","Draft"],"tags":[]},{"location":"argo/draft/#1-installation-dargocd-via-helm","level":2,"title":"1. Installation d'ArgoCD via Helm","text":"<pre><code># Cr√©er le namespace argocd\nkubectl create namespace argocd\n\n# Ajouter le repo Helm d'ArgoCD\nhelm repo add argo https://argoproj.github.io/argo-helm\nhelm repo update\n\n# Cr√©er un fichier values pour ArgoCD avec l'extension\ncat &gt; argocd-values.yaml &lt;&lt;EOF\nserver:\n  extensions:\n    enabled: true\n    extensionList:\n      - name: rollout-extension\n        env:\n          - name: EXTENSION_URL\n            value: https://github.com/argoproj-labs/rollout-extension/releases/download/v0.3.7/extension.tar\nEOF\n\n# Installer ArgoCD avec l'extension\nhelm install argocd argo/argo-cd \\\n  --namespace argocd \\\n  --version 7.7.12 \\\n  --values argocd-values.yaml\n</code></pre>","path":["ARGO","DRAFT","Draft"],"tags":[]},{"location":"argo/draft/#2-installation-dargo-rollouts-necessaire-pour-utiliser-lextension","level":2,"title":"2. Installation d'Argo Rollouts (n√©cessaire pour utiliser l'extension)","text":"<pre><code># Cr√©er le namespace pour argo-rollouts\nkubectl create namespace argo-rollouts\n\n# Installer Argo Rollouts\nkubectl apply -n argo-rollouts -f https://github.com/argoproj/argo-rollouts/releases/latest/download/install.yaml\n</code></pre>","path":["ARGO","DRAFT","Draft"],"tags":[]},{"location":"argo/draft/#3-acces-a-linterface-argocd","level":2,"title":"3. Acc√®s √† l'interface ArgoCD","text":"<pre><code># R√©cup√©rer le mot de passe admin initial\nkubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d\n\n# Port-forward pour acc√©der √† l'interface (ou utilise ton LoadBalancer avec kind)\nkubectl port-forward svc/argocd-server -n argocd 8080:443\n</code></pre>","path":["ARGO","DRAFT","Draft"],"tags":[]},{"location":"argo/draft/#4-test-de-lextension-avec-un-rollout-exemple","level":2,"title":"4. Test de l'extension avec un Rollout exemple","text":"<pre><code># D√©ployer les exemples du getting started d'Argo Rollouts\nkubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/basic/rollout.yaml\nkubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/basic/service.yaml\n</code></pre>","path":["ARGO","DRAFT","Draft"],"tags":[]},{"location":"argo/draft/#alternative-si-tu-preferes-configurer-directement-avec-les-volumes","level":2,"title":"Alternative : Si tu pr√©f√®res configurer directement avec les volumes","text":"<p>Si la m√©thode avec <code>server.extensions</code> ne fonctionne pas, tu peux utiliser cette configuration alternative dans ton <code>argocd-values.yaml</code> :</p> <pre><code>server:\n  initContainers:\n    - name: rollout-extension\n      image: quay.io/argoprojlabs/argocd-extension-installer:v0.0.8\n      env:\n      - name: EXTENSION_URL\n        value: https://github.com/argoproj-labs/rollout-extension/releases/download/v0.3.7/extension.tar\n      volumeMounts:\n        - name: extensions\n          mountPath: /tmp/extensions/\n      securityContext:\n        runAsUser: 1000\n        allowPrivilegeEscalation: false\n  volumeMounts:\n    - name: extensions\n      mountPath: /tmp/extensions/\n  volumes:\n    - name: extensions\n      emptyDir: {}\n</code></pre> <p>Une fois tout install√©, tu devrais voir l'onglet Rollout dans l'interface ArgoCD quand tu visualises une ressource de type Rollout !</p>","path":["ARGO","DRAFT","Draft"],"tags":[]},{"location":"argo/draft/#bluegreen-initial-version","level":2,"title":"Blue/Green - Initial Version","text":"<p>BG est une strat√©gie de publication d'application qui utilise deux environnement de production identiques</p> <p>Blue qui est celui en ligne et Green qui est l'options de secours.</p>","path":["ARGO","DRAFT","Draft"],"tags":[]},{"location":"argo/draft/#specification","level":3,"title":"Specification","text":"<p>Deployer une nouvelle version green au cot√©s de la version blue stable et ensuite rediriger le trafic en direct vers elle seulement apr√®s qu'elle ait √©t√© enti√®rement v√©rifi√©</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Rollout\nmetadata:\n  name: web-api-rollout\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: web-api\n  spec:\n    container:\n    - name: web-api-container\n      image: my-repo/web-api:v1\n      ports:\n      - containerPort: 8080\n  strategy:\n    blueGreen:\n      activeService: web-api-active-svc\n      previewService: web-api-preview-svc\n      previewReplicaCount: 1\n      autoPromotionEnables: false\n      postPromotionAnalysis:\n        templates:\n        - templateName: check-error-rate\n      scaleDownDelaySeconds: 30\n      abortScaleDownDelaySeconds: 10\n</code></pre> <p>activeService: est le service kubernetes principal pour le traffic de production en direct previewService: est un service interne s√©parer qui pointe toujours cers la nouvelle version Green cela fournit un point de terminaison stable pour les test automatis√©es et le QA manual avant que le trafic utilisateur ne soit affect√©</p> <p>previewReplicaCount: Cela va essentiellement d√©ployer un nombre sp√©cifici√© plus petit de r√©plicas pour la version Green durant la phase de pr√©visualisation afin de conserver les ressources du cluster tout en permettant des tests complets autoPromotionEnabled: Comme un interrupteur de s√©curit√© critique. Le rollout d√©ploie la version Green, mais ensuite il fait une pause et attend une approbation manuelle en utilisant kubectl ou une UI avant de charger le traffic. Par d√©faut si vous ne mentionnez pas cela, l'auto-promotion activ√©es est toujorus vraie!</p> <pre><code>kubectl argo rollouts promote\n</code></pre> <p>postPromotionAnalysis: configurer pour ex√©cuter une v√©rification de sant√© d'analyse automatis√© comme interroger Prometheus apr√®s que la nouvelle version prenne 100% du traffic en direct pour s'assurer que les SLOs sont toujours respecter.</p> <p>scaleDownDelaySecondes: C'est comme une fen√™tres de rollback rapide. Apr√®s avoir redirige le traffic vers la nouvelle version Green, le controleur attend ce nombre de secondes avant de terminer les anciens pods bleus ce qui permet un rollback instantan√© si un probl√®mes est d√©tecter.</p> <p>abortScaleDownDelaySeconds: Pour annuler la reduction d'√©ch√®lle. Donc si le rollout est annul√© manuellement, le controlleur attend ce nombre de secondes avant de terminer les pods vers d√©fectueux. Ce bref d√©lai permet au d√©veloppeurs de collecter des logs ou des dumps de m√©moire √† des fin de d√©bogage.</p>","path":["ARGO","DRAFT","Draft"],"tags":[]},{"location":"argo/draft/#demo-blue-green-deployment","level":3,"title":"Demo blue green deployment","text":"","path":["ARGO","DRAFT","Draft"],"tags":[]},{"location":"argo/draft/#canary","level":3,"title":"Canary","text":"<pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Rollout\nmetadata:\n  name: web-api-rollout\nspec:\n  replicas: 10\n  selector:\n    matchLabels:\n      app: web-api\n  template:\n    metadata:\n      labels:\n        app: web-api\n  spec:\n    container:\n    - name: web-api-container\n      image: my-repo/web-api:v1\n      ports:\n      - containerPort: 8080\n  strategy:\n    canary:\n      steps:\n      - setWeight: 10\n      - pause: {}\n      - setWeight: 25\n      - analysis:\n          templates:\n          - templateName: check-api-error-rate\n          args:\n          - name: service-name\n            value: web-api-canary-svc\n      - setWeight: 50\n      - pause: { duration: 5m }\n      canaryService: web-api-canary-svc # pointe 1.1\n      stableService: web-api-stable-svc # pointe 1.0\n</code></pre> Feature Blue/Green Canary Deployment Primary Goal Zero Downtime full environment switch, rapid rollback Reduce risk by gradual exposure. Real-user feedback Environment Two complete, identical environments (Blue &amp; Green) Single environment, with a small subset of new instances Traffic Split All-or-nothing (or very rapid) switch of all traffic Gradual, weighted routing of portion of traffic Rollback Instantaneous switch back to the old \"Blue\" Environment Revert traffic weight to 0% for the new version Complexity Can be simpler to set up initially, but resource-heavy (two full environments) More complex to set up due to traffic routing (service mesh) Feedback Loop Feedback mostly gathered before the full traffic switch in Green Continious, real-time feedback from a small user segment during rollout Risk Reduction Reduces risk of downtime, high confidence after green validation Reduces \"blast radius\" of issues to a small subset of users Resource Usage Higher ( two full environments running simultaneously) Lower (only a small percentage of new resources initially) Best for Applications sensitive to downtime, quick \"go/no-go\" decisions Risk-averse releases, A/B testing, user behavior monitoring, unknown impatcs","path":["ARGO","DRAFT","Draft"],"tags":[]},{"location":"argo/argo_cd/","level":1,"title":"Argo CD","text":"","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#introduction","level":2,"title":"Introduction","text":"","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#quest-ce-que-gitops","level":3,"title":"Qu'est-ce que GitOps","text":"<p>GitOps est un cadre op√©rationnel qui utilise Git comme la source unique de v√©rit√© pour g√©rer √† la fois l'infrastructure et le code des applications. Il √©tend les principes de l'Infrastructure as Code, permettant des d√©ploiements et des retours en arri√®re automatis√©s en contr√¥lant l'ensemble du pipeline de livraison du code via le contr√¥le de version Git.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#workflow-gitops","level":4,"title":"Workflow GitOps","text":"<p>Les d√©veloppeurs commencent par committer leurs changements dans un repository Git centralis√©. En g√©n√©ral, ils travaillent dans des feature branches cr√©√©es comme des copies de la base de code principale. Ces branches permettent aux √©quipes de d√©velopper de nouvelles fonctionnalit√©s en isolation jusqu'√† ce qu'elles soient pr√™tes. Un service de Continuous Integration (CI) construit automatiquement l'application et ex√©cute des tests unitaires sur le nouveau code. Une fois les tests r√©ussis, les changements sont soumis √† un processus de r√©vision et d'approbation par les membres de l'√©quipe concern√©s avant d'√™tre merg√©s dans le repository central.</p> <p>La derni√®re √©tape du pipeline est le Continuous Deployment (CD), o√π les changements du repository sont automatiquement d√©ploy√©s vers des Kubernetes clusters.</p> <p>L'image illustre le workflow GitOps, montrant l'int√©gration de l'infrastructure, de la configuration et du code de l'application dans un repository Git, suivie des processus de continuous integration (CI) et de continuous deployment (CD) vers un Kubernetes cluster. Elle repr√©sente √©galement le processus de branching et de merging dans Git.</p> <p>Au c≈ìur de GitOps se trouve le concept d'√©tat d√©fini de mani√®re d√©clarative. Cela implique de maintenir votre infrastructure, les configurations des applications et les composants associ√©s dans un ou plusieurs repositories Git. Un processus automatis√© v√©rifie en continu que l'√©tat stock√© dans Git correspond √† l'√©tat r√©el dans l'environnement de production. Cette synchronisation est g√©r√©e par un GitOps operator qui fonctionne au sein d'un Kubernetes cluster. L'operator surveille le repository pour d√©tecter des mises √† jour et applique les changements souhait√©s au cluster ‚Äî ou m√™me √† d'autres clusters si n√©cessaire.</p> <p>Lorsque un d√©veloppeur merge du nouveau code dans le repository de l'application, une s√©rie d'√©tapes automatis√©es est d√©clench√©e : les tests unitaires sont ex√©cut√©s, l'application est construite, une image Docker est cr√©√©e et pouss√©e dans un container registry, et enfin, les Kubernetes manifests dans un autre repository Git sont mis √† jour.</p> <p>L'image illustre un workflow GitOps, montrant le processus depuis le merging du code de l'application, la continuous integration, jusqu'au d√©ploiement des Kubernetes manifests, avec les GitOps operators garantissant que l'√©tat souhait√© correspond √† l'√©tat r√©el dans les environnements de production.</p> <p>L'operator GitOps compare en continu l'√©tat souhait√© (tel que d√©fini dans Git) avec l'√©tat r√©el dans le Kubernetes cluster. Si des √©carts sont d√©tect√©s, l'operator effectue les changements n√©cessaires pour garantir que l'environnement de production reste align√© avec la configuration souhait√©e.</p> <p>L'image illustre un workflow GitOps, montrant le processus depuis le repository du code de l'application jusqu'√† l'int√©gration continue, puis le d√©ploiement dans Kubernetes, mettant en √©vidence la synchronisation entre les √©tats souhait√© et r√©el.</p> <p>Facilit√© des Rollbacks</p> <p>L'un des principaux avantages de GitOps est la simplicit√© du processus de rollback. Puisque toute la configuration est maintenue dans Git, revenir √† un √©tat pr√©c√©dent est aussi simple que d'ex√©cuter la commande git revert. L'operator GitOps d√©tecte ce changement et effectue automatiquement un rollback de l'environnement de production pour correspondre √† l'√©tat souhait√©.</p> <p>L'image illustre un workflow GitOps, montrant le processus depuis le repository du code de l'application jusqu'√† l'int√©gration continue, puis le d√©ploiement dans Kubernetes, soulignant la synchronisation entre les √©tats souhait√© et r√©el.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#les-principes-de-gitops","level":3,"title":"Les principes de GitOps","text":"<p>Dans cette le√ßon, nous allons explorer les principes fondamentaux de GitOps, une approche du d√©ploiement continu qui utilise Git comme la source unique de v√©rit√© pour l'√©tat de l'infrastructure et des applications. La m√©thodologie GitOps repose sur quatre principes de base.</p> <p>Rappel</p> <p>GitOps assure la coh√©rence du syst√®me et r√©duit les erreurs humaines en imposant un mod√®le d√©claratif pour la gestion de l'infrastructure.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#1-approche-declarative-vs-imperative","level":4,"title":"1. Approche d√©clarative vs. imp√©rative","text":"<p>Le premier principe met l'accent sur une m√©thodologie d√©clarative plut√¥t qu'imp√©rative. Dans le mod√®le d√©claratif, l'ensemble du syst√®me, y compris l'infrastructure et les manifests des applications, est d√©crit dans un √©tat souhait√©. Cela contraste avec l'approche imp√©rative, o√π des commandes sp√©cifiques sont ex√©cut√©es s√©quentiellement pour modifier l'√©tat du syst√®me. S'appuyer sur un style imp√©ratif peut compliquer la r√©conciliation, car il ne maintient pas un enregistrement complet de l'√©tat pr√©vu du syst√®me.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#2-stockage-de-letat-souhaite-dans-git","level":4,"title":"2. Stockage de l'√©tat souhait√© dans Git","text":"<p>Le deuxi√®me principe stipule que tous les fichiers d√©claratifs, qui repr√©sentent l'√©tat souhait√© du syst√®me, doivent √™tre stock√©s dans un repository Git. Git offre non seulement de puissantes capacit√©s de contr√¥le de version, mais pr√©serve √©galement l'immuabilit√©. Stocker l'√©tat souhait√© dans Git en fait la source d√©finitive de v√©rit√© pour la configuration du syst√®me. Toute modification envoy√©e √† Git est automatiquement reconnue et appliqu√©e √† travers le syst√®me.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#3-application-automatisee-de-letat-souhaite-via-les-operateurs-gitops","level":4,"title":"3. Application automatis√©e de l'√©tat souhait√© via les op√©rateurs GitOps","text":"<p>Le troisi√®me principe consiste √† utiliser des GitOps operators, des agents logiciels qui surveillent en continu Git pour d√©tecter des mises √† jour. Une fois qu'ils d√©tectent des changements, ces op√©rateurs r√©cup√®rent automatiquement l'√©tat souhait√© depuis le repository et l'appliquent √† un ou plusieurs clusters ou environnements. Voici un exemple de manifeste de d√©ploiement qu'un op√©rateur GitOps pourrait g√©rer :</p> deployment.yaml<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n</code></pre> <p>Cet op√©rateur peut fonctionner dans un seul cluster et propager les changements de configuration vers d'autres clusters si n√©cessaire, garantissant ainsi l'uniformit√© et la scalabilit√©.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#4-reconciliation-et-self-healing","level":4,"title":"4. R√©conciliation et Self-Healing","text":"<p>Le dernier principe se concentre sur la r√©conciliation continue. Les op√©rateurs GitOps maintiennent un syst√®me de Self-Healing en v√©rifiant constamment les √©carts entre l'√©tat actuel du syst√®me et l'√©tat souhait√© stock√© dans Git. Ils ex√©cutent ce processus en trois √©tapes cl√©s :</p> <ul> <li>Observe: Surveiller un repository Git pour les mises √† jour.</li> <li>Diff: Comparer l'√©tat souhait√© depuis Git avec l'√©tat actuel du cluster.</li> <li>Act: R√©concilier les diff√©rences en mettant √† jour le syst√®me pour refl√©ter l'√©tat souhait√© d√©clar√©.</li> </ul> <p>Ce cycle de r√©conciliation continu minimise les risques de configuration drift et aide √† maintenir un syst√®me robuste et r√©sistant aux erreurs.</p> <p>En comprenant et en appliquant ces principes GitOps, vous pouvez garantir que votre infrastructure reste coh√©rente, scalable et r√©siliente face aux changements.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#gitops-introduction","level":2,"title":"GitOps Introduction","text":"","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#devops-vs-gitops","level":3,"title":"DevOps vs GitOps","text":"<p>Cette le√ßon explore les approches contrast√©es de DevOps et GitOps ‚Äî deux m√©thodologies qui partagent des objectifs communs mais diff√®rent de mani√®re significative dans leur ex√©cution et leurs outils.</p> <p>GitOps utilise des technologies de containerisation telles que OpenShift et Kubernetes. Il se base sur Git comme la source unique de v√©rit√© pour l'infrastructure et les d√©ploiements. En comparaison, DevOps est une m√©thodologie plus large qui peut √™tre appliqu√©e √† divers environnements d'applications et workflows.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#pipeline-devops","level":4,"title":"Pipeline DevOps","text":"<p>Un pipeline DevOps typique fonctionne comme suit :</p> <ol> <li>Un d√©veloppeur √©crit du code dans un Integrated Development Environment (IDE) et le commit dans un syst√®me de gestion de code source.</li> <li>Un processus de Continuous Integration (CI) d√©tecte le commit, ex√©cute des tests et construit les artefacts n√©cessaires.</li> <li>Le pipeline cr√©e ensuite une image de conteneur et la publie dans un container repository.</li> <li>Enfin, le processus de Continuous Deployment (CD) se connecte √† un Kubernetes cluster et utilise un outil en ligne de commande tel que kubectl (avec des commandes imp√©ratives) pour pousser les mises √† jour directement vers le cluster.</li> </ol> <p>Point cl√©</p> <p>Dans DevOps, le d√©ploiement est initi√© par l'envoi des changements directement dans le cluster.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#pipeline-gitops","level":4,"title":"Pipeline GitOps","text":"<p>Bien que les processus de CI dans un pipeline GitOps ressemblent √† ceux de DevOps jusqu'√† la publication de l'image de conteneur, le processus de d√©ploiement est distinct :</p> <ol> <li>Deux Git repositories s√©par√©s sont maintenus : l'un d√©di√© au code de l'application et l'autre aux Kubernetes manifests.</li> <li>Une fois l'image publi√©e, le manifest repository est clon√© et mis √† jour ‚Äî g√©n√©ralement le nom de la nouvelle image est sp√©cifi√©. Ces changements sont ensuite commit et pouss√©s.</li> <li>Le pipeline cr√©e automatiquement une pull request pour le manifest repository. Un membre de l'√©quipe examine la pull request, sugg√®re des ajustements si n√©cessaire, et merge les changements apr√®s approbation.</li> <li>Un GitOps operator, fonctionnant dans le Kubernetes cluster, surveille en continu le repository. Lorsque des changements sont d√©tect√©s, il synchronise l'√©tat du cluster pour correspondre √† la configuration du repository.</li> </ol> <p>√Ä</p> <p>Dans un pipeline GitOps, l'operator de d√©ploiement tire les changements depuis le repository et les applique au cluster, contrairement √† l'approche push des workflows DevOps traditionnels.</p> <p>L'image ci-dessous illustre une comparaison c√¥te √† c√¥te des pipelines CI/CD dans DevOps et GitOps. Elle met en √©vidence les √©tapes cl√©s, allant du d√©veloppement du code au d√©ploiement, et souligne les diff√©rences dans la gestion et l'application des mises √† jour.</p> <p>L'image compare les pipelines CI/CD de DevOps et GitOps, illustrant les processus d'int√©gration continue et de d√©ploiement pour chaque approche. Elle met en √©vidence les √©tapes du d√©veloppement du code au d√©ploiement, montrant les diff√©rences dans la gestion et l'application des mises √† jour.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#conclusion","level":3,"title":"Conclusion","text":"<p>Cet article a compar√© les pipelines DevOps et GitOps en d√©taillant les √©tapes cl√©s de chaque processus. Comprendre ces diff√©rences est essentiel pour choisir la bonne m√©thodologie pour vos projets. Dans la prochaine le√ßon, nous explorerons les avantages et les d√©fis associ√©s √† chaque approche.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#push-vs-pull-deployment","level":3,"title":"Push vs Pull Deployment","text":"<p>Dans cet article, nous explorons les diff√©rences entre les strat√©gies de d√©ploiement push-based et pull-based pour les Kubernetes clusters. Nous examinerons leurs avantages, d√©fis et cas d'utilisation, afin de vous aider √† d√©terminer la meilleure approche pour votre environnement.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#push-based-deployment","level":4,"title":"Push-Based Deployment","text":"<p>Le push-based deployment est couramment utilis√© dans les pipelines CI/CD. Avec cette approche, le code de l'application passe par diff√©rentes √©tapes dans le pipeline CI avant que les mises √† jour ne soient pouss√©es directement vers le Kubernetes cluster.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#caracteristiques-cles","level":5,"title":"Caract√©ristiques cl√©s","text":"<ul> <li>Le syst√®me CI n√©cessite un acc√®s en lecture-√©criture au Kubernetes cluster, ce qui signifie que les informations d'identification Kubernetes sont stock√©es dans le syst√®me CI, en dehors du cluster. Cette organisation peut introduire des risques potentiels de s√©curit√©.</li> <li>En g√©n√©ral, le syst√®me CI a un acc√®s en lecture seule au Git repository et un acc√®s en lecture-√©criture au container registry, tandis que le Kubernetes cluster lui-m√™me n'a qu'un acc√®s en lecture seule au registre.</li> <li>Les d√©ploiements peuvent tirer parti de divers plugins et outils. Par exemple, Jenkins peut utiliser plusieurs plugins ou approches, et les plugins Helm simplifient davantage le d√©ploiement des Helm charts.</li> </ul> <p>Warning</p> <p>Le stockage des informations d'identification Kubernetes dans le syst√®me CI expose un risque de s√©curit√© potentiel, car ces informations permettent un acc√®s en lecture-√©criture au cluster.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#challenges","level":5,"title":"Challenges","text":"<ul> <li>La configuration du d√©ploiement est √©troitement li√©e au syst√®me CI. Migrer d'une plateforme CI √† une autre (par exemple, passer de Jenkins √† une autre plateforme) n√©cessite souvent de retravailler de nombreuses configurations de d√©ploiement.</li> <li>L'int√©gration des informations d'identification du cluster dans le syst√®me CI augmente le risque d'acc√®s non autoris√© si le syst√®me CI est compromis.</li> </ul>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#pull-based-deployment","level":4,"title":"Pull-Based Deployment","text":"<p>Le pull-based deployment, souvent associ√© √† GitOps, utilise un operator fonctionnant au sein du Kubernetes cluster. Cet operator surveille les changements ‚Äî soit dans un container registry pour les nouvelles images, soit dans un Git repository pour les manifests mis √† jour ‚Äî et d√©ploie ensuite ces changements de mani√®re autonome.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#caracteristiques-cles_1","level":5,"title":"Caract√©ristiques cl√©s","text":"<ul> <li>Le syst√®me CI/CD n'a besoin que d'un acc√®s en lecture-√©criture au container registry, sans n√©cessiter d'acc√®s direct au Kubernetes cluster.</li> <li>Les d√©ploiements sont ex√©cut√©s en interne depuis le cluster, am√©liorant la s√©curit√© en minimisant l'acc√®s externe.</li> <li>Les GitOps operators sont particuli√®rement utiles dans les environnements multi-tenant, permettant aux √©quipes de g√©rer plusieurs repositories et namespaces. Par exemple, diff√©rentes √©quipes peuvent maintenir des Git repositories distincts et des namespaces correspondants pour leurs d√©ploiements.</li> <li>Les secrets peuvent √™tre g√©r√©s de mani√®re s√©curis√©e en les cryptant √† l'aide d'outils comme HashiCorp Vault ou Bitnami Sealed Secrets. Ces secrets crypt√©s sont stock√©s dans Git ou d√©crypt√©s lors du processus de d√©ploiement.</li> <li>Les GitOps operators peuvent surveiller les container registries pour les nouvelles versions d'images et d√©clencher automatiquement le d√©ploiement des derni√®res images.</li> </ul> <p>Gestion des secrets</p> <p>Bien que GitOps encourage la gestion d√©clarative, y compris des secrets, dans Git, le processus n√©cessite souvent des outils et des √©tapes suppl√©mentaires (par exemple, la cryptographie et le d√©cryptage) pour garantir la s√©curit√©, en particulier pour les d√©ploiements Helm charts.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#challenges_1","level":5,"title":"Challenges","text":"<p>La gestion des secrets et des configurations peut √™tre plus complexe par rapport au mod√®le push-based. Bien que les principes de GitOps promeuvent une approche d√©clarative, la gestion des informations d'identification crypt√©es ajoute une couche de complexit√© suppl√©mentaire.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#comparaison-visuelle","level":4,"title":"Comparaison visuelle","text":"<p>L'image compare les m√©thodes de d√©ploiement push-based et pull-based pour Kubernetes, en mettant en √©vidence leurs processus, permissions d'acc√®s, avantages et inconv√©nients.</p> <p>L'image compare les m√©thodes de d√©ploiement push-based et pull-based pour Kubernetes, illustrant les processus d'int√©gration continue et de d√©ploiement pour chaque approche. Elle met en √©vidence les √©tapes allant du d√©veloppement du code au d√©ploiement, montrant les diff√©rences dans la gestion et l'application des mises √† jour.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#resume","level":4,"title":"R√©sum√©","text":"Strat√©gie de d√©ploiement Avantages Inconv√©nients Push-Based - Int√©gration directe avec les pipelines CI/CD- Configuration de d√©ploiement flexible en utilisant divers outils et plugins - N√©cessite que le syst√®me CI ait des informations d'identification du cluster- Li√© √©troitement aux syst√®mes CI, ce qui rend les migrations difficiles Pull-Based (GitOps) - S√©curit√© am√©lior√©e en limitant l'acc√®s externe- Prise en charge des environnements multi-tenant et des mises √† jour automatiques des images - Gestion des secrets plus complexe- Outils suppl√©mentaires n√©cessaires pour la cryptographie et le d√©cryptage des configurations <p>En r√©sum√©, les strat√©gies de d√©ploiement push-based simplifient certains aspects de l'automatisation mais peuvent entra√Æner des probl√®mes de flexibilit√© et de s√©curit√© potentiels. En revanche, les d√©ploiements pull-based (GitOps) am√©liorent la gestion interne et la s√©curit√© au prix d'une complexit√© accrue dans la gestion des secrets et des configurations.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#ensemble-de-fonctionnalites-gitops","level":3,"title":"Ensemble de fonctionnalit√©s GitOps","text":"<p>Cet article fournit une vue d√©taill√©e des fonctionnalit√©s GitOps et de leurs cas d‚Äôusage associ√©s, d√©montrant comment le fait de stocker chaque configuration de mani√®re d√©clarative dans un d√©p√¥t Git peut transformer vos workflows de d√©ploiement.</p> <p>Chaque configuration est stock√©e de fa√ßon d√©clarative dans Git, qui sert de single source of truth contenant l‚Äô√©tat d√©sir√© complet du syst√®me. Cette approche simplifie non seulement les application rollbacks ‚Äî permettant une r√©cup√©ration rapide via un simple git revert ‚Äî mais garantit √©galement que les audit trails sont automatiquement disponibles via les pull requests et l‚Äôhistorique des commits.</p> <p>Avantage cl√©</p> <ul> <li>Stocker les configurations dans Git permet aux √©quipes de revenir facilement √† un √©tat pr√©c√©dent.</li> <li>Maintien d‚Äôune trace compl√®te (audit trail) pour tous les changements.</li> </ul>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#automatisation-cicd-et-deploiement-continu","level":4,"title":"Automatisation CI/CD et d√©ploiement continu","text":"<p>L‚Äôautomatisation CI/CD est un pilier fondamental de GitOps. En tirant parti de cette automatisation :</p> <ul> <li>Les t√¢ches de build, de test et de d√©ploiement sont d√©clench√©es automatiquement en fonction de l‚Äô√©tat d√©sir√© stock√© dans Git.</li> <li>Le continuous deployment devient fluide et coh√©rent : les applications sont d√©ploy√©es automatiquement sur les clusters sans intervention manuelle.</li> </ul>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#etendre-gitops-a-linfrastructure-et-aux-ressources-de-cluster","level":4,"title":"√âtendre GitOps √† l‚Äôinfrastructure et aux ressources de cluster","text":"<p>Une fois GitOps √©tabli pour le d√©ploiement applicatif, ces pratiques peuvent √™tre √©tendues √† la gestion des ressources de cluster et de l‚ÄôInfrastructure as Code. Par exemple, dans des environnements Kubernetes, il est possible de g√©rer diff√©rentes ressources, notamment :</p> <ul> <li>Secrets management</li> <li>Networking agents et configurations de service mesh</li> <li>Database provisioning</li> <li>Prometheus monitoring</li> </ul> <p>Le principe central ici est l‚Äôautomatic reconciliation : le syst√®me compare continuellement l‚Äô√©tat d√©sir√© dans Git avec l‚Äô√©tat r√©el dans le cluster. Si des changements non d√©sir√©s apparaissent, le syst√®me les annule automatiquement afin d‚Äôassurer la coh√©rence.</p> <p>Automatic Reconciliation</p> <ul> <li>GitOps compare continuellement l‚Äô√©tat d√©sir√© d√©fini dans Git avec l‚Äô√©tat r√©el au runtime.</li> <li>Tout drift est d√©tect√© et automatiquement annul√© pour maintenir l‚Äôalignement de l‚Äôinfrastructure.</li> </ul>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#detection-et-prevention-du-configuration-drift","level":4,"title":"D√©tection et pr√©vention du configuration drift","text":"<ul> <li>La d√©tection pr√©coce du configuration drift permet de r√©soudre les incoh√©rences d√®s leur apparition.</li> <li>Agir rapidement √©vite que de petites divergences se transforment en probl√®mes majeurs.</li> <li>Cette posture proactive distingue GitOps des autres m√©thodologies de d√©ploiement.</li> </ul>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#deploiement-multi-clusters-simplifie","level":4,"title":"D√©ploiement multi-clusters simplifi√©","text":"<p>G√©rer plusieurs clusters, en particulier r√©partis g√©ographiquement, peut √™tre complexe. GitOps simplifie ce processus en centralisant l‚Äô√©tat des clusters dans Git. Cela signifie :</p> <ul> <li>Un seul operator peut d√©ployer des applications sur plusieurs clusters.</li> <li>Pas besoin d‚Äôinstaller ou de configurer l‚Äôoperator individuellement sur chaque cluster.</li> <li>Le processus de d√©ploiement est rationalis√© et significativement plus efficace.</li> </ul> <p>L‚Äôimage illustre un ensemble de fonctionnalit√©s GitOps et des cas d‚Äôusage, pr√©sentant un workflow impliquant des outils comme Helm et Jenkins, des d√©p√¥ts Git et des clusters Kubernetes pour le continuous deployment et l‚Äôautomatisation. Elle met en avant des concepts tels que single source of truth, everything as code, auditable processes et multi-cluster deployments.</p> <p>En adoptant GitOps, les √©quipes peuvent atteindre un haut niveau d‚Äôefficacit√© de d√©ploiement, une meilleure gestion √† travers des environnements vari√©s et des m√©canismes de r√©cup√©ration robustes, faisant de cette pratique une strat√©gie essentielle pour la gestion d‚Äôinfrastructures modernes.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#avantages-et-inconvenients-de-gitops","level":3,"title":"Avantages et inconv√©nients de GitOps","text":"<p>Cet article passe en revue les principaux avantages et d√©fis associ√©s √† GitOps, fournissant des √©l√©ments pour g√©rer efficacement les d√©ploiements d‚Äôapplications Kubernetes.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#avantages-de-gitops","level":4,"title":"Avantages de GitOps","text":"<p>GitOps offre plusieurs avantages convaincants :</p> <ul> <li>Il est l√©ger et vendor-neutral, s‚Äôappuyant sur le protocole Git open source pour fonctionner de mani√®re transparente sur des plateformes vari√©es.</li> <li>GitOps permet des d√©ploiements plus rapides et plus s√ªrs en garantissant des environnements immuables et reproductibles.</li> <li>Dans des √©quipes o√π des changements d‚Äôenvironnement peuvent survenir de mani√®re inattendue, GitOps emp√™che les modifications non d√©sir√©es. The GitOps operator impose la coh√©rence en interdisant les mises √† jour manuelles, √©liminant ainsi le configuration drift.</li> <li>En cas de mise √† jour manuelle, the GitOps operator restaure automatiquement l‚Äô√©tat d√©sir√© depuis Git.</li> <li>Les d√©veloppeurs b√©n√©ficient de la familiarit√© de Git et des outils CI/CD. Le workflow reste simple : push the code to the repository, et un pipeline CI/CD g√®re les tests et le d√©ploiement.</li> <li>L‚Äôhistorique Git permet de comparer facilement les r√©visions de fichiers d√©claratifs, facilitant la corr√©lation des changements avec des change requests sp√©cifiques.</li> </ul> <p>Note</p> <p>Pour plus de d√©tails sur les int√©grations CI/CD avec GitOps, consultez la GitOps Documentation officielle.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#defis-de-gitops","level":4,"title":"D√©fis de GitOps","text":"<p>Malgr√© ses avantages, GitOps introduit quelques d√©fis √† adresser :</p> <ul> <li>Centralized Secret Management : GitOps ne s√©curise pas les secrets par d√©faut. Bien qu‚Äôil recommande de stocker les secrets de mani√®re d√©clarative dans les repositories Git, les √©quipes ops doivent int√©grer des outils suppl√©mentaires pour g√©rer les secrets de fa√ßon s√©curis√©e.</li> <li>Repository Organization : √Ä mesure que le nombre de microservices et d‚Äôenvironnements augmente, l‚Äôorganisation des repositories Git devient complexe. Il faut d√©cider s‚Äôil convient de stocker le source code et les manifests dans un seul repository ou d‚Äôutiliser plusieurs repositories/branches. Il n‚Äôexiste pas de solution universelle ‚Äî chaque organisation doit adapter cette approche √† ses besoins applicatifs.</li> <li>Update Conflicts : Des mises √† jour fr√©quentes en environnement de continuous delivery peuvent d√©clencher des processus CI simultan√©s, conduisant √† plusieurs pull requests. Cela peut provoquer des conflits lorsque plusieurs processus tentent de mettre √† jour le GitOps repository en concurrence, n√©cessitant souvent une r√©solution manuelle.</li> <li>Governance and Policy Enforcement : S‚Äôappuyer sur les pull requests (PRs) pour l‚Äôapprobation peut r√©duire l‚Äôefficacit√© de l‚Äôapplication stricte des politiques d‚Äôentreprise apr√®s approbation d‚Äôune PR.</li> <li>Configuration Validation : Des fichiers YAML mal form√©s ou des erreurs de configuration peuvent survenir. Des outils externes de validation sont essentiels pour garantir que les manifest files respectent les standards requis.</li> </ul> <p>Warning</p> <p>Assurez-vous d‚Äôint√©grer une gestion robuste des secrets et une strat√©gie claire d‚Äôorganisation des repositories lors de la mise en place de GitOps afin d‚Äôatt√©nuer efficacement ces d√©fis.</p> <p>L‚Äôimage liste les avantages et les d√©fis de GitOps, mettant en √©vidence des points tels que sa l√©g√®ret√© et son ind√©pendance vis-√†-vis des fournisseurs, ainsi que des enjeux comme la gestion des secrets et la multiplication des Git repositories.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#projets-et-outils-gitops","level":3,"title":"Projets et outils GitOps","text":"<p>Dans cet article, nous explorons une vari√©t√© de projets et outils GitOps disponibles √† ce jour. Ces solutions sont con√ßues pour simplifier la gestion des applications Kubernetes en tirant parti des pratiques GitOps √† travers diff√©rents controllers et outils d‚Äôautomatisation.</p> <p>Aper√ßu</p> <p>Ce guide fournit des informations sur les controllers GitOps ainsi que sur les outils compl√©mentaires qui am√©liorent le d√©ploiement et la gestion des applications Kubernetes.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#gitops-controller-argocd","level":4,"title":"GitOps Controller : ArgoCD","text":"<p>ArgoCD est notre principal GitOps controller. Il s‚Äôagit d‚Äôun outil de d√©ploiement continu d√©claratif pour Kubernetes qui simplifie la gestion des applications tout en garantissant que le processus de d√©ploiement reste automatis√© et coh√©rent.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#outils-gitops-supplementaires","level":4,"title":"Outils GitOps suppl√©mentaires","text":"<p>Am√©liorez vos workflows GitOps pour Kubernetes avec ces outils suppl√©mentaires :</p> <ul> <li>Atlantis : Automatise les workflows Terraform en s‚Äôint√©grant directement aux pull requests.</li> <li>AutoApply : Applique automatiquement les modifications de configuration depuis un repository Git vers votre cluster Kubernetes, r√©duisant l‚Äôintervention manuelle.</li> <li>CloudRollout : Fournit un syst√®me avanc√© de feature flagging, permettant aux √©quipes de d√©ployer et it√©rer rapidement sans compromettre la s√©curit√©.</li> <li>GitOps avec FluxCD : Offre des solutions de livraison continue et progressive optimis√©es pour les environnements Kubernetes.</li> <li>Helm Operator : Automatise le d√©ploiement des Helm charts selon les principes GitOps.</li> <li>Flagger : Un operator Kubernetes sp√©cialis√© dans la livraison progressive. Il prend en charge les canary releases, les tests A/B et les d√©ploiements blue-green.</li> <li>Ignite : Fonctionne comme un gestionnaire de machines virtuelles avec une exp√©rience utilisateur proche des containers, int√©grant des fonctionnalit√©s GitOps.</li> <li>Faros : Un GitOps controller utilisant les Custom Resource Definitions (CRDs) pour simplifier les op√©rations.</li> <li>GitKube : Facilite la construction d‚Äôimages Docker et leur d√©ploiement sur des clusters Kubernetes via un workflow Git push.</li> <li>Jenkins X : Con√ßu pour Kubernetes, cette plateforme CI/CD offre l‚Äôautomatisation des pipelines avec GitOps int√©gr√© et des environnements de preview.</li> <li>KubeStack : Utilise Terraform pour fournir un framework GitOps pour les distributions cloud Kubernetes comme AKS, GKE et EKS, avec des exemples CI/CD.</li> <li>Weave Cloud : Une plateforme d‚Äôautomatisation et de gestion supportant √† la fois les √©quipes de d√©veloppement et DevOps.</li> <li>PipeCD : Une solution de continuous delivery pour Kubernetes d√©claratif, serverless et applications d‚Äôinfrastructure.</li> </ul> <p>L‚Äôimage liste ces diff√©rents projets et outils GitOps, chacun repr√©sent√© par un logo et un nom, tels que ArgoCD, FluxCD et JenkinsX.</p> <p>Pour aller plus loin</p> <p>Pour approfondir vos connaissances sur Kubernetes et les pratiques GitOps, vous pouvez consulter des ressources telles que la documentation Kubernetes et Docker Hub.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#argocd-basics","level":2,"title":"ArgoCD Basics","text":"","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#whatwhyhow-argocd","level":3,"title":"WhatWhyHow ArgoCD","text":"<p>Cette le√ßon explore ArgoCD, en d√©taillant ce que c‚Äôest, pourquoi l‚Äôutiliser, et comment il fonctionne pour transformer votre workflow de continuous delivery.</p> <p>ArgoCD utilise des sp√©cifications d√©claratives et la gestion de configuration bas√©e sur Git, offrant des avantages significatifs pour le continuous delivery. Il constitue un √©l√©ment central pour atteindre des op√©rations continues gr√¢ce √† la combinaison de monitoring, d‚Äôanalyses et de rem√©diation automatis√©e, ce qui le rend id√©al pour les environnements d‚Äôentreprise. Ses fonctionnalit√©s avanc√©es telles que l‚Äôauditabilit√©, la conformit√©, la s√©curit√©, le RBAC et le SSO renforcent encore son attrait.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#quest-ce-quargocd","level":4,"title":"Qu‚Äôest-ce qu‚ÄôArgoCD ?","text":"<p>ArgoCD est un outil GitOps de continuous delivery con√ßu pour Kubernetes. Il consid√®re un repository Git comme la single source of truth pour l‚Äô√©tat d√©sir√© de vos applications. En surveillant continuellement les applications en cours d‚Äôex√©cution, ArgoCD compare l‚Äô√©tat actuel avec l‚Äô√©tat d√©sir√© stock√© dans Git. En cas de divergences, il signale ces diff√©rences et fournit des informations visuelles, permettant aux d√©veloppeurs de synchroniser l‚Äô√©tat live avec la configuration d√©sir√©e, soit manuellement, soit automatiquement.</p> <p>Point</p> <p>ArgoCD simplifie la gestion des ressources Kubernetes en garantissant que l‚Äô√©tat live de votre application refl√®te toujours la configuration d√©finie dans votre repository Git.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#comment-fonctionne-argocd","level":4,"title":"Comment fonctionne ArgoCD ?","text":"<p>ArgoCD suit le mod√®le GitOps en utilisant les repositories Git comme r√©f√©rence pour l‚Äô√©tat d√©sir√© de vos applications et pour l‚Äôenvironnement de d√©ploiement cible. Cette approche transparente et coh√©rente rend les processus de d√©ploiement fiables et facilement auditable.</p> <p>L‚Äôimage est une infographie expliquant ArgoCD, un outil GitOps de continuous delivery pour Kubernetes, d√©taillant ce que c‚Äôest, pourquoi l‚Äôutiliser et comment il fonctionne. Elle met en avant ses avantages, tels que les sp√©cifications d√©claratives, le monitoring continu et les fonctionnalit√©s adapt√©es aux entreprises.</p> <p>ArgoCD prend en charge une vari√©t√© de manifests Kubernetes. Que vous travailliez avec des applications personnalis√©es, des Helm charts, des fichiers JSON ou YAML, ArgoCD automatise le processus de synchronisation pour garantir que l‚Äô√©tat des applications d√©ploy√©es dans tous les environnements cibles correspond toujours √† celui d√©fini dans Git.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#concepts-et-terminologie","level":3,"title":"Concepts et terminologie","text":"<p>Avant d‚Äôexplorer l‚Äôarchitecture d‚ÄôArgoCD, il est essentiel de comprendre ses concepts cl√©s et sa terminologie. Une bonne connaissance de Git, Docker, Kubernetes, des principes CI/CD et de GitOps est fortement recommand√©e pour tirer le meilleur parti d‚ÄôArgoCD.</p> <p>Recommandation</p> <p>Avant de commencer, assurez-vous de bien ma√Ætriser les technologies fondamentales mentionn√©es ci-dessus afin de faciliter l‚Äôint√©gration avec ArgoCD.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#concepts-cles-dargocd","level":4,"title":"Concepts cl√©s d‚ÄôArgoCD","text":"","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#argocd-applications","level":5,"title":"ArgoCD Applications","text":"<p>Dans ArgoCD, vous travaillez principalement avec des objets appel√©s ArgoCD applications. Ces applications sont des Custom Resource Definitions (CRD) install√©es avec ArgoCD qui d√©finissent √† la fois la source (repository Git) et la destination (cluster Kubernetes) de vos ressources Kubernetes.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#source-types","level":5,"title":"Source Types","text":"<p>Chaque ArgoCD application est associ√©e √† un source type qui identifie l‚Äôoutil ou la m√©thode utilis√©e pour construire l‚Äôapplication.</p> <ul> <li>Helm et Kustomize sont des exemples courants de source types.</li> <li>Cette approche modulaire permet d‚Äôutiliser diff√©rents outils selon votre strat√©gie de d√©ploiement.</li> </ul>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#argocd-projects","level":5,"title":"ArgoCD Projects","text":"<p>Un ArgoCD project agit comme un regroupement logique d‚Äôapplications, facilitant la gestion des ressources, surtout lorsque plusieurs √©quipes sont impliqu√©es. Regrouper les applications li√©es sous un m√™me projet simplifie l‚Äôapplication des politiques et la segmentation des ressources.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#target-state-vs-live-state","level":5,"title":"Target State vs Live State","text":"<ul> <li>Target State : La configuration d√©sir√©e stock√©e dans votre repository Git.</li> <li>Live State : L‚Äô√©tat actuel des ressources d√©ploy√©es (pods, secrets, config maps, etc.) dans votre cluster Kubernetes.</li> </ul> <p>ArgoCD compare continuellement ces deux √©tats.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#sync-operation","level":5,"title":"Sync Operation","text":"<p>Lorsque vous cr√©ez une ArgoCD application, l‚Äôoutil synchronise le Target State (depuis Git) avec le Live State (dans le cluster). Ce processus, appel√© sync, r√©concilie la configuration actuelle du cluster avec la version sp√©cifi√©e dans Git. Par exemple, si des modifications sont apport√©es dans le repository Git, une op√©ration de sync met √† jour le cluster Kubernetes en cons√©quence.</p> <ul> <li>Sync Status : Indique si le Live State correspond au Target State.</li> <li>Operation Status : Montre si une op√©ration de sync s‚Äôest termin√©e avec succ√®s.</li> </ul>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#refresh-operation","level":5,"title":"Refresh Operation","text":"<p>Une op√©ration de refresh dans ArgoCD r√©√©value le code le plus r√©cent dans Git par rapport au Live State actuel. Elle :</p> <ul> <li>Identifie les diff√©rences √©ventuelles.</li> <li>Peut automatiquement initier un sync ou inviter un administrateur √† le d√©clencher manuellement.</li> </ul>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#health-assessments","level":5,"title":"Health Assessments","text":"<p>ArgoCD inclut des √©valuations de sant√© int√©gr√©es pour les ressources Kubernetes standards. Ces √©valuations fournissent un √©tat de sant√© global de vos applications, assurant une visibilit√© sur leur √©tat op√©rationnel et leur conformit√© √† la configuration.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#features","level":3,"title":"Features","text":"<p>ArgoCD se distingue en rationalisant le d√©ploiement, la surveillance et la gestion des applications √† travers plusieurs clusters. Voici un aper√ßu de ses principales fonctionnalit√©s :</p> <ul> <li>D√©ploiement automatique : D√©ploie continuellement les applications vers les cibles d√©sign√©es √† travers les clusters, garantissant coh√©rence et mises √† jour rapides.</li> <li>Tra√ßabilit√© compl√®te : Offre une visibilit√© totale gr√¢ce √† une piste d‚Äôaudit d√©taill√©e des √©v√©nements applicatifs et des appels API.</li> <li>Int√©grations SSO : S√©curise votre infrastructure avec des int√©grations single sign-on via GitHub, GitLab, Microsoft, LinkedIn, et d'autres plateformes.</li> <li>Synchronisation via Webhooks : R√©agit imm√©diatement aux push events en synchronisant les applications via des webhooks provenant de GitHub, Bitbucket ou GitLab.</li> <li>Capacit√©s de rollback : Permet de revenir facilement √† toute configuration ant√©rieure stock√©e dans votre d√©p√¥t Git en cas de probl√®me.</li> <li>Interface Web intuitive : Permet de surveiller les activit√©s applicatives en temps r√©el via une interface simple et conviviale.</li> <li>D√©tection et visualisation du drift : D√©tecte automatiquement les √©carts entre l‚Äô√©tat d√©sir√© et l‚Äôenvironnement r√©el, avec des outils visuels pour identifier les diff√©rences.</li> <li>Monitoring int√©gr√© : Exploite les m√©triques Prometheus pr√™tes √† l‚Äôemploi, visualisables dans Grafana pour une surveillance avanc√©e.</li> <li>Gestion flexible des configurations : Supporte plusieurs outils de configuration tels que Kustomize, Helm et les fichiers YAML simples selon vos besoins de d√©ploiement.</li> <li>Strat√©gies de d√©ploiement avanc√©es : Am√©liore les d√©ploiements gr√¢ce aux hooks pre-sync, sync et post-sync, offrant un support pour les strat√©gies blue-green et canary.</li> <li>Support multi-tenant : Permet d‚Äôimpl√©menter des politiques RBAC personnalis√©es pour une gestion s√©curis√©e des r√¥les dans des environnements multi-locataires.</li> <li>CLI robuste et automatisation : Propose une CLI compl√®te et un support des tokens d‚Äôacc√®s pour une int√©gration fluide avec les workflows CI.</li> <li>Analyse de l‚Äô√©tat de sant√© : Fournit une compr√©hension d√©taill√©e de la sant√© de toutes les ressources applicatives gr√¢ce √† des v√©rifications int√©gr√©es et personnalisables.</li> <li>Synchronisation personnalisable : Autorise le choix entre une synchronisation automatique ou manuelle afin de maintenir l‚Äô√©tat d√©sir√© des applications.</li> </ul> <p>Note</p> <p>La richesse fonctionnelle d‚ÄôArgoCD simplifie des workflows de d√©ploiement complexes, le rendant hautement scalable et id√©al pour les organisations de toutes tailles.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#architecture","level":3,"title":"Architecture","text":"<p>Cet article fournit un aper√ßu complet de l‚Äôarchitecture d‚ÄôArgoCD, d√©taillant son int√©gration avec Kubernetes et son puissant workflow GitOps. En tirant parti d‚ÄôArgoCD, vous pouvez g√©rer de mani√®re fluide vos applications et votre infrastructure, en veillant √† ce que l‚Äôenvironnement live soit toujours align√© avec l‚Äô√©tat d√©sir√© d√©fini dans vos d√©p√¥ts Git.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#fonctionnalites-cles-et-capacites","level":4,"title":"Fonctionnalit√©s cl√©s et capacit√©s","text":"<p>ArgoCD fonctionne comme un Kubernetes controller une fois install√© dans votre environnement Kubernetes. Il fournit une interface robuste via une command-line interface (CLI) et une web-based UI, vous permettant de :</p> <ul> <li>Cr√©er et g√©rer des applications ArgoCD</li> <li>Organiser et maintenir des projets efficacement</li> <li>Int√©grer le single sign-on (SSO) avec des fournisseurs externes</li> <li>Ajuster finement les options de synchronisation pour les d√©ploiements applicatifs</li> </ul> <p>Comment</p> <p>ArgoCD surveille continuellement les applications d√©ploy√©es en comparant l‚Äô√©tat en cours d‚Äôex√©cution avec l‚Äô√©tat d√©sir√© stock√© dans votre d√©p√¥t Git. Lorsque des changements sont commit dans le d√©p√¥t, ArgoCD r√©cup√®re et applique automatiquement ces mises √† jour, assurant que vos environnements restent coh√©rents.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#workflow-gitops-et-deploiement-multi-cluster","level":4,"title":"Workflow GitOps et d√©ploiement multi-cluster","text":"<p>L‚Äôapproche GitOps d‚ÄôArgoCD garantit que toute modification dans le d√©p√¥t Git d√©clenche une mise √† jour automatis√©e des environnements cibles. Vous pouvez √©galement configurer un webhook sur votre d√©p√¥t Git pour alerter ArgoCD d‚Äô√©v√©nements sp√©cifiques, entra√Ænant une synchronisation rapide.</p> <p>Cette conception prend en charge les mod√®les de d√©ploiement single-cluster et multi-cluster, vous permettant de connecter et d√©ployer des ressources √† travers plusieurs environnements Kubernetes tels que development, staging et production.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#api-et-integration","level":4,"title":"API et int√©gration","text":"<p>Le c≈ìur des fonctionnalit√©s d‚ÄôArgoCD r√©side dans son API server, impl√©ment√© comme un gRPC REST server. Cette API est accessible :</p> <ul> <li>Via l‚Äôinterface utilisateur Web (UI)</li> <li>Via la command-line interface (CLI)</li> <li>Par les syst√®mes CI/CD</li> </ul> <p>Cette structure API flexible permet une int√©gration simple avec une vari√©t√© d‚Äôoutils et workflows, soutenant une automatisation et une orchestration fluides de vos t√¢ches d‚Äôinfrastructure.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#monitoring-et-observabilite","level":4,"title":"Monitoring et observabilit√©","text":"<p>ArgoCD fournit √©galement un service de notification out-of-the-box comprenant plusieurs triggers et des mod√®les de messages personnalisables. Ce service peut envoyer des alertes √† diverses plateformes tierces comme Slack, email et GitHub. De plus, il expose une suite de m√©triques Prometheus que vous pouvez visualiser avec Grafana, am√©liorant consid√©rablement le monitoring et l‚Äôobservabilit√© globale du syst√®me.</p> <p>Monitoring</p> <p>Assurez-vous de configurer Prometheus et Grafana pour tirer pleinement parti des capacit√©s de monitoring d‚ÄôArgoCD, vous permettant d‚Äôobtenir des informations critiques sur les performances des applications et les √©v√©nements de synchronisation.</p> <p>L‚Äôimage illustre l‚Äôarchitecture d‚ÄôArgoCD, montrant le workflow de GitHub vers divers clusters Kubernetes (prod, dev, staging) avec des interactions via UI, CLI et gRPC/REST, et des notifications envoy√©es vers des plateformes comme Teams, Gmail et Slack.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#resume_1","level":4,"title":"R√©sum√©","text":"<p>ArgoCD simplifie le d√©ploiement applicatif en int√©grant les principes GitOps avec Kubernetes. Sa capacit√© √† surveiller, automatiser et alerter √† travers plusieurs clusters en fait un outil puissant pour les pipelines de d√©ploiement continu modernes.</p> <p>Voici la traduction simple de ton texte en gardant les termes techniques en anglais, comme demand√© :</p> <p>Voici la traduction avec les titres format√©s en Markdown, comme demand√© :</p> <p>D'accord, je m'excuse pour l'oubli des nombres dans les titres et des points de bullet. Je ferai plus attention la prochaine fois. Voici la version corrig√©e avec les titres num√©rot√©s comme dans l'original, et en conservant la structure intacte pour les sections √† puces :</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#options-dinstallation","level":3,"title":"Options d'Installation","text":"<p>Explorez les diff√©rents mod√®les d'installation d'ArgoCD pour d√©terminer lequel correspond le mieux √† vos besoins de d√©ploiement. ArgoCD peut √™tre install√© en deux modes principaux : une installation de base pour un usage mono-tenant et une installation multi-tenant pour des environnements n√©cessitant un acc√®s isol√© pour plusieurs √©quipes.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#installation-de-base","level":4,"title":"Installation de base","text":"<p>L'installation de base est con√ßue pour les utilisateurs qui ex√©cutent ArgoCD comme un service autonome. Ce mode fournit un d√©ploiement minimal, non haute disponibilit√© (non-HA), ce qui le rend parfait pour des configurations simples qui ne n√©cessitent pas de multi-tenancy.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#installation-multi-tenant","level":4,"title":"Installation Multi-Tenant","text":"<p>Les d√©ploiements multi-tenant sont id√©aux pour les organisations avec plusieurs √©quipes de d√©veloppement d'applications, g√©n√©ralement g√©r√©es par une √©quipe plateforme centralis√©e. Dans le mod√®le multi-tenant, vous avez deux variantes d'installation :</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#1-non-high-availability-non-ha","level":5,"title":"1. Non-High Availability (non-HA)","text":"<p>Cette variante est excellente pour l'√©valuation, les tests et les d√©ploiements de proof-of-concept, bien qu'elle ne soit pas recommand√©e pour un usage en production.</p> <p>install.yaml : D√©ploie ArgoCD avec un acc√®s cluster-admin, ce qui le rend adapt√© aux clusters o√π ArgoCD d√©ploie √©galement des applications. De plus, les identifiants fournis permettent de d√©ployer sur des clusters distants.</p> <p>namespace-installed.yaml : Configure ArgoCD pour un acc√®s au niveau du namespace, offrant des permissions restreintes. Cette option est utile lorsque vous voulez limiter l'acc√®s d'ArgoCD tout en d√©ployant des applications dans le m√™me cluster si n√©cessaire.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#2-high-availability-ha","level":5,"title":"2. High Availability (HA)","text":"<p>Pour les environnements de production, l'option haute disponibilit√© est le choix recommand√©. Elle am√©liore la r√©silience en d√©ployant plusieurs r√©plicas pour les composants critiques. Deux manifestes sont fournis :</p> <p>ha-install.yaml ha-namespace-installed.yaml L'image est un diagramme de flux illustrant les options d'installation, montrant les chemins pour \"Core\" et \"Multi-Tenant\" menant aux configurations \"Non High Availability\" et \"High Availability\" avec les fichiers YAML correspondants. Un personnage de pieuvre est sur le c√¥t√© gauche.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#note-de-deploiement","level":4,"title":"Note de D√©ploiement","text":"<p>Dans cette le√ßon, nous d√©ploierons ArgoCD dans le namespace argocd en utilisant le manifeste d'installation non-HA (install.yaml).</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#installation-dargocd-avec-helm","level":4,"title":"Installation d'ArgoCD avec Helm","text":"<p>En plus de l'installation standard, ArgoCD peut √™tre install√© √† l'aide de Helm via un chart maintenu par la communaut√©. Par d√©faut, le chart Helm d√©ploie la version non-HA d'ArgoCD.</p> <p>Apr√®s l'installation, t√©l√©chargez le CLI ArgoCD depuis son d√©p√¥t GitHub et d√©placez-le dans votre r√©pertoire binaire local. Le CLI vous permet d'interagir efficacement avec le serveur API d'ArgoCD.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#commandes-dinstallation","level":4,"title":"Commandes d'Installation","text":"<p>Utilisez les commandes suivantes pour installer ArgoCD :</p> <pre><code># D√©ployer ArgoCD en utilisant le manifeste Kubernetes\nkubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml\n\n# Ajouter le d√©p√¥t Helm pour ArgoCD\nhelm repo add argo https://argoproj.github.io/argo-helm\n\n# Installer ArgoCD en utilisant Helm (version non-HA)\nhelm install my-argo-cd argo/argo-cd --version 4.8.0\n\n# T√©l√©charger le dernier CLI ArgoCD\ncurl -sSL -o /usr/local/bin/argocd https://github.com/argoproj/argo-cd/releases/latest/download/argocd-linux-amd64\n\n# D√©finir les permissions d'ex√©cution pour le CLI\nchmod +x /usr/local/bin/argocd\n</code></pre> <p>√âtapes Suivantes</p> <p>Dans les prochains labs pratiques, vous allez configurer le CLI, vous connecter et interagir avec le serveur API d'ArgoCD via ces commandes.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#_1","level":4,"title":"Argo CD","text":"","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#_2","level":4,"title":"Argo CD","text":"","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#argo-cd-intermediate","level":2,"title":"Argo CD Intermediate","text":"","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#configuration-declarative","level":3,"title":"Configuration d√©clarative","text":"<p>Dans ce guide, nous d√©montrons comment configurer des ressources Kubernetes de mani√®re d√©clarative √† l‚Äôaide d‚Äôun exemple d‚Äôapplication Mono. La gestion d√©clarative consiste √† d√©finir les ressources Kubernetes (telles que les Deployments, Services, Secrets et ConfigMaps) et les objets ArgoCD (y compris Applications, Repositories et Projects) dans des fichiers de manifeste. Ces fichiers peuvent √™tre appliqu√©s √† l‚Äôaide de l‚Äôinterface en ligne de commande kubectl pour garantir que l'√©tat souhait√© soit maintenu.</p> <p>Auparavant, nous avons cr√©√© des applications ArgoCD √† l‚Äôaide de l'interface en ligne de commande (CLI) et de l'interface utilisateur (UI), en fournissant de mani√®re interactive la source et la destination. En revanche, cette approche utilise des fichiers de manifeste d√©claratifs pour d√©finir et g√©rer ces applications de mani√®re syst√©matique.</p> <p>Pourquoi utiliser la configuration d√©clarative ?</p> <p>D√©finir votre infrastructure sous forme de code dans Git offre une tra√ßabilit√© claire, un contr√¥le de version et simplifie la gestion des changements √† travers diff√©rents environnements.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#structure-du-depot-git","level":4,"title":"Structure du d√©p√¥t Git","text":"<p>Supposons que vous ayez un d√©p√¥t Git avec la structure ci-dessous. Le d√©p√¥t contient un r√©pertoire appel√© \"declarative\" avec deux sous-r√©pertoires : \"manifests\" et \"mono-app\".</p> <pre><code>Structure du d√©p√¥t Git :\n‚îî‚îÄ‚îÄ declarative\n    ‚îú‚îÄ‚îÄ manifests\n    ‚îÇ   ‚îú‚îÄ‚îÄ geocentric-model\n    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ deployment.yml\n    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ service.yml\n    ‚îî‚îÄ‚îÄ mono-app\n        ‚îî‚îÄ‚îÄ geocentric-app.yml\n</code></pre> <p>Dans le r√©pertoire \"mono-app\", vous trouverez un fichier YAML d‚Äôapplication ArgoCD. Ce fichier d√©finit l‚Äôapplication en sp√©cifiant :</p> <ul> <li>Project Name: Le projet dans ArgoCD.</li> <li>Source Configuration: Inclut l'URL du d√©p√¥t Git, la r√©vision (par exemple, HEAD), et le chemin pointant vers les manifestes Kubernetes souhait√©s (c‚Äôest-√†-dire le r√©pertoire \"geocentric-model\" dans \"declarative/manifests\"). Ce r√©pertoire contient deux fichiers YAML : un pour le Deployment et un pour le Service.</li> <li>Destination Configuration: Sp√©cifie le cluster cible (en utilisant l'URL du cluster local) et le namespace o√π les ressources seront d√©ploy√©es.</li> <li>Sync Policy: Une politique optionnelle qui peut automatiser le processus de synchronisation et d'auto-r√©paration.</li> </ul>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#creation-de-lapplication","level":4,"title":"Cr√©ation de l'application","text":"<p>Une fois que vous avez d√©fini votre manifeste, cr√©ez l‚Äôapplication en ex√©cutant la commande suivante :</p> <pre><code>$ kubectl apply -f mono-app/geocentric-app.yml  \napplication.argoproj.io/geocentric-model-app created  \n</code></pre> <p>Apr√®s la cr√©ation de l'application, ArgoCD va r√©cup√©rer les manifestes de Deployment et de Service depuis le d√©p√¥t Git. Il cr√©era ensuite les ressources sur le cluster cible, soit manuellement via une op√©ration de synchronisation, soit automatiquement si la synchronisation est configur√©e.</p> <p>Avantage</p> <p>En g√©rant √† la fois la d√©finition de votre application et ses ressources correspondantes dans le contr√¥le de version, vous pouvez facilement suivre les changements et maintenir un √©tat coh√©rent entre vos environnements.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#exemple-de-manifeste-dapplication-argocd","level":4,"title":"Exemple de manifeste d'application ArgoCD","text":"<p>Voici un exemple de manifeste d'application ArgoCD qui encapsule cette configuration d√©clarative :</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: geocentric-model-app\n  namespace: argocd\nspec:\n  project: default\n  source:\n    repoURL: https://github.com/sidd-harth/test-cd.git\n    targetRevision: HEAD\n    path: ./declarative/manifests/geocentric-model\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: geocentric-model\n  syncPolicy:\n    syncOptions:\n      - CreateNamespace=true\n    automated:\n      selfHeal: true\n</code></pre> <p>Ce manifeste d√©finit l‚Äôapplication en sp√©cifiant tous les d√©tails n√©cessaires, y compris le d√©p√¥t, la r√©vision, et les chemins des ressources. Il configure √©galement des options de synchronisation automatis√©e, garantissant que votre application correspond en permanence √† l'√©tat souhait√© d√©fini dans Git.</p> <p>Utiliser une approche d√©clarative avec Git comme source unique de v√©rit√© garantit que les changements d‚Äôinfrastructure sont versionn√©s, audit√©s et facilement annul√©s si n√©cessaire. Pour plus de lecture sur les meilleures pratiques Kubernetes et ArgoCD, consultez la Documentation Kubernetes et la Documentation ArgoCD.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#configuration-declarative-de-lapplication-mono","level":3,"title":"Configuration d√©clarative de l‚Äôapplication Mono","text":"<p>Dans cette le√ßon, vous apprendrez comment g√©rer une application ArgoCD unique de mani√®re d√©clarative. Contrairement aux approches pr√©c√©dentes qui utilisaient l'interface en ligne de commande (CLI) ou l'interface utilisateur (UI) d'ArgoCD, cette m√©thode consiste √† stocker et g√©rer les manifestes d'application ArgoCD dans un d√©p√¥t Git, tout comme pour tout autre manifeste de d√©ploiement ou de service Kubernetes.</p> <p>L‚Äôimage montre une interface web pour configurer une application dans ArgoCD, avec des options pour d√©finir le nom de l'application, le nom du projet, la politique de synchronisation et d‚Äôautres param√®tres. La barre lat√©rale gauche affiche les indicateurs d'√©tat de synchronisation et de sant√©.</p> <p>Dans l'UI ArgoCD, vous d√©finissez les m√©tadonn√©es de l'application, les d√©tails de la source et de la destination. En coulisse, cela cr√©e une sp√©cification YAML qui d√©crit l‚Äôapplication. Voici un exemple de manifeste :</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: sample-app\nspec:\n  destination:\n    namespace: sample\n    server: https://kubernetes.default.svc\n  source:\n    path: ./sample\n    repoURL: http://139.59.21.103:3000/siddhanth/gitops-argocd\n    targetRevision: HEAD\n  project: default\n  syncPolicy:\n    automated:\n      prune: true\n      selfHeal: true\n</code></pre> <p>Note</p> <p>ArgoCD peut cr√©er et g√©rer des applications automatiquement en utilisant des sp√©cifications YAML stock√©es dans des d√©p√¥ts Git.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#creer-une-application-argocd-declarative","level":4,"title":"Cr√©er une application ArgoCD d√©clarative","text":"<p>Pour configurer une application ArgoCD unique en utilisant l‚Äôapproche d√©clarative, suivez ces √©tapes :</p> <ul> <li>1. Localiser le r√©pertoire du d√©p√¥t Dans votre d√©p√¥t GitOps ArgoCD, trouvez le r√©pertoire \"MonoApplication\". Dans cet exemple, le manifeste de l'application est stock√© dans le r√©pertoire \"mono-app\". Vous devriez y voir un fichier nomm√© geocentric-app.yml.</li> </ul> <p>L‚Äôimage montre une interface de d√©p√¥t Gitea avec des d√©tails sur les branches, les commits et les changements r√©cents. Elle comprend des options pour cr√©er un fichier, t√©l√©charger un fichier ou appliquer un patch.</p> <ul> <li>2. V√©rifier le manifeste YAML Voici un extrait du fichier geocentric-app.yml qui d√©finit l'application ArgoCD. Cet exemple inclut des champs essentiels tels que le type, la version API, le projet et les d√©tails de la source :</li> </ul> application.yaml<pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: geocentric-model-app\n  namespace: argocd\n  finalizers:\n    - resources-finalizer.argocd.argoproj.io\nspec:\n  project: default\n  source:\n    repoURL: http://165.22.209.118:3000/siddharth/gitops-argocd.git\n    targetRevision: HEAD\n    path: ./declarative/manifests/geocentric-model\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: geocentric-model\n  syncPolicy:\n    syncOptions:\n      - CreateNamespace=true\n    automated:\n      prune: true\n      selfHeal: true\n</code></pre> <p>Ce manifeste indique √† ArgoCD de r√©cup√©rer les manifestes de l'application depuis le r√©pertoire sp√©cifi√©, de les d√©ployer dans le namespace geocentric-model, et d'activer la synchronisation automatis√©e avec des options de nettoyage (pruning) et d'auto-r√©paration (self-healing).</p> <p>Tip</p> <p>Les extraits YAML dupliqu√©s ont √©t√© consolid√©s pour plus de clart√©. Utilisez le m√™me manifeste pour garantir la coh√©rence des d√©ploiements.</p> <ul> <li>3. D√©ployer l‚Äôapplication avec kubectl</li> </ul> <p>Suivez ces √©tapes pour r√©cup√©rer le d√©p√¥t dans votre cluster Kubernetes et cr√©er l'application :</p> <p>Clonez le d√©p√¥t et naviguez jusqu‚Äôau dossier mono-app :</p> <pre><code>mkdir demo\ncd demo/\ngit clone http://139.59.21.103:3000/siddharth/gitops-argocd\ncd gitops-argocd/\nll\n</code></pre> <p>Naviguez vers le r√©pertoire declarative/mono-app/ et confirmez le contenu du fichier geocentric-app.yml :</p> <pre><code>cd declarative/mono-app/\ncat geocentric-app.yml\n</code></pre> <p>Appliquez le manifeste dans le namespace argocd avec la commande suivante :</p> <pre><code>kubectl -n argocd apply -f geocentric-app.yml\n</code></pre> <p>Une fois appliqu√©, vous devriez voir la sortie suivante, indiquant un succ√®s :</p> <pre><code>application.argoproj.io/geocentric-model-app created\n</code></pre> <ul> <li>4. V√©rifier le d√©ploiement de l‚Äôapplication</li> </ul> <p>Confirmez l'√©tat de l'application en utilisant √† la fois la CLI ArgoCD et les commandes kubectl :</p> <p>Listez les applications avec ArgoCD :</p> <pre><code>argocd app list\n</code></pre> <p>Cela affichera la liste des applications, y compris geocentric-model-app.</p> <p>Vous pouvez √©galement v√©rifier son √©tat avec :</p> <pre><code>kubectl -n argocd get applications\n</code></pre> <p>Avec la synchronisation automatis√©e activ√©e, ArgoCD mettra √† jour automatiquement l'application. V√©rifiez l'UI ArgoCD pour confirmer que les ressources de d√©ploiement et de service sont correctement d√©ploy√©es.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#exposer-lapplication","level":4,"title":"Exposer l'application","text":"<p>Acc√©dez √† l'application d√©ploy√©e via le service expos√© en NodePort. Par exemple, si le service est expos√© sur le port 30682, vous pouvez utiliser ce port pour acc√©der √† l'UI de l'application. Voici un exemple de manifeste de service :</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    kubectl.kubernetes.io/last-applied-configuration: &gt;\n      {\"apiVersion\":\"v1\",\"kind\":\"Service\",\"metadata\":{},\"labels\":{\"app.kubernetes.io/instance\":\"geocentric-model-app\"},\"name\":\"geocentric-model-svc\",\"namespace\":\"geocentric-model\"}\n  creationTimestamp: '2022-09-23T17:52:34Z'\n  labels:\n    app.kubernetes.io/instance: geocentric-model-app\n  name: geocentric-model-svc\n  namespace: geocentric-model\nspec:\n  clusterIP: 10.111.125.181\n  ports:\n    - nodePort: 38684\n      port: 80\n      protocol: TCP\n      targetPort: 80\n  selector:\n    app: geocentric-model\n  type: NodePort\n</code></pre> <p>Warning</p> <p>Assurez-vous que le fichier geocentric-app.yml est cr√©√© dans le namespace ArgoCD pour √©viter tout probl√®me de d√©ploiement.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#resume_2","level":4,"title":"R√©sum√©","text":"<p>Dans ce guide, vous avez appris √† g√©rer une application ArgoCD unique de mani√®re d√©clarative en :</p> <ul> <li>Stockant le manifeste YAML dans un d√©p√¥t Git.</li> <li>D√©ployant l'application √† l'aide de kubectl.</li> <li>V√©rifiant l'√©tat de l'application via la CLI et l'UI ArgoCD.</li> </ul> <p>Cette approche d√©clarative simplifie la gestion des applications en int√©grant les principes GitOps, garantissant que le d√©ploiement de votre application soit reproductible et contr√¥l√© en version.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#app-of-apps","level":3,"title":"App of Apps","text":"<p>Dans cet article, nous explorons le mod√®le \"App-of-Apps\" dans ArgoCD‚Äîune approche d√©clarative qui simplifie la cr√©ation et la gestion des applications ArgoCD. Plut√¥t que de d√©ployer manuellement chaque application, ce mod√®le g√©n√®re et g√®re de mani√®re programmatique plusieurs applications ArgoCD √† partir d'une seule configuration racine.</p> <p>L'id√©e principale est de cr√©er une application racine ArgoCD dont la source pointe vers un dossier contenant des fichiers de d√©finition YAML pour chaque microservice ou application. Chaque fichier YAML sp√©cifie un chemin vers un r√©pertoire contenant les manifestes Kubernetes associ√©s. Une fois tous ces fichiers de configuration commis dans un d√©p√¥t Git, ArgoCD d√©tecte automatiquement et d√©ploie les applications d√©finies.</p> <p>Comment cela fonctionne</p> <p>L'application racine agit en tant qu'orchestre. Elle indique √† ArgoCD de parcourir le r√©pertoire sp√©cifi√©, en lisant chaque fichier YAML pour instancier les applications associ√©es. Cela garantit que les mises √† jour dans votre d√©p√¥t Git d√©clenchent une synchronisation automatique avec votre cluster Kubernetes.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#exemple-de-structure-du-depot","level":4,"title":"Exemple de structure du d√©p√¥t","text":"<p>Supposons que votre d√©p√¥t Git soit organis√© de sorte que chaque r√©pertoire contienne les fichiers n√©cessaires pour une application sp√©cifique. L'exemple suivant examine le fichier YAML de l'application racine ArgoCD, qui est plac√© dans un r√©pertoire multi-applications. Cette application racine pointe vers le r√©pertoire <code>app-of-apps</code>, incitant ArgoCD √† cr√©er toutes les applications enfants d√©finies dans ce dossier.</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: app-of-apps\nspec:\n  project: default\n  source:\n    repoURL: https://github.com/sidd-harth/test-cd.git\n    targetRevision: HEAD\n    path: ./declarative/app-of-apps\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: argocd\n  syncPolicy:\n    automated:\n      prune: true\n      selfHeal: true\n</code></pre> <p>Dans cet exemple, trois fichiers YAML d'application ArgoCD sont d√©finis dans le r√©pertoire sp√©cifi√©. ArgoCD lit ces d√©finitions et cr√©e automatiquement les applications correspondantes.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#examen-detaille-fichier-yaml-de-lapplication-circle","level":4,"title":"Examen d√©taill√© : Fichier YAML de l'application Circle","text":"<p>Voyons maintenant le fichier YAML de l‚Äôapplication Circle App en tant qu'exemple. Chaque fichier YAML d'application comprend un champ source qui fait r√©f√©rence √† son r√©pertoire de manifeste sp√©cifique. Dans l'exemple de Circle App, ArgoCD utilise les d√©tails fournis pour cr√©er ou mettre √† jour le d√©ploiement et le service dans le cluster Kubernetes.</p> <pre><code># declarative/app-of-apps/app-of-apps.yml\napiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: app-of-apps\n  namespace: argocd\nspec:\n  project: default\n  source:\n    repoURL: https://github.com/sidd-harth/test-cd.git\n    targetRevision: HEAD\n    path: ./declarative/app-of-apps\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: argocd\n  syncPolicy:\n    automated:\n      prune: true\n      selfHeal: true\n\n# declarative/app-of-apps/circle-app.yml\napiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: circle-app\n  namespace: argocd\nspec:\n  project: default\n  source:\n    repoURL: https://github.com/sidd-harth/test-cd.git\n    targetRevision: HEAD\n    path: ./declarative/manifests/circle\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: circle\n  syncPolicy:\n    syncOptions:\n      - CreateNamespace=true\n</code></pre> <p>En utilisant ce mod√®le, ArgoCD cr√©e automatiquement toutes les applications list√©es et d√©ploie leurs manifestes Kubernetes associ√©s. Cette approche peut √™tre √©tendue pour g√©rer n'importe quel objet Kubernetes‚Äîy compris ArgoCD lui-m√™me. Chaque fois qu'une d√©finition d'application est mise √† jour ou qu'une nouvelle application est ajout√©e dans votre d√©p√¥t Git, ArgoCD garantit que vos d√©ploiements restent synchronis√©s en mettant √† jour ou en cr√©ant automatiquement les applications correspondantes.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#avantages-du-modele-app-of-apps","level":4,"title":"Avantages du mod√®le App-of-Apps","text":"Avantage Description Contr√¥le centralis√© G√©rer plusieurs applications √† partir d'une seule application ArgoCD. Synchronisation automatis√©e D√©tecter et d√©ployer automatiquement les changements de votre d√©p√¥t Git vers votre cluster Kubernetes. Gestion √©volutive Ajouter ou mettre √† jour des applications facilement sans intervention manuelle sur chaque d√©ploiement.","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#resume_3","level":4,"title":"R√©sum√©","text":"<p>Le mod√®le App-of-Apps dans ArgoCD permet de g√©rer plusieurs applications de mani√®re d√©clarative et automatis√©e √† partir d'une application racine. Ce mod√®le facilite la gestion de d√©ploiements complexes tout en am√©liorant l'efficacit√© op√©rationnelle, ce qui en fait un outil essentiel dans les workflows modernes bas√©s sur GitOps.</p> <p>Pour plus d‚Äôinformations sur Kubernetes et les mod√®les de d√©ploiement d√©claratifs, vous pouvez consulter les ressources suivantes :</p> <p>L'utilisation du mod√®le App-of-Apps simplifie les d√©ploiements complexes et am√©liore l'efficacit√© des op√©rations, en garantissant que chaque modification dans votre d√©p√¥t Git se refl√®te instantan√©ment dans vos d√©ploiements Kubernetes.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#configuration-declarative-avec-lapp-of-apps","level":3,"title":"Configuration d√©clarative avec l'App of Apps","text":"<p>Dans cette le√ßon, vous apprendrez √† √©tendre l'approche d√©clarative des applications ArgoCD en d√©ployant plusieurs applications √† l'aide du mod√®le \"app-of-apps\". Ce mod√®le permet √† une seule application ArgoCD de g√©rer d'autres applications ArgoCD, simplifiant ainsi le d√©ploiement et la gestion de plusieurs services.</p> <p>La structure du d√©p√¥t pour cette configuration organise plusieurs configurations ArgoCD dans des r√©pertoires distincts, chacun contenant les fichiers manifestes pour des applications individuelles.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#application-argocd-multi-applications","level":4,"title":"Application ArgoCD multi-applications","text":"<p>L'application principale multi-applications pointe vers le d√©p√¥t Git et sp√©cifie le chemin contenant les manifestes qui g√®rent d'autres applications ArgoCD. Sa d√©finition est la suivante :</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: app-of-apps\n  namespace: argocd\nspec:\n  project: default\n  source:\n    repoURL: http://165.22.209.118:3000/siddharth/gitops-argocd.git\n    targetRevision: HEAD\n    path: ./declarative/multi-app\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: argocd\n  syncPolicy:\n    automated:\n      prune: true\n      selfHeal: true\n</code></pre> <p>Note</p> <p>La configuration multi-app r√©cup√®re les manifestes √† partir d'un r√©pertoire sp√©cifique, offrant ainsi une configuration centralis√©e pour g√©rer les applications imbriqu√©es.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#configuration-de-lapp-of-apps","level":4,"title":"Configuration de l'App of Apps","text":"<p>L'application principale fait r√©f√©rence au r√©pertoire <code>app-of-apps</code>, o√π plusieurs manifestes d'applications ArgoCD r√©sident. La configuration pour l'App of Apps est la suivante :</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: app-of-apps\n  namespace: argocd\nspec:\n  project: default\n  source:\n    repoURL: http://165.22.209.118:3000/siddharth/gitops-argocd.git\n    targetRevision: HEAD\n    path: ./declarative/app-of-apps\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: argocd\n  syncPolicy:\n    automated:\n      prune: true\n      selfHeal: true\n</code></pre> <p>Dans le r√©pertoire <code>declarative/app-of-apps</code>, vous trouverez trois manifestes d'applications ArgoCD :</p> <ul> <li>Heliocentric App : r√©cup√®re les manifestes Kubernetes √† partir d'un chemin sp√©cifique.</li> <li>Geocentric App : pointe vers un autre chemin de manifeste.</li> <li>Applications suppl√©mentaires : vous pouvez ajouter d'autres sous-applications pour √©tendre encore les capacit√©s de d√©ploiement.</li> </ul>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#manifeste-de-lapplication-heliocentric","level":4,"title":"Manifeste de l'application Heliocentric","text":"<p>Voici l'exemple du manifeste pour l'application Heliocentric :</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: heliocentric-app\n  namespace: argocd\n  finalizers:\n    - resources-finalizer.argocd.argoproj.io\nspec:\n  project: default\n  source:\n    repoURL: http://165.22.209.118:3000/siddharth/gitops-argocd.git\n    targetRevision: HEAD\n    path: ./declarative/manifests/heliocentric-model\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: heliocentric\n  syncPolicy:\n    syncOptions:\n      - CreateNamespace=true\n    automated:\n      prune: true\n      selfHeal: true\n</code></pre>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#manifeste-de-lapplication-geocentric","level":4,"title":"Manifeste de l'application Geocentric","text":"<p>Le manifeste de l'application Geocentric est configur√© de mani√®re similaire :</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: geocentric-app\n  namespace: argocd\n  finalizers:\n    - resources-finalizer.argocd.argoproj.io\nspec:\n  project: default\n  source:\n    repoURL: http://165.22.209.118:3000/siddharth/gitops-argocd.git\n    targetRevision: HEAD\n    path: ./declarative/manifests/geocentric-model\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: geocentric\n  syncPolicy:\n    syncOptions:\n      - CreateNamespace=true\n    automated:\n      prune: true\n      selfHeal: true\n</code></pre>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#manifestes-kubernetes-des-applications","level":4,"title":"Manifestes Kubernetes des applications","text":"<p>Au-del√† des manifestes d'applications ArgoCD, le r√©pertoire d√©claratif contient √©galement des manifestes Kubernetes s√©par√©s pour chaque application, notamment des d√©ploiements et des services. Lorsque vous cr√©ez l'application App-of-Apps ArgoCD principale, elle d√©clenche automatiquement la cr√©ation de trois autres applications ArgoCD.</p> <p>Par exemple, une version alternative du manifeste app-of-apps avec un namespace de destination diff√©rent pourrait √™tre d√©finie comme suit :</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: app-of-apps\n  namespace: argocd\nspec:\n  project: default\n  source:\n    repoURL: http://165.22.209.118:3000/siddharth/gitops-argocd.git\n    targetRevision: HEAD\n    path: ./declarative/app-of-apps\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: kubernetes\n  syncPolicy:\n    automated:\n      prune: true\n      selfHeal: true\n</code></pre> <p>Chaque application imbriqu√©e d√©ploie ensuite ses ressources Kubernetes respectives en fonction de son propre manifeste.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#processus-de-deploiement","level":4,"title":"Processus de d√©ploiement","text":"<p>Pour d√©ployer des applications √† l'aide du mod√®le App-of-Apps, suivez ces √©tapes :</p> <ol> <li>Supprimez toute application Geocentric model pr√©c√©demment d√©ploy√©e si elle existe, car elle sera recr√©√©e sous le nouveau mod√®le de gestion.</li> <li>Naviguez vers le r√©pertoire multi-app, qui contient le fichier app-of-apps.yml.</li> <li>Appliquez le manifeste dans le namespace <code>argocd</code> en ex√©cutant :</li> </ol> <pre><code>cd multi-app/\nll\n# Assurez-vous que le fichier app-of-apps.yml est pr√©sent ici.\nkubectl -n argocd apply -f app-of-apps.yml\n</code></pre> <p>Apr√®s le d√©ploiement, l'interface ArgoCD affichera plusieurs applications :</p> <ul> <li>app-of-apps (g√©rant les applications imbriqu√©es)</li> <li>geocentric-app</li> <li>heliocentric-app</li> <li>heliocentric-app sans Pluto (une variante alternative)</li> </ul>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#conseil-de-deploiement","level":4,"title":"Conseil de d√©ploiement","text":"<p>Assurez-vous qu'aucune ancienne configuration n'interf√®re avec le nouveau d√©ploiement en supprimant les applications obsol√®tes avant d'appliquer le nouveau manifeste.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#affichage-des-applications-dans-linterface-argocd","level":3,"title":"Affichage des applications dans l'interface ArgoCD","text":"<p>Dans le tableau de bord ArgoCD, cliquez sur n'importe quelle sous-application (par exemple, geocentric-app ou heliocentric-app) pour afficher des informations d√©taill√©es sur le manifeste. Voici un extrait de manifeste pour une application en cours de synchronisation :</p> <pre><code>status:\n  health:\n    status: Healthy\n  history:\n    deployStartDate: '2022-09-23T20:05:21Z'\n    deployEndDate: '2022-09-23T20:05:24Z'\n    revision: e2edb3a016b752028c38adb898d34114c1ec6\n    path: ./declarative/manifests/heliocentric-model\n    repoURL: http://165.22.209.118:3000/siddharth/gitops-argocd.git\n  operationState:\n    finishedAt: '2022-09-23T20:05:24Z'\n    message: successfully synced (all tasks run)\n    initiatedBy:\n      automated: true\n    retry:\n      limit: 5\n    sync:\n      prune: true\n      revision: e2edb3a016b752028c38adb898d34114c1ec6\n      syncOptions:\n        - CreateNamespace=true\n    phase: Succeeded\n    startedAt: '2022-09-23T20:05:21Z'\n    syncResult:\n      resources:\n        - group: \n            hookPhase: Succeeded\n            kind: Namespace\n            message: namespace/heliocentric created\n            namespace: heliocentric\n            status: Synced\n            syncPhase: PreSync\n            version: v1\n        - group: \n            hookPhase: Running\n            kind: Service\n            message: service/heliocentric-model-svc created\n            name: heliocentric-model-svc\n            namespace: heliocentric\n            status: Sync\n</code></pre>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#acceder-aux-applications-kubernetes-deployees","level":3,"title":"Acc√©der aux applications Kubernetes d√©ploy√©es","text":"<p>Chaque sous-application d√©ploie son propre ensemble de ressources Kubernetes, y compris des services qui exposent les applications. Voici les d√©tails de configuration pour ces services :</p> <p>Service du mod√®le Geocentric :</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    kubectl.kubernetes.io/last-applied-configuration: &gt;\n      {\"apiVersion\":\"v1\",\"kind\":\"Service\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/instance\":\"geocentric-app\"},\"name\":\"geocentric-model-svc\",\"namespace\":\"geocentric\"}}\n  labels:\n    app.kubernetes.io/instance: geocentric-app\n  name: geocentric-model-svc\n</code></pre> <p>Service du mod√®le Heliocentric (avec NodePort) :</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    kubectl.kubernetes.io/last-applied-configuration: &gt;\n      {\"apiVersion\":\"v1\",\"kind\":\"Service\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/instance\":\"heliocentric-app\"},\"name\":\"heliocentric-model-svc\",\"namespace\":\"heliocentric\"},\"spec\":{\"clusterIP\":\"10.183.221.139\",\"clusterIPs\":[\"10.183.221.139\"],\"externalTrafficPolicy\":\"Cluster\",\"internalTrafficPolicy\":\"Cluster\",\"ipFamilies\":[\"IPv4\"],\"ipFamilyPolicy\":\"SingleStack\",\"ports\":[{\"nodePort\":31334,\"port\":80,\"protocol\":\"TCP\",\"targetPort\":80}],\"selector\":{\"app\":\"heliocentric-model\"},\"sessionAffinity\":\"None\",\"type\":\"NodePort\"}}\nspec:\n  clusterIP: 10.183.221.139\n  clusterIPs:\n    - 10.183.221.139\n  externalTrafficPolicy: Cluster\n  internalTrafficPolicy: Cluster\n  ipFamilies:\n    - IPv4\n  ipFamilyPolicy: SingleStack\n  ports:\n    - nodePort: 31334\n      port: 80\n      protocol: TCP\n      targetPort: 80\n  selector:\n    app: heliocentric-model\n  sessionAffinity: None\n  type: NodePort\n  loadBalancer: {}\n</code></pre> <p>Service du mod√®le Heliocentric (sans Pluto) :</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    kubectl.kubernetes.io/last-applied-configuration: &gt;\n      {\"apiVersion\":\"v1\",\"kind\":\"Service\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/instance\":\"heliocentric-model-no-pluto-app\"},\"name\":\"heliocentric-model-no-pluto-svc\",\"resourceVersion\":\"1964046\",\"uid\":\"73603805-36c2-4296-bd9c-2f14cac42080\"}}\nspec:\n  clusterIP: 10.180.97.241\n  externalIPs:\n    - 10.180.97.241\n  externalTrafficPolicy: Cluster\n  internalTrafficPolicy: Cluster\n  ipFamily:\n    - IPv4\n  ipFamilyPolicy: SingleStack\n  ports:\n    - name: nodePort\n      port: 80\n      protocol: TCP\n      targetPort: 80\n  selector:\n    app: heliocentric-model-no-pluto\n  sessionAffinity: None\n  type: NodePort\nstatus:\n  loadBalancer: {}\n</code></pre> <p>Fun</p> <p>Beaucoup d‚Äôutilisateurs consid√®rent que Pluto ne devrait pas √™tre class√© comme une plan√®te en raison de son orbite distante et de sa rotation r√©trograde, ce qui explique pourquoi une variante d‚Äôapplication excluant Pluto est fournie.</p> <p>Toutes ces applications sont d√©ploy√©es √† l'aide d'une seule configuration ArgoCD App-of-Apps. Ce mod√®le simplifie non seulement la gestion, mais permet √©galement un contr√¥le centralis√© sur plusieurs applications √† partir d'un seul d√©p√¥t Git et instance ArgoCD.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#deployer-des-applications-avec-un-helm-chart","level":3,"title":"D√©ployer des applications avec un Helm Chart","text":"<p>Dans cet article, nous explorons comment ArgoCD d√©ploie et g√®re des applications en utilisant des Helm Charts, simplifiant ainsi la gestion de Kubernetes et adoptant les principes de GitOps.</p> <p>Helm est le gestionnaire de paquets pour Kubernetes qui simplifie l'installation et la gestion du cycle de vie des applications en utilisant des collections de fichiers de configuration YAML, appel√©es Helm Charts. Ces Charts regroupent les d√©finitions YAML n√©cessaires au d√©ploiement d'un ensemble de ressources Kubernetes. Avec les d√©finitions d'applications d√©claratives comme principe fondamental de GitOps, les Helm Charts peuvent √™tre stock√©s dans des d√©p√¥ts sp√©cialement con√ßus pour les empaqueter et les distribuer.</p> <p>ArgoCD am√©liore ce processus en d√©ployant des Helm Charts empaquet√©s, en les surveillant pour des mises √† jour, et en g√©rant leur cycle de vie apr√®s leur d√©ploiement. Par exemple, si vous avez un d√©p√¥t Git structur√© avec un Helm Chart, l'interface en ligne de commande (CLI) d'ArgoCD peut √™tre utilis√©e pour cr√©er une application qui le d√©ploie. En sp√©cifiant l'URL du d√©p√¥t, le chemin vers le chart, et les valeurs de remplacement via l'option <code>Helm Set</code>, vous obtenez un processus de d√©ploiement flexible et continu.</p> <p>ArgoCD est polyvalent et prend en charge le d√©ploiement √† partir de diverses sources :</p> <ul> <li>Artifactory Hub</li> <li>Bitnami Helm Charts</li> </ul> <p>De plus, vous pouvez d√©ployer un Helm Chart en utilisant l'interface utilisateur (UI) d'ArgoCD. L'UI simplifie la connexion aux d√©p√¥ts en prenant en charge les int√©grations SSH, HTTPS ou GitHub App. Elle permet √©galement de configurer les d√©tails du d√©p√¥t, tels que le nom, le type, l'URL, et les informations d'identification pour acc√©der aux d√©p√¥ts priv√©s.</p> <p>Note</p> <p>Une fois qu'ArgoCD d√©ploie une application en utilisant un Helm Chart, la gestion est enti√®rement transf√©r√©e √† ArgoCD. Par cons√©quent, l'ex√©cution de la commande <code>helm ls</code> ne permettra pas d'afficher le release d√©ploy√©, car il n'est plus g√©r√© par Helm.</p> <p>Voici un exemple montrant comment cr√©er deux applications ArgoCD :</p> <ol> <li>Une qui d√©ploie un Helm Chart depuis un d√©p√¥t Git.</li> <li>Une autre qui d√©ploie le Helm Chart Nginx depuis le d√©p√¥t Bitnami.</li> </ol> <p>Apr√®s le d√©ploiement, la commande <code>helm ls</code> confirme que les applications sont enti√®rement g√©r√©es par ArgoCD.</p> <pre><code>$ argocd app create random-shapes \\\n  --repo https://github.com/sidd-harth/test-cd.git \\\n  --path helm-chart \\\n  --helm-set replicaCount=2 \\\n  --helm-set color.circle=pink \\\n  --helm-set color.square=violet \\\n  --helm-set service.type=NodePort \\\n  --dest-namespace default \\\n  --dest-server https://kubernetes.default.svc\napplication 'random-shapes' created\n</code></pre> <pre><code>$ argocd app create nginx \\\n  --repo https://charts.bitnami.com/bitnami \\\n  --helm-chart nginx \\\n  --revision 12.0.3 \\\n  --values-literal-file values.yaml \\\n  --dest-namespace default \\\n  --dest-server https://kubernetes.default.svc\napplication 'nginx' created\n</code></pre> <pre><code>$ helm ls\nNAME   NAMESPACE REVISION UPDATED STATUS CHART APP VERSION\n</code></pre> <p>Cet exemple illustre clairement les points suivants :</p> <ul> <li>Comment cr√©er des applications ArgoCD en utilisant la CLI.</li> <li>Comment configurer les param√®tres d'un Helm Chart lors du d√©ploiement.</li> <li>Comment v√©rifier qu'apr√®s le d√©ploiement, la gestion est assur√©e par ArgoCD et non par Helm.</li> </ul>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#principaux-ressources-et-references","level":4,"title":"Principaux Ressources et R√©f√©rences","text":"Composant Description Commande Exemple / Lien ArgoCD CLI Utilis√© pour cr√©er et g√©rer les applications ArgoCD Exemples de commande <code>argocd app create</code> ci-dessus Helm Package Simplifie l'installation et la gestion des applications Kubernetes Documentation Helm Principe GitOps Configuration d√©clarative et automatisation du d√©ploiement des applications GitOps Kubernetes <p>Warning</p> <p>Assurez-vous que vous avez les bonnes autorisations d'acc√®s et que vos informations d'identification sont correctement configur√©es lorsque vous vous connectez √† des d√©p√¥ts priv√©s. Une mauvaise configuration peut entra√Æner des √©checs de d√©ploiement.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#deploiement-dapplications-multi-clusters","level":3,"title":"D√©ploiement d'applications multi-clusters","text":"<p>ArgoCD simplifie la gestion des d√©ploiements multi-clusters en permettant de d√©ployer des applications soit dans son propre cluster, soit sur des clusters externes. Dans ce guide, nous vous expliquerons comment configurer un cluster Kubernetes externe et d√©ployer des applications sur plusieurs clusters √† l‚Äôaide d'ArgoCD.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#vue-densemble","level":4,"title":"Vue d'ensemble","text":"<p>Avant de d√©ployer des applications sur plusieurs clusters, vous devez disposer d'un cluster Kubernetes externe. ArgoCD int√®gre des clusters externes en lisant leurs d√©tails d'identification depuis le fichier kubeconfig. Cela permet √† ArgoCD de g√©rer les d√©ploiements en dehors de son propre cluster.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#configuration-du-cluster-externe","level":4,"title":"Configuration du Cluster Externe","text":"<p>Commencez par mettre √† jour votre fichier kubeconfig pour enregistrer votre cluster externe. Utilisez les commandes suivantes pour d√©finir la configuration du cluster et des identifiants :</p> <pre><code>$ kubectl config set-cluster prod --server=https://1.2.3.4 --certificate-authority=prod.crt\nCluster \"prod\" set.\n</code></pre> <pre><code>$ kubectl config set-credentials admin --client-certificate=admin.crt --client-key=admin.key\nUser \"admin\" set.\n</code></pre> <p>Apr√®s avoir d√©fini le cluster externe dans votre kubeconfig, ajoutez-le √† ArgoCD en r√©f√©rencant le nom de contexte appropri√© :</p> <pre><code>$ argocd cluster add &lt;context-name&gt;\n</code></pre> <p>Avertissement</p> <p>Cette commande cr√©e un compte de service sur le cluster externe avec un acc√®s complet au niveau du cluster. Assurez-vous de comprendre les implications de s√©curit√© avant de proc√©der.</p> <p>Une fois que vous confirmez, ArgoCD cr√©e automatiquement le compte de service, le r√¥le de cluster, et l'association du r√¥le de cluster, validant ainsi le cluster externe comme cible de d√©ploiement.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#exemple-detaille","level":4,"title":"Exemple d√©taill√©","text":"<p>Voici un exemple complet illustrant toutes les √©tapes essentielles pour ajouter un cluster externe :</p> <pre><code>$ kubectl config set-cluster prod --server=https://1.2.3.4 --certificate-authority=prod.crt\nCluster \"prod\" set.\n\n$ kubectl config set-credentials admin --client-certificate=admin.crt --client-key=admin.key\nUser \"admin\" set.\n\n$ kubectl config set-context admin-prod --cluster=prod --user=admin --namespace=prod-app\nContext \"admin-prod\" set.\n\n$ argocd cluster add admin-prod\nWARNING: This will create a service account `argocd-manager` on the cluster referenced by context `admin-prod` with full cluster level admin privileges. Do you want to continue [y/N]? y\nINFO[0011] ServiceAccount \"argocd-manager\" created in namespace \"kube-system\"\nINFO[0011] ClusterRole \"argocd-manager-role\" created\nINFO[0011] ClusterRoleBinding \"argocd-manager-role-binding\" created\nCluster 'https://1.2.3.4' added\n\n$ argocd cluster list\nSERVER                                  NAME         VERSION  STATUS      MESSAGE   PROJECT\nhttps://1.2.3.4                         admin-prod   1.21     Successful            &lt;none&gt;\nhttps://kubernetes.default.svc          in-cluster   1.20     Successful            &lt;none&gt;\n</code></pre> <p>Vous pouvez v√©rifier la liste des clusters disponibles pour le d√©ploiement en ex√©cutant la commande <code>argocd cluster list</code>. Notez que les identifiants pour les clusters externes (ou serveurs API) sont stock√©s de mani√®re s√©curis√©e en tant que secrets dans le namespace ArgoCD.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#argocd-advancedadmin","level":2,"title":"ArgoCD AdvancedAdmin","text":"","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#gestion-des-utilisateurs-rbac-dans-argocd","level":3,"title":"Gestion des utilisateurs RBAC dans ArgoCD","text":"<p>Dans cette le√ßon, nous explorons comment ArgoCD met en ≈ìuvre le contr√¥le d'acc√®s bas√© sur les r√¥les (RBAC) pour g√©rer l'acc√®s √† ses ressources de mani√®re efficace. ArgoCD utilise des politiques d√©finies en notation CSV, qui sont appliqu√©es soit √† des utilisateurs individuels, soit √† des groupes SSO. Il est important de noter que les permissions RBAC pour les applications peuvent diff√©rer de celles appliqu√©es √† d'autres types de ressources.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#comment-fonctionne-le-rbac-dans-argocd","level":4,"title":"Comment fonctionne le RBAC dans ArgoCD","text":"<p>Au c≈ìur de la politique RBAC dans ArgoCD, on retrouve quatre composants :</p> <ul> <li>R√¥le: Un ensemble de permissions.</li> <li>Ressource: La ressource cible, comme les clusters, les certificats, les applications, les d√©p√¥ts ou les journaux.</li> <li>Action: L'op√©ration autoris√©e (par exemple, obtenir, cr√©er, mettre √† jour, supprimer, synchroniser, ou remplacer).</li> <li>Projet/Objet: Pour les objets d'application, le chemin de la ressource est exprim√© par le nom du projet suivi d'un slash et du nom de l'application.</li> </ul> <p>Note</p> <p>Les politiques RBAC personnalis√©es permettent un contr√¥le granulaire sur qui peut effectuer des actions sp√©cifiques, garantissant que seuls les utilisateurs autoris√©s ont acc√®s aux ressources sensibles.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#exemple-creation-dun-role-creer-un-cluster-personnalise","level":4,"title":"Exemple : Cr√©ation d'un r√¥le \"Cr√©er un cluster\" personnalis√©","text":"<p>L'exemple suivant montre comment configurer un r√¥le personnalis√© nomm√© <code>createCluster</code> qui permet √† un utilisateur de cr√©er des clusters dans ArgoCD. Dans ce sc√©nario, le r√¥le est attribu√© √† un utilisateur nomm√© <code>jai</code> en patchant la configmap RBAC d'ArgoCD avec la politique appropri√©e.</p> <pre><code>$ kubectl -n argocd patch configmap argocd-rbac-cm \\\n--patch='{\"data\":{\"policy.csv\":\"p, role:create-cluster, clusters, create, *, jai, role:create-cluster\"}}'\nconfigmap/argocd-rbac-cm patched\n</code></pre> <p>Dans cette configuration, le r√¥le <code>createCluster</code> est attribu√© √† l'utilisateur <code>jai</code>, lui permettant de cr√©er des clusters. Vous pouvez v√©rifier les permissions avec la commande <code>argocd account can-i</code> :</p> <ul> <li>V√©rifier si <code>jai</code> peut cr√©er des clusters renvoie \"oui\".</li> <li>Tenter de supprimer un cluster renvoie \"non\" car le r√¥le <code>createCluster</code> ne comprend pas la permission de suppression.</li> </ul>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#exemple-attribution-dun-role-au-niveau-du-projet","level":4,"title":"Exemple : Attribution d'un r√¥le au niveau du projet","text":"<p>Les r√¥les peuvent √©galement √™tre attribu√©s au niveau du projet. Consid√©rons un r√¥le personnalis√© nomm√© <code>kia-admins</code>, qui accorde des droits de modification illimit√©s pour toute application au sein du projet <code>kia-project</code>. Ce r√¥le est attribu√© √† un utilisateur nomm√© <code>ali</code>. Lorsque <code>ali</code> tente de synchroniser les applications dans le projet <code>kia-project</code>, le syst√®me confirme les permissions avec une r√©ponse \"oui\".</p> <pre><code>$ kubectl -n argocd patch configmap argocd-rbac-cm \\\n--patch='{\"data\":{\"policy.csv\":\"p, role:create-cluster, clusters, create, *, jai, role:create-cluster\"}}'\nconfigmap/argocd-rbac-cm patched\n\n$ argocd account can-i create clusters '*'\nyes # Logged in as - jai\n\n$ kubectl -n argocd patch configmap argocd-rbac-cm \\\n--patch='{\"data\":{\"policy.csv\":\"p, role:kia-admins, applications, *, kia-project/*, allow, ali, role:kia-admins\"}}'\nconfigmap/argocd-rbac-cm patched\n\n$ argocd account can-i delete clusters '*'\nno # Logged in as - jai\n\n$ argocd account can-i sync applications kia-project/*\nyes # Logged in as - ali\n</code></pre> <p>Warning</p> <p>Bien que l'utilisateur <code>ali</code> ait re√ßu des permissions au sein du projet <code>kia-project</code>, il n'est pas autoris√© √† synchroniser des applications dans d'autres projets.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#conclusion_1","level":4,"title":"Conclusion","text":"<p>Cette le√ßon a fourni un aper√ßu de la mani√®re de configurer le RBAC dans ArgoCD pour garantir que seuls les utilisateurs correctement autoris√©s puissent effectuer des actions sp√©cifiques sur diverses ressources. En d√©finissant soigneusement des politiques √† l'aide de la notation CSV, vous pouvez g√©rer efficacement l'acc√®s aux clusters, aux applications et √† d'autres composants critiques dans ArgoCD.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#gestion-des-utilisateurs-dans-argocd-avec-rbac","level":3,"title":"Gestion des utilisateurs dans ArgoCD avec RBAC","text":"<p>Cet article explique comment g√©rer les utilisateurs dans ArgoCD, en se concentrant sur la gestion des utilisateurs locaux. Par d√©faut, ArgoCD inclut un utilisateur administrateur int√©gr√© avec un acc√®s complet de super-utilisateur. Pour de meilleures pratiques de s√©curit√©, il est recommand√© d'utiliser le compte administrateur uniquement pour la configuration initiale, puis de le d√©sactiver une fois que tous les utilisateurs n√©cessaires ont √©t√© ajout√©s.</p> <p>ArgoCD prend en charge deux types de comptes utilisateur :</p> <ul> <li>Utilisateurs locaux</li> <li>Utilisateurs authentifi√©s via Single Sign-On (SSO) (par exemple, via Okta ou des produits similaires)</li> </ul> <p>Dans ce guide, nous nous concentrons sur la configuration des utilisateurs locaux.</p> <p>Important</p> <p>Il est recommand√© de d√©sactiver le compte administrateur par d√©faut apr√®s avoir configur√© des comptes suppl√©mentaires afin de minimiser les risques de s√©curit√©.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#configuration-des-utilisateurs-locaux","level":4,"title":"Configuration des utilisateurs locaux","text":"<p>Les utilisateurs locaux dans ArgoCD sont g√©r√©s en mettant √† jour le ConfigMap. Chaque utilisateur est d√©fini avec des capacit√©s associ√©es, telles que la g√©n√©ration de cl√© API et l'acc√®s √† l'interface utilisateur. La capacit√© de cl√© API permet √† un utilisateur de cr√©er un JSON Web Token (JWT) pour les interactions API, tandis que la capacit√© de connexion permet l'acc√®s √† l'interface utilisateur.</p> <p>Apr√®s avoir modifi√© le ConfigMap, votre liste d'utilisateurs pourrait ressembler √† ceci :</p> <pre><code>$ argocd account list\nNAME   ENABLED  CAPABILITIES\nadmin  true     login\njai    true     apiKey, login\nali    true     apiKey, login\n</code></pre> <p>Pour ajouter ou mettre √† jour les comptes utilisateur, vous pouvez patcher le ConfigMap avec les commandes appropri√©es :</p> <pre><code>$ kubectl -n argocd patch configmap argocd-cm --patch='{\"data\":{\"accounts.jai\": \"apiKey,login\"}}'\nconfigmap/argocd-cm patched\n\n$ kubectl -n argocd patch configmap argocd-cm --patch='{\"data\":{\"accounts.ali\": \"apiKey,login\"}}'\nconfigmap/argocd-cm patched\n</code></pre>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#mise-a-jour-des-mots-de-passe-des-utilisateurs","level":4,"title":"Mise √† jour des mots de passe des utilisateurs","text":"<p>ArgoCD fournit des commandes CLI pour d√©finir ou mettre √† jour les mots de passe des utilisateurs. Lorsque vous √™tes connect√© en tant qu'administrateur, vous devez entrer le mot de passe actuel de l'administrateur pour changer le mot de passe d'un autre utilisateur. Notez que les nouveaux utilisateurs n'ont pas d'acc√®s tant que leur mot de passe n'est pas configur√©.</p> <p>ArgoCD vient avec deux r√¥les pr√©d√©finis :</p> <ul> <li>Lecture seule : Accorde aux utilisateurs un acc√®s uniquement pour visualiser les ressources.</li> <li>Administrateur : Accorde aux utilisateurs un acc√®s complet et illimit√©.</li> </ul> <p>Par d√©faut, le compte administrateur se voit attribuer le r√¥le administrateur. Cependant, vous pouvez modifier cette attribution ou cr√©er des r√¥les personnalis√©s en √©ditant le ConfigMap RBAC d'ArgoCD.</p> <p>Par exemple, pour mettre √† jour le mot de passe de l'utilisateur \"jai\", utilisez la commande suivante :</p> <pre><code>$ argocd account update-password --account jai\n*** Entrez le mot de passe de l'utilisateur actuellement connect√© (admin) :\n*** Entrez le nouveau mot de passe pour l'utilisateur jai :\n*** Confirmez le nouveau mot de passe pour l'utilisateur jai :\nMot de passe mis √† jour\n</code></pre> <p>Alternativement, vous pouvez ex√©cuter la mise √† jour dans une seule commande :</p> <pre><code>$ argocd account update-password \\\n--account jai \\\n--new-password j‚Ç¨i_p@ssw0rd \\\n--current-password @dmin_p@$sword\nMot de passe mis √† jour\n</code></pre>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#personnalisation-des-roles","level":4,"title":"Personnalisation des r√¥les","text":"<p>Le r√¥le lecture seule par d√©faut permet aux utilisateurs de visualiser toutes les ressources sans apporter de modifications. Pour attribuer des r√¥les personnalis√©s ou modifier les attributions de r√¥les, vous devez √©diter le ConfigMap RBAC d'ArgoCD. En configurant ces param√®tres, vous pouvez vous assurer que les utilisateurs sans mappages de r√¥les explicites se voient attribuer automatiquement un r√¥le lecture seule par d√©faut.</p> <p>Pour une compr√©hension plus approfondie du contr√¥le d'acc√®s bas√© sur les r√¥les (RBAC) dans ArgoCD et pour explorer des configurations d√©taill√©es, r√©f√©rez-vous √† la documentation officielle d'ArgoCD.</p> <p>Lectures suppl√©mentaires</p> <p>Pour plus d'informations sur la gestion s√©curis√©e des utilisateurs et la configuration RBAC dans ArgoCD, explorez des ressources suppl√©mentaires sur RBAC dans Kubernetes et les meilleures pratiques en mati√®re de s√©curit√© cloud-native.</p>","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/#_3","level":3,"title":"Argo CD","text":"","path":["ARGO","ARGO CD","Argo CD"],"tags":["Argo","Argo CD"]},{"location":"argo/argo_cd/argo-cd-autopilot/","level":1,"title":"Create a project","text":"<pre><code>kind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nname: management\nnetworking:\n  apiServerAddress: 192.168.1.23\n  apiServerPort: 6443\n\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nname: production\nnetworking:\n  apiServerAddress: 192.168.1.23\n  apiServerPort: 6444\n\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nname: staging\nnetworking:\n  apiServerAddress: 192.168.1.23\n  apiServerPort: 6445\n</code></pre> <pre><code>kind create cluster --config management.yaml\nkind create cluster --config staging.yaml\nkind create cluster --config production.yaml\n</code></pre> <pre><code>kctx kind-management\n</code></pre> <pre><code>‚ùØ kubectl config set-context --current --namespace=argocd\nContext \"kind-management\" modified.\n‚ùØ argocd login --core\nContext 'kubernetes' updated\n‚ùØ argocd cluster add kind-production\nWARNING: This will create a service account `argocd-manager` on the cluster referenced by context `kind-production` with full cluster level privileges. Do you want to continue [y/N]? y\nINFO[0001] ServiceAccount \"argocd-manager\" created in namespace \"kube-system\"\nINFO[0001] ClusterRole \"argocd-manager-role\" created\nINFO[0001] ClusterRoleBinding \"argocd-manager-role-binding\" created\nINFO[0001] Created bearer token secret for ServiceAccount \"argocd-manager\"\nCluster 'https://127.0.0.1:35999' added\n‚ùØ argocd cluster add kind-staging\nWARNING: This will create a service account `argocd-manager` on the cluster referenced by context `kind-staging` with full cluster level privileges. Do you want to continue [y/N]? y\nINFO[0001] ServiceAccount \"argocd-manager\" created in namespace \"kube-system\"\nINFO[0001] ClusterRole \"argocd-manager-role\" created\nINFO[0001] ClusterRoleBinding \"argocd-manager-role-binding\" created\nINFO[0001] Created bearer token secret for ServiceAccount \"argocd-manager\"\nCluster 'https://127.0.0.1:37981' added\n</code></pre> <pre><code>export GIT_T0KEN=XXXX\nexport GIT_REPO=https://github.com/Mathod95/argocd-autopilot\nargocd-autopilot repo bootstrap\n</code></pre> recover existing repo<pre><code>argocd-autopilot repo bootstrap --recover\n</code></pre>","path":["Create a project"],"tags":[]},{"location":"argo/argo_cd/argo-cd-autopilot/#create-a-project","level":1,"title":"Create a project","text":"<pre><code>argocd-autopilot project create production\nargocd-autopilot project create staging\nargocd-autopilot project create management\n</code></pre> <p>This will create the¬†<code>production</code>,<code>staging</code>and <code>management</code> AppProject¬†and¬†ApplicationSet.  You should see that it was pushed to your installation repository under: - /projects/production.yaml - /project/staging.yaml - /project/management.yaml</p>","path":["Create a project"],"tags":[]},{"location":"argo/argo_cd/argo-cd-autopilot/#add-an-application","level":1,"title":"Add an Application","text":"<pre><code>argocd-autopilot app create hello-world --app github.com/argoproj-labs/argocd-autopilot/examples/demo-app/ -p production --wait-timeout 2m\nargocd-autopilot app create hello-world --app github.com/argoproj-labs/argocd-autopilot/examples/demo-app/ -p staging --wait-timeout 2m\n</code></pre>","path":["Create a project"],"tags":[]},{"location":"argo/argo_cd/argo-cd-autopilot/#usefull-link","level":1,"title":"Usefull link","text":"<ul> <li>https://www.youtube.com/watch?v=nR-i0Hn6trw</li> <li>https://argocd-autopilot.readthedocs.io/en/stable/Getting-Started/</li> <li>https://github.com/argoproj/argocd-example-apps</li> </ul>","path":["Create a project"],"tags":[]},{"location":"argo/argo_cd/introduction/","level":1,"title":"Introduction","text":"","path":["ARGO","ARGO CD","Introduction"],"tags":[]},{"location":"argo/argo_cd/introduction/#chapter-overview-and-objectives","level":2,"title":"Chapter Overview and Objectives","text":"<p>In this chapter, we explore the detailed aspects of Argo CD, a pivotal tool within the Argo suite, designed specifically for Continuous Delivery (CD) and GitOps practices. Through thematic discussions and hands-on labs, we aim to equip you with a comprehensive understanding of Argo CD's architecture, installation, configuration, security considerations, and its extensibility.</p> <p>By the end of this chapter, you should be able to:</p> <p>Explain how Argo CD leverages the GitOps principles. Describe the core components of Argo CD, their roles and tasks. Install and configure Argo CD on a Kubernetes cluster. Deploy and update applications using Argo CD.</p>","path":["ARGO","ARGO CD","Introduction"],"tags":[]},{"location":"argo/argo_cd/lab/labs01/","level":1,"title":"Installing Argo CD","text":"","path":["Installing Argo CD"],"tags":[]},{"location":"argo/argo_cd/lab/labs02/","level":1,"title":"Managing Applications with Argo CD","text":"","path":["Managing Applications with Argo CD"],"tags":[]},{"location":"argo/argo_cd/lab/labs03/","level":1,"title":"Argo CD Security and RBAC","text":"","path":["Argo CD Security and RBAC"],"tags":[]},{"location":"argo/argo_events/","level":1,"title":"ARGO EVENTS","text":"<p>Event driven workflow automation framework for kubernetes</p> <p>Event Sources</p> <p>Webhooks AWS S3 AWS SQS/SNS Kafka NATS GCP Pub/Sub Schedules ...</p> <p>EventSources Consumes &amp; Transforms Events</p> <p>Event Configuration CloudEvents Format</p> <p>EventBus</p> <p>NATS Streaming Pub/Sub System</p> <p>sensor</p> <p>Triggers</p> <p>Event Sources &gt; EventSource &gt; EventBus &gt; Sensor</p> <p> </p> <ul> <li>EventSource: Publishes events </li> <li>EventBus: Acts as a central highway for all events</li> <li>EventSource: Subscribe to events</li> </ul> <p>Argo Events utilise NATS streaming</p>","path":["ARGO","Argo Events"],"tags":["Argo Events"]},{"location":"argo/argo_events/#utiliser-jetstream-a-lavenir","level":2,"title":"(Utiliser JetStream √† l'avenir)","text":"","path":["ARGO","Argo Events"],"tags":["Argo Events"]},{"location":"argo/argo_events/#introduction","level":2,"title":"Introduction","text":"","path":["ARGO","Argo Events"],"tags":["Argo Events"]},{"location":"argo/argo_events/#chapter-overview-and-objectives","level":3,"title":"Chapter Overview and Objectives","text":"<p>In this chapter, we discuss Argo Events, exploring its role in implementing event-driven architecture within Kubernetes. Starting with a conceptual overview, we'll understand the key components of Argo Events‚ÄîEvent Sources, Sensors, EventBus, and Triggers, and their significance in Kubernetes. The chapter then transitions to practical learning, with labs focused on configuring event sources and triggers, and integrating Argo Events with external systems like webhooks and message queues.</p> <p>By the end of this chapter, you should be able to:</p> <ul> <li>Learn how event sources initiate the event-driven process in Kubernetes.</li> <li>Understand the detection and response mechanism of sensors and triggers in event-driven systems.</li> <li>Grasp the importance of the EventBus in managing event flow within Argo Events.</li> </ul>","path":["ARGO","Argo Events"],"tags":["Argo Events"]},{"location":"argo/argo_events/#the-main-components","level":2,"title":"The Main Components","text":"","path":["ARGO","Argo Events"],"tags":["Argo Events"]},{"location":"argo/argo_events/#event-driven-architecture","level":3,"title":"Event-Driven Architecture","text":"<p>In this section, we explore the concept of event-driven architecture (EDA) and its practical application in Kubernetes environments. Unlike traditional architectures where components operate in a linear, request-response manner, EDA is based on a more dynamic and fluid model. This model is particularly relevant in Kubernetes, a system that manages containerized applications across clusters and thrives on responsiveness and adaptability.</p> <p>At the core of Kubernetes are events - these are various actions or changes within the system, like pod lifecycle changes or service updates. EDA in Kubernetes involves responding to these events in a way that's both automated and scalable. This method of operation allows for a more efficient handling of the ever-changing state within a Kubernetes cluster.</p> <p>Argo Events enters the picture as a tool designed for Kubernetes, aimed at facilitating the implementation of event-driven paradigms. It isn't just an add-on but rather an integration that amplifies Kubernetes' capabilities. Let's take a look at the main components of Argo Events.</p> <p>Event Source: This is where events are generated. Event sources in Argo Events can be anything from a simple webhook or a message from a message queue, to a scheduled event. Understanding event sources is key to knowing how your system will interact with various external and internal stimuli. An example of an event source is provided below:</p> <p>apiVersion: argoproj.io/v1alpha1 kind: EventSource metadata:   name: webhook-example spec:   service:     ports:       - port: 12000         targetPort: 12000   webhook:     example-endpoint:       endpoint: /example       method: POST</p> <p>Sensor: Sensors are the event listeners in Argo Events. They wait for specific events from the event sources and, upon detecting these events, trigger predefined actions. Understanding sensors involves knowing how to respond to different types of events. A sensor would be specified with the following spec:</p> <p>apiVersion: argoproj.io/v1alpha1 kind: Sensor metadata:   name: webhook-sensor spec:   dependencies:     - name: example-dep       eventSourceName: webhook-example       eventName: example-endpoint   triggers:     - template:         name: k8s-trigger         k8s:           group: batch           version: v1           resource: jobs           operation: create           source:             resource:               apiVersion: batch/v1               kind: Job               metadata:                 generateName: webhook-job-               spec:                 template:                   spec:                     containers:                       - name: hello                         image: busybox                         command: [\"echo\", \"Hello from Argo Sensor!\"]                     restartPolicy: Never</p> <p>EventBus: The EventBus acts as a backbone for event distribution within Argo Events. It's responsible for managing the delivery of events from sources to sensors. Understanding the EventBus is crucial for managing the flow of events within your system.</p> <p>apiVersion: argoproj.io/v1alpha1 kind: EventBus metadata:   name: default spec:   nats:     native:       replicas: 1</p> <p>Trigger: Triggers in Argo Events are the mechanisms that respond to events detected by sensors. They can perform a wide range of actions, from starting a workflow to updating a resource. Understanding triggers is essential for automating responses to events. Triggers are defined within a sensor specification, so the following excerpt focuses on the trigger itself:</p> <p>trigger:   template:     name: argo-workflow-trigger     argoWorkflow:       source:         resource:           apiVersion: argoproj.io/v1alpha1           kind: Workflow           metadata:             generateName: hello-world-           spec:             entrypoint: whalesay             templates:             - name: whalesay               container:                 image: docker/whalesay:latest                 command: [cowsay]                 args: [\"Hello from Argo Events!\"]</p> <p>Architecture of Argo Events</p> <p>The image depicts the architecture of Argo Events, showing three main components: Event Source, Event Bus, and Sensor, each with a controller and deployment. The Event Source receives various events (like SNS, SQS, GCP PubSub, S3, Webhooks, etc.), which are managed by the Event Source Controller and passed on to the Event Source Deployment. This connects to the Event Bus with NATS Streaming through the Event Bus Controller. Finally, the Sensor Controller manages the Sensor Deployment, which triggers workflows in Kubernetes and functions in AWS Lambda, illustrated by respective icons.</p>","path":["ARGO","Argo Events"],"tags":["Argo Events"]},{"location":"argo/argo_events/#lab-exercises","level":2,"title":"Lab Exercises","text":"","path":["ARGO","Argo Events"],"tags":["Argo Events"]},{"location":"argo/argo_events/#lab-61-setting-up-event-triggers-with-argo","level":3,"title":"Lab 6.1. Setting Up Event Triggers with Argo","text":"<p>Lab Tips and Best Practices When working on the lab exercises, please keep the following in mind:</p> <p>When accessing external URLs embedded in the PDF document below, always use right-click and open in a new tab or window. Attempting to open the URL by directly clicking on it will close your course window/tab. Depending on your PDF viewer, if you are cutting and pasting from the document, you may lose the original formatting. For example, underscores might disappear and be replaced by spaces. Therefore, you may need to manually edit the text. Always double-check that the pasted text is correct.</p>","path":["ARGO","Argo Events"],"tags":["Argo Events"]},{"location":"argo/argo_events/#lab-62-integrating-argo-events-with-external-systems","level":3,"title":"Lab 6.2. Integrating Argo Events with External Systems","text":"<p>Lab Tips and Best Practices When working on the lab exercises, please keep the following in mind:</p> <p>When accessing external URLs embedded in the PDF document below, always use right-click and open in a new tab or window. Attempting to open the URL by directly clicking on it will close your course window/tab. Depending on your PDF viewer, if you are cutting and pasting from the document, you may lose the original formatting. For example, underscores might disappear and be replaced by spaces. Therefore, you may need to manually edit the text. Always double-check that the pasted text is correct.</p>","path":["ARGO","Argo Events"],"tags":["Argo Events"]},{"location":"argo/argo_rollouts/concepts/","level":1,"title":"Argo Rollouts Architecture and Core Components","text":"","path":["ARGO","ARGO ROLLOUTS","Argo Rollouts Architecture and Core Components"],"tags":[]},{"location":"argo/argo_rollouts/concepts/#building-blocks-of-argo-rollouts","level":2,"title":"Building Blocks of Argo Rollouts","text":"<p>In this section, we will discuss the building blocks of Argo Rollouts. To give you an overview of what to expect, we‚Äôll briefly describe the relevant components of an Argo Rollouts setup before we discover them in more detail.</p> <p></p> <p>Argo Rollouts Components</p> <ul> <li>Argo Rollouts Controller: An operator that manages Argo Rollout Resources. It reads all the details of a rollout (and other resources) and ensures the desired cluster state.</li> <li>Argo Rollout Resource: A custom Kubernetes resource managed by the Argo Rollouts Controller. It is largely compatible with the native Kubernetes Deployment resource, adding additional fields that manage the stages, thresholds, and techniques of sophisticated deployment strategies, including canary and blue-green deployments.</li> <li>Ingress and the Gateway API: The Kubernetes Ingress resource is used to enable traffic management for various traffic providers such as service meshes (e.g., Istio or Linkerd) or Ingress Controllers (e.g., Nginx Ingress Controller). The Kubernetes Gateway API is also supported with a separate plugin and provides similar functionality.</li> <li>Service: Argo Rollouts utilizes the Kubernetes Service resource to redirect ingress traffic to the respective workload version by adding specific metadata to a Service.</li> <li>ReplicaSet: Standard Kubernetes ReplicaSet resource used by Argo Rollouts to keep track of different versions of an application deployment.</li> <li> <p>AnalysisTemplate and AnalysisRun: Analysis is an optional feature of Argo Rollouts and enables the connection of Rollouts to a monitoring system. This allows automation of promotions and rollbacks. To perform an analysis an AnalysisTemplate defines a metric query and their expected result. If the query matches the expectation, a Rollout will progress or rollback automatically, if it doesn‚Äôt. An AnalysisRuns is an instantiation of an AnalysisTemplate (similar to Kubernetes Jobs).</p> </li> <li> <p>Metric Providers: Metric providers can be used to automate promotions or rollbacks of a rollout. Argo Rollouts provides native integration for popular metric providers such as Prometheus and other monitoring systems.</p> </li> </ul> <p>Please note, that not all of the mentioned components are mandatory to every Argo Rollouts setup. The usage of Analysis resources or metric providers is entirely optional and relevant for more advanced use cases. Also note that the Argo Rollouts components are independent of other Argo projects (like Argo CD or Argo Workflows) and do not require them to function properly.</p>","path":["ARGO","ARGO ROLLOUTS","Argo Rollouts Architecture and Core Components"],"tags":[]},{"location":"argo/argo_rollouts/concepts/#a-refresher-the-kubernetes-replica-set","level":2,"title":"A Refresher: The Kubernetes Replica Set","text":"<p>A Refresher: The Kubernetes Replica Set To grasp the workings of Argo Rollouts in handling workloads, it's essential to understand some basics of Kubernetes. Essentially, Argo Rollouts functions in a manner quite similar to Kubernetes Deployment resources. What is less commonly known is that Deployments provide another layer of abstraction for workload management. The Deployment resource was a relatively later addition to Kubernetes, debuting in version 1.5 as part of the apps/v1beta1 API and achieving stability in version 1.9 with the apps/v1 API. Before the introduction of Deployments, workload management was accomplished using ReplicaSets. And under the hood, they are used until today!</p> <p>A Kubernetes ReplicaSet is a resource used to ensure that a specified number of pod replicas are running at any given time. Essentially, it's a way to manage the lifecycle of pods. The main function of a ReplicaSet is to maintain a stable set of pod replicas running at any given time. It does so by scheduling pods as needed to reach the desired number.</p> <p>If a pod fails, the ReplicaSet will replace it; if there are more pods than needed, it will terminate the extra pods. ReplicaSets are used to achieve redundancy and high availability within Kubernetes applications.</p> <p>For more sophisticated orchestration like rolling updates, rollbacks or scaling a ReplicaSet is not enough. Kubernetes introduced a higher-level (and usually better known) concept called Deployment resource that manages both the deployment and updating of applications.</p> <p>A deployment is managed by the Kubernetes deployment controller and is responsible for updating ReplicaSets by providing declarative updates for them.</p> <p>Lets create a Deployment of nginx proxies to demonstrate the ownership between Deployment and ReplicaSet.</p> <pre><code>kubectl create deploy nginx-deployment --image=nginx --replicas=3\ndeployment.apps/nginx-deployment created\n</code></pre> <p>Now make sure it properly scaled up.</p> <pre><code>kubectl get deployment\nNAME               READY   UP-TO-DATE   AVAILABLE   AGE\nnginx-deployment   3/3     3            3           47s\n</code></pre> <pre><code>$ kubectl get replicaset\nNAME                          DESIRED   CURRENT   READY   AGE\nnginx-deployment-66fb7f764c   3         3         3       47s\n</code></pre> <p>The ReplicaSet nginx-deployment-66fb7f764c is managed by nginx-deployment. You can tell this by inspecting the ReplicaSet with the following command:</p> <pre><code>kubectl get replicaset nginx-deployment-66fb7f764c -ojsonpath='{.metadata.ownerReferences}' | jq\n[\n  {\n    \"apiVersion\": \"apps/v1\",\n    \"blockOwnerDeletion\": true,\n    \"controller\": true,\n    \"kind\": \"Deployment\",\n    \"name\": \"nginx-deployment\",\n    \"uid\": \"1dd44efd-aab5-4475-aff2-32670201e2ef\"\n  }\n]\n</code></pre> <p>As we see, the ownerReferences of the ReplicaSet state, that this resource is ‚Äúowned‚Äù by a Deployment resource with the uid ‚Äú1dd44efd-aab5-4475-aff2-32670201e2ef‚Äù. And indeed, this uid matches with the other Deployment we just created.</p> <p><pre><code>kubectl get deployment nginx-deployment -ojsonpath='{.metadata.uid}'\n1dd44efd-aab5-4475-aff2-32670201e2ef\n</code></pre> Deployments are a great invention of vanilla Kubernetes and are a successful abstraction. Rarely do people manage their pods manually through ReplicaSets. Deployments are the standard.</p> <p>But despite all the praise, Deployment resources are still limited in their capabilities. They still do not support all deployment strategies we described in the previous section, ‚ÄúA Primer on Progressive Delivery‚Äù.</p>","path":["ARGO","ARGO ROLLOUTS","Argo Rollouts Architecture and Core Components"],"tags":[]},{"location":"argo/argo_rollouts/concepts/#argo-rollouts","level":2,"title":"Argo Rollouts","text":"<p>Here, we will explore the Argo Rollouts resource, which is the central element in Argo Rollouts, enabling advanced deployment strategies. A Rollout, in essence, is a Kubernetes resource that closely mirrors the functionality of a Kubernetes Deployment object. However, it steps in as a more advanced substitute for Deployment objects, particularly in scenarios demanding intricate deployment of progressive delivery techniques.</p>","path":["ARGO","ARGO ROLLOUTS","Argo Rollouts Architecture and Core Components"],"tags":[]},{"location":"argo/argo_rollouts/concepts/#key-features-of-argo-rollouts","level":2,"title":"Key Features of Argo Rollouts","text":"<p>Argo Rollouts outshine regular Kubernetes Deployments with several enhanced features.</p> <p>Argo Rollouts Functionalities:</p> <ul> <li>Blue-green deployments: This approach minimizes downtime and risk by switching traffic between two versions of the application.</li> <li>Canary deployments: Gradually roll out changes to a subset of users to ensure stability before full deployment.</li> <li>Advanced traffic routing: Integrates seamlessly with ingress controllers and service meshes, facilitating sophisticated traffic management.</li> <li>Integration with metric providers: Offers analytical insights for blue-green and canary deployments, enabling informed decisions.</li> <li>Automated decision making: Automatically promote or roll back deployments based on the success or failure of defined metrics.</li> </ul> <p>The Rollout resource is a custom Kubernetes resource introduced and managed by the Argo Rollouts Controller. This Kubernetes controller monitors resources of type Rollout and ensures that the described state will be reflected in the cluster.</p> <p>The Rollout resource maintains high compatibility with the conventional Kubernetes Deployment resource but is augmented with additional fields. These fields are instrumental in governing the phases, thresholds, and methodologies of advanced deployment approaches, such as canary and blue-green strategies.</p> <p>It‚Äôs crucial to understand that the Argo Rollouts controller is attuned exclusively to changes in Rollout resources. It remains inactive for standard deployment resources. Consequently, to use the Argo Rollouts for existing Deployments, a migration from traditional Deployments to Rollouts is required.</p> <p>Overall, Deployment and Rollout resources look pretty similar. Refer to the following table to understand the minimal differences between both.</p> Deployment Resource Argo Rollout Resource Comment Basic resource metadata. replicas: 3 replicas: 3 Number of desired pods. Defaults to 1. Label selector for pods. Describes the pod template that will be used to instantiate pods. The template does not differ. A Deployment strategy can be either ‚ÄúRollingUpdate‚Äù (default) or ‚ÄúRecreate‚Äù. A Rollout strategy can either be ‚ÄúblueGreen‚Äù or ‚Äúcanary‚Äù <p>Of course, there are way more configuration options to control the behavior of a Rollout. Please refer to the official Argo Rollouts specification for more options.</p>","path":["ARGO","ARGO ROLLOUTS","Argo Rollouts Architecture and Core Components"],"tags":[]},{"location":"argo/argo_rollouts/concepts/#migrating-existing-deployments-to-rollouts","level":2,"title":"Migrating Existing Deployments to Rollouts","text":"<p>The similarity of Deployments and Rollouts spec makes it easier to convert from one to the other resource type. Argo Rollouts supports a great way to migrate existing Deployment resources to Rollouts.</p> <p>By providing a spec.workloadRef instead of spec.template a Rollout can refer to a Deployments template:</p> <p><pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Rollout\nmetadata:\n  name: nginx-rollout\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  workloadRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: nginx-deployment\n[...]\n</code></pre> The Rollout will fetch the template information from the Deployment (in our example named nginx-deployment) and start the in the Rollout specified number of pods.</p> <p>Please note, that lifecycles of Deployment and Rollouts are distinct and managed by their respective controllers. This means that the Kubernetes Deployment controller will not start to manage Pods created by the Rollout. Also, the Rollout will not start to manage pods that are controlled by the Deployment.</p> <p>This enables a zero-downtime introduction of Argo Rollouts to your existing cluster. It furthermore makes experimentation with multiple deployment scenarios possible.</p>","path":["ARGO","ARGO ROLLOUTS","Argo Rollouts Architecture and Core Components"],"tags":[]},{"location":"argo/argo_rollouts/concepts/#discussion-create-rollouts-or-reference-deployments-from-rollouts","level":2,"title":"Discussion: Create Rollouts or Reference Deployments from Rollouts?","text":"<p>As Rollout resources can exist and operate without vanilla Deployments, the following question might arise: Should I always reference Deployments or is it better to start over with an independent Rollout resource, without the dependency of a reference?</p> <p>And the simple answer to it is‚Ä¶ it depends.</p> <p>Generally, workloadRef has been invented to enable a simple and seamless way of migrating from Deployments to Rollouts. We even consider it useful as Administrators who are unfamiliar with Argo Rollouts might be confused if they see an array of Pods running but neither a running Deployment nor StatefulSet. To lower the barrier, referencing existing Deployments from a Rollout can be a good option.</p> <p>If you use Deployment referencing, the Argo controller will copy the generation number of the referenced Deployment and stores it in a status field called workloadObservedGeneration. Therefore the rollouts own rollout.argoproj.io/workload-generation annotation should always match the generation of the deployment. This helps to identify deviation due to manipulation of either of the resources.</p> <p>However, referencing comes at the cost of another resource dependency. Yet another resource to check in case of failure!</p> <p>So, if you are sure you want to work with Argo Rollouts, use the native Rollout Resource.</p> <p>Hint: It is also possible to migrate a Rollout resource to a native Deployment. Please refer to the official documentation for further information.</p> <p>Additional learning resources:</p> <p>To explore the detailed specification of a Rollout, visit Argo Rollouts Specification. For guidance on transitioning from a Deployment to a Rollout, consult Migrating a Deployment to Rollout.</p>","path":["ARGO","ARGO ROLLOUTS","Argo Rollouts Architecture and Core Components"],"tags":[]},{"location":"argo/argo_rollouts/concepts/#ingress-and-service-resources","level":2,"title":"Ingress and Service Resources","text":"<p>Relevant Resources for Traffic Routing</p> <p>Kubernetes Ingress: A Kubernetes Ingress is a Kubernetes native resource that manages external access to services in a cluster (typically via HTTP). An Ingress allows defining rules for inbound connections to reach cluster-internal Kubernetes Services. As such, they are an important abstraction to programmatically control the flow of incoming network traffic. They can even be used for SSL/TLS termination.</p> <p>This approach is expanded by the Kubernetes Gateway API. The Gateway API splits the Ingress approach into the Kubernetes Gateway and Kubernetes HTTPRoute - the latter of which is managed by Argo Rollouts as it did Ingress. The advantage is that the Gateway API provides additional modes of access beyond HTTP/HTTPS and does not require controller-specific code like Ingress did.</p> <p>Kubernetes Service: A Kubernetes Service is a resource that abstracts how to expose an application running on a set of Pods. Services can load-balance traffic and provide service discovery within the cluster. The primary role of a Service is to provide a consistent IP address and port number for accessing the running application, irrespective of the changes in the pods.</p> <p>In the context of Argo Rollouts, these resources play a pivotal role when it comes to, for example, canary deployments. The general behavior of Service and Ingress resources is no different when used with Argo. Argo Rollouts uses Kubernetes Services to manage traffic flow to different versions of an application during a rollout process and they do so by augmenting the service with additional metadata.</p> <p>Pod Template Hash: Argo Rollouts utilizes the Pod Template Hash, which uniquely identifies Pods of a common ReplicaSset. So to switch incoming traffic from the ‚Äúold‚Äù ReplicaSet to our new ReplicaSet, the Argo Rollouts controller mutates the Service spec.selector to match the new Pod Template Hash.</p> <p>Kubernetes Services have selectors that find matching pods according to their label set; the pod-template-hash label is added to every ReplicaSet and used to make routing decisions</p> <p>Stable/Canary ReplicaSets: By introducing a ‚Äústable service‚Äù and ‚Äúcanary services‚Äù in the Rollouts Spec, Argo can not only switch the traffic to Stable/Canary ReplicaSets, but also decide about the distribution of which ReplicaSet should receive how much traffic.</p>","path":["ARGO","ARGO ROLLOUTS","Argo Rollouts Architecture and Core Components"],"tags":[]},{"location":"argo/argo_rollouts/concepts/#rollout-analysis-experiments","level":2,"title":"Rollout Analysis &amp; Experiments","text":"<p>The ability to split traffic between stable and canary workloads is good. But how do we decide if the canary workload is performing well and is therefore considered \"stable\"? That's right, metrics! An operator would closely observe the monitoring system (e.g., Prometheus, VMWare Wavefront or others) for certain metrics that indicate the application is working well. If you're thinking that this \"observing metrics and making a decision\" could be automated, you're right!</p> <p>Argo Rollouts allows the user to run ‚ÄúAnalysis‚Äù during the progressive delivery process. It primarily focuses on evaluating and ensuring the success of deployment based on defined criteria. These criteria can include custom metrics of your specific metric monitoring provider (see the official documentation for a conclusive list of supported metric providers).</p> <p>The analysis process in Argo Rollouts involves following custom resources that work hand in hand with the already discussed resources.</p> <p>Table 5.4: Analysis Custom Resource Definitions</p> Templates Description/Use Case <code>AnalysisTemplate</code> This template defines the metrics to be queried and the conditions for success or failure. The AnalysisTemplate specifies what metrics should be monitored and the thresholds for determining the success or failure of a deployment. It can be parameterized with input values to make it more dynamic and adaptable to different situations. <code>AnalysisRun</code> An AnalysisRun is an instantiation of an AnalysisTemplate. It is a Kubernetes resource that behaves similarly to a job in that it runs to completion. The outcome of an AnalysisRun can be successful, failed, or inconclusive, and this result directly impacts the progression of the Rollout's update. If the AnalysisRun is successful, the update continues; if it fails, the update is aborted; and if it's inconclusive, the update is paused. <p>Analysis resources allow Argo Rollouts to make informed decisions during the deployment process, like promoting a new version, rolling back to a previous version, or pausing the rollout for further investigation based on real-time data and predefined success criteria.</p> <p>AnalysisRuns support various providers like Prometheus or multiple other monitoring solutions to obtain measurements for analysis. Those measurements can then be used to automate promotion decisions.</p> <p>Besides just looking at metrics, there are other ways to decide if your rollout is doing well. The most basic (but commonly used) one might be the Kubernetes ‚ÄúJob‚Äù provider: if a job is successful, the metric is considered ‚Äúsuccessful\". If the job returns with anything else than return code zero, the metric is considered ‚Äúfailed‚Äù.</p> <p>The Web provider helps with seamless integration to custom services to help make promotion decisions.</p> <p>Remember, it's not mandatory to use analysis and metrics when you're rolling out updates in Argo Rollouts.</p> <p>If you want, you can control the rollout yourself. This means you can stop or advance the rollout whenever you choose. You can do this through the API or the command line. Also, you don't have to rely on automatic metrics for using Argo Rollouts. It's totally fine to combine automatic steps, like those based on analysis, with your own manual steps.</p>","path":["ARGO","ARGO ROLLOUTS","Argo Rollouts Architecture and Core Components"],"tags":[]},{"location":"argo/argo_rollouts/concepts/#experiments","level":2,"title":"Experiments","text":"<p>Experiments are an extended feature of Argo Rollouts designed to test and evaluate changes in two or more versions of an application in a controlled, temporary environment. The Experiment custom resource can launch AnalysisRuns alongside ReplicaSets. This is useful to confirm that new ReplicaSets are running as expected.</p> <p>You can use experiments in Argo Rollouts to test different versions of your app at the same time. This is like doing A/B/C testing. You can set up each experiment with its own version of the app to see which one works best. Each experiment uses a template to define its specific version of the app.</p> <p>The great thing about these experiments is that you can run several of them simultaneously, and each one is separate from the others. This means they don't interfere with each other.</p> <p>To learn more about Analysis or Experiments, please consult the official documentation.</p> <p>Analysistemplate</p> <p>Analysis Tempalte : A reusable blueprint for health checks - defines metrics, data sources (example Prometheus), and success or failure conditions</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: AnalysisTemplate\nmetadata:\n  name: api-performance-check\nspec:\n  args:\n  - name: service-name\n  - name: namespace\n  metrics:\n  - name: check-success-rate\n    provider:\n      prometheus:\n        address: http://prometheus.kub-prometheus-stack.svc.cluster.local.9090\n        query: |\n          sum(rate(http_requests_total{service_name=\"{{args.service-name}}\",namespace=\"{{args.namespace}}\",code!~\"5.*\"}[2m]))\n          /\n          sum(rate(http_requests_total{service_name=\"{{args.service-name}}\",namespace=\"{{args.namespace}}\"[2m]))\n    count: 3 #(1)!\n    interval: 20s #(2)!\n    failureLimite: 1 #(3)!\n    successCondition: result &gt;= 0.99 #(4)!\n</code></pre> <ol> <li>Indique d'√©xectuter 3 fois la query </li> <li>La query seras executer avec 20s d'intervalle</li> <li>Si une seul des trois mesures √©choue alors l'ensemble de l'analyse est consid√©rer comme √©chou√©</li> <li>Uniquement r√©ussi si 0.99 atteint</li> </ol> <p>background Analysis Continious monitoring that runs parallel to deployment stages</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Rollout\nmetadata:\n  name: app-api\nspec:\n...\n  strategy:\n    canary:\n      analysis:\n        template:\n        - templateName: api-performance-check\n        startingStep: 2\n        args:\n        - name: service-name\n          value: app-svc.default.svc.cluster.local\n      steps:\n      - setWeight: 20\n      - pause: {duration: 10m}\n      - setWeight: 40\n      - pause: {duration: 10m}\n      - setWeight: 60\n      - pause: {duration: 10m}\n      - setWeight: 80\n      - pause: {duration: 10m}\n</code></pre> <p>Inline Analysis Discrete verification checkpoints that must pass before advancing</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Rollout\nmetadata:\n  name: app-api\nspec:\n...\n  strategy:\n    canary:\n      steps:\n      - setWeight: 20\n      - pause: {duration: 5m}\n      - analysis:\n          template:\n          - templateName: api-performance-check\n          args:\n          - name: service-name\n            value: app-svc.default.svc.cluster.local\n</code></pre> <p>Analysistemplate</p> <p>PrePromotion Test then switch - Validate before user exposure <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Rollout\nmetadata:\n  name: app-api\nspec:\n...\n  strategy:\n    blueGreen:\n      activeService: active-svc\n      previewService: preview-svc\n      prePromotionAnalysis:\n        template:\n        - templateName: api-performance-check\n        args:\n        - name: service-name\n          value: preview-svc.default.svc.cluster.local\n</code></pre></p> <p>PostPromotion Switch then verify - Validate with real traffic, auto rollback <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Rollout\nmetadata:\n  name: app-api\nspec:\n...\n  strategy:\n    blueGreen:\n      activeService: active-svc\n      previewService: preview-svc\n      scaleDownDelaySeconds: 600 # 10 minutes\n      postPromotionAnalysis:\n        template:\n        - templateName: api-performance-check\n        args:\n        - name: service-name\n          value: preview-svc.default.svc.cluster.local\n</code></pre></p>","path":["ARGO","ARGO ROLLOUTS","Argo Rollouts Architecture and Core Components"],"tags":[]},{"location":"argo/argo_rollouts/concepts/#demo-health-check","level":2,"title":"Demo: Health Check","text":"","path":["ARGO","ARGO ROLLOUTS","Argo Rollouts Architecture and Core Components"],"tags":[]},{"location":"argo/argo_rollouts/concepts/#demo-analysisrun","level":2,"title":"Demo AnalysisRun","text":"","path":["ARGO","ARGO ROLLOUTS","Argo Rollouts Architecture and Core Components"],"tags":[]},{"location":"argo/argo_rollouts/introduction/","level":1,"title":"Introduction","text":"","path":["ARGO","ARGO ROLLOUTS","Introduction"],"tags":[]},{"location":"argo/argo_rollouts/introduction/#chapter-overview-and-objectives","level":2,"title":"Chapter Overview and Objectives","text":"<p>In this chapter, we delve into Argo Rollouts, a pivotal tool within the Argo suite, designed specifically for Continuous Delivery (CD) and GitOps practices. Argo Rollouts can be used as a stand-alone tool and therefore does not require any prior knowledge of ArgoCD (or other Argo-related tools). Through thematic discussions and hands-on labs, we aim to equip you with a comprehensive understanding of Argo Rollouts‚Äô architecture, installation, and usage.</p> <p>By the end of this chapter, you should be able to:</p> <p>Understand and differentiate various Progressive Delivery patterns and decide when to use which. Have a thorough understanding of what Argo Rollouts is and in what scenarios it might help. Have an overview of Argo Rollouts architecture and functionality.</p>","path":["ARGO","ARGO ROLLOUTS","Introduction"],"tags":[]},{"location":"argo/argo_rollouts/progressiveDelivery/","level":1,"title":"Progressive Delivery","text":"","path":["ARGO","ARGO ROLLOUTS","Progressive Delivery"],"tags":[]},{"location":"argo/argo_rollouts/progressiveDelivery/#essentials-of-cicd-and-progressive-delivery-in-software-development","level":2,"title":"Essentials of CI/CD and Progressive Delivery in Software Development","text":"<p>Continuous Integration (CI), Continuous Delivery (CD), and Progressive Delivery are key concepts in modern software development, particularly in the context of DevOps and agile practices. They represent different stages or approaches in the software release process. We will discuss them more in this chapter.</p>","path":["ARGO","ARGO ROLLOUTS","Progressive Delivery"],"tags":[]},{"location":"argo/argo_rollouts/progressiveDelivery/#continuous-integration","level":2,"title":"Continuous Integration","text":"<p>Continuous Integration is a development practice where developers frequently integrate their code into a shared repository, preferably several times daily. Each integration is then verified by an automated build and automated tests.</p> <p>CI Features</p> <ul> <li>Frequent code commits: Encourage developers to often integrate their code into the main branch, reducing integration challenges.</li> <li>Automated tests: Cover frequent code commits. Automatically running tests on the new code to ensure it integrates well with the existing codebase. This does not only include unit tests, but also any other higher-order testing method, such as integration- or end-to-end tests.</li> <li>Immediate problem detection: Allows for quick detection and fixing of integration issues.</li> <li>Reduced integration problems: Help to minimize the problems associated with integrating new code.</li> </ul> <p>The main goal of CI is to provide rapid feedback so that if a defect is introduced into the code base, it is identified and corrected as soon as possible.</p> <p>Once code is in our main branch, it is not deployed in production or even released. This is where the concept of Continuous Delivery comes into play.</p>","path":["ARGO","ARGO ROLLOUTS","Progressive Delivery"],"tags":[]},{"location":"argo/argo_rollouts/progressiveDelivery/#continuous-delivery","level":2,"title":"Continuous Delivery","text":"<p>Continuous Delivery is an extension of CI, ensuring the software can be reliably released anytime. It involves the automation of the entire software release process.</p> <p>CD Features:</p> <ul> <li>Automated release process: Every change that passes the automated tests can be released to production through an automated process.</li> <li>Reliable deployments: Ensure that the software is always in a deployable state.</li> <li>Rapid release cycles: Facilitate frequent and faster release cycles.</li> <li>Close collaboration between teams: A close alignment between development, QA, and operations teams is required.</li> </ul> <p>The objective of Continuous Delivery is to establish a process where software deployments become predictable, routine, and can be executed on demand.</p>","path":["ARGO","ARGO ROLLOUTS","Progressive Delivery"],"tags":[]},{"location":"argo/argo_rollouts/progressiveDelivery/#progressive-delivery_1","level":2,"title":"Progressive Delivery","text":"<p>Progressive delivery is often described as an evolution of continuous delivery. It focuses on releasing updates of a product in a controlled and gradual manner, thereby reducing the risk of the release, typically coupling automation and metric analysis to drive the automated promotion or rollback of the update.</p> <ul> <li> <p>Progressive Delivery Features:</p> <ul> <li>Canary releases: Gradually roll out the change to a small subset of users before rolling it out to the entire user base.</li> <li>Feature flags: Control who gets to see what feature in the application, allowing for selective and targeted deployment.</li> <li>Experiments &amp; A/B testing: Test different versions of a feature with different segments of the user base.</li> <li>Phased rollouts: Slowly roll out features to incrementally larger segments of the user base, monitoring and adjusting based on feedback.</li> </ul> </li> </ul> <p>The primary goal of Progressive Delivery is to reduce the risk associated with releasing new features and to enable faster iteration by getting early feedback from users.</p>","path":["ARGO","ARGO ROLLOUTS","Progressive Delivery"],"tags":[]},{"location":"argo/argo_rollouts/progressiveDelivery/#deployment-strategies","level":2,"title":"Deployment Strategies","text":"<p>Every software system is different, and deploying complex systems oftentimes requires additional steps and checks. This is why different deployment strategies emerged over time to manage the process of deploying new software versions in a production environment.</p> <p>These strategies are an integral part of DevOps practices, especially in the context of CI/CD workflows. The choice of a deployment strategy can significantly impact the availability, reliability, and user experience of a software application or software service.</p> <p>On the following pages, we will present the four most important deployment strategies and discuss their impact on user experience during deployment:</p> <ul> <li>Recreate</li> <li>Rolling update</li> <li>Blue-green deployment</li> <li>Canary deployment</li> </ul>","path":["ARGO","ARGO ROLLOUTS","Progressive Delivery"],"tags":[]},{"location":"argo/argo_rollouts/progressiveDelivery/#recreate","level":2,"title":"Recreate","text":"<p>Un d√©ploiement <code>Recreate</code> supprime l‚Äôancienne version de l‚Äôapplication avant de d√©marrer la nouvelle. Par cons√©quent, cela garantit que deux versions de l‚Äôapplication ne s‚Äôex√©cutent jamais en m√™me temps, mais il y a un temps d‚Äôarr√™t pendant le d√©ploiement.</p> <p></p> <p>Cette strat√©gie est une option de l‚Äôobjet Deployment de Kubernetes et convient aux environnements o√π un bref temps d‚Äôarr√™t est acceptable ou lorsque la persistance de l‚Äô√©tat n‚Äôest pas une pr√©occupation.</p>","path":["ARGO","ARGO ROLLOUTS","Progressive Delivery"],"tags":[]},{"location":"argo/argo_rollouts/progressiveDelivery/#rolling-update","level":2,"title":"Rolling Update","text":"<p>Une Rolling Update remplace progressivement les pods ex√©cutant l‚Äôancienne version du container par de nouveaux pods ex√©cutant la nouvelle version du container.. √Ä mesure que la nouvelle version est mise en service, les anciens pods sont r√©duits afin de maintenir le nombre total d‚Äôinstances de l‚Äôapplication, tout en surveillant la sant√© et la disponibilit√© du service apr√®s chaque √©tape. Cela permet de r√©duire le temps d‚Äôarr√™t et les risques, car la nouvelle version est d√©ploy√©e de mani√®re contr√¥l√©e.  </p> <p></p>","path":["ARGO","ARGO ROLLOUTS","Progressive Delivery"],"tags":[]},{"location":"argo/argo_rollouts/progressiveDelivery/#blue-green-deployment","level":2,"title":"Blue-Green deployment","text":"<p>A blue-green deployment has both the new and old versions of the application deployed at the same time. During this time, only the old version of the application will receive production traffic. This allows the developers to run tests against the new version before switching the live traffic to the new version. Once the new version is ready and tested, the traffic is switched (often at the load balancer level) from the old environment to the new one. The advantage here is a quick rollback in case of issues and minimal downtime during deployment.</p> <p>An important drawback of a blue-green deployment is, that twice the amount of instances is created during the time of the deployment. This is a common show-stopper for this pattern.</p> <p>To learn more about the blue-green deployment, see the article by Martin Fowler.</p> <p></p>","path":["ARGO","ARGO ROLLOUTS","Progressive Delivery"],"tags":[]},{"location":"argo/argo_rollouts/progressiveDelivery/#canary-deployment","level":2,"title":"Canary deployment","text":"<p>A small subset of users are directed to the new version of the application while the majority still use the old version. Based on the feedback and performance of the new version, the deployment is gradually rolled out to more users. This reduces risk by affecting a small user base initially, allows for A/B testing and real-world feedback. While this is technically possible in native Kubernetes by manually adjusting Service Selectors between the ‚Äúold‚Äù and ‚Äúnew‚Äù versions of a deployment, having an automated solution is more ideal.</p> <p>Some more detailed information can be found in the Canary Release article by Danilo Sato.</p> <p></p>","path":["ARGO","ARGO ROLLOUTS","Progressive Delivery"],"tags":[]},{"location":"argo/argo_rollouts/progressiveDelivery/#strategies-for-smooth-and-reliable-releases","level":2,"title":"Strategies for Smooth and Reliable Releases","text":"<p>In summary, deployment strategies are fundamental in modern software development and operations for ensuring smooth, safe, and efficient software releases. They cater to the need for balancing rapid deployment with the stability and reliability of production environments.</p> <p>Benefits of Introducing Deployment Strategies</p> Benefit Description Risk mitigation - They allow for safer deployments by reducing the risk of introducing bugs or performance issues into the production environment.  - Strategies like canary deployments enable gradual exposure to new changes. User experience - Maintaining a consistent and high-quality user experience is essential.  - Strategies like blue-green deployments minimize downtime and potential disruptions to the user experience. Feedback and testing - They provide a framework for gathering real-world user feedback.  - Canary deployments, in particular, are valuable for understanding how changes perform in a live environment. Rollback capabilities - In case new versions have critical issues, strategies like blue-green deployments allow for quick rollbacks to the previous stable version. <p>Common Use Cases for Each Strategy</p> Strategy Supported By Common Use Cases Fixed deployment Kubernetes Native - The most basic way to deploy a workload is whenever downtime is acceptable.  - Often stateful workloads (e.g., Databases) require a ‚Äúrecreation‚Äù to avoid data corruption. Rolling update Kubernetes Native - Commonly used for stateless, low-maintenance workloads like proxies, RESTful APIs, etc. Blue-green deployment Argo Rollouts - Use when a) you can afford the extra cost of running twice the resources and b) need a quick and easy rollback option.  - B/G can also be helpful for experimentation scenarios.  - Can be advantageous to update services that depend on stateful connections, e.g., via WebSockets. Canary deployment Argo Rollouts - Use it whenever a partial rollout is desirable (experimentation with a subset of users, desire a gradual rollout over hours or days, want to make rollout dependent on certain conditions).  - It can be a good alternative if the deployments are too large and the infra cost of running a full blue-green is too high.","path":["ARGO","ARGO ROLLOUTS","Progressive Delivery"],"tags":[]},{"location":"argo/argo_rollouts/labs/labs01/","level":1,"title":"Installing Argo Rollouts","text":"","path":["ARGO","ARGO ROLLOUTS","Labs","Installing Argo Rollouts"],"tags":[]},{"location":"argo/argo_rollouts/labs/labs01/#objective","level":2,"title":"Objective","text":"<p>Set up a local Kubernetes cluster using Kind, install Argo Rollouts, and understand how to access Argo Rollout resources.</p>","path":["ARGO","ARGO ROLLOUTS","Labs","Installing Argo Rollouts"],"tags":[]},{"location":"argo/argo_rollouts/labs/labs01/#prerequisites","level":2,"title":"Prerequisites","text":"<ul> <li>Basic understanding of Docker, Kubernetes, and command-line interface operations.</li> <li>Access to a computer with an internet connection.</li> <li>An installation of Kubernetes that you have full control over</li> <li>See Chapter 2‚Äôs Deploying Kubernetes for Argo section for details on how to set one up for yourself</li> </ul>","path":["ARGO","ARGO ROLLOUTS","Labs","Installing Argo Rollouts"],"tags":[]},{"location":"argo/argo_rollouts/labs/labs01/#install-cluster-and-argo-rollouts","level":2,"title":"Install Cluster and Argo Rollouts","text":"<p>NOTE: Steps 1-4 might not be necessary if you already followed the setup during a previous chapter.</p> <ol> <li> <p>Installing Docker Ensure Docker is installed and running on your machine. ‚óè Installation instructions can be found on the Docker website.</p> </li> <li> <p>Installing Kind Download and install Kind following the instructions from the Kind official website.</p> </li> <li> <p>Creating a Kubernetes Cluster with Kind Create a cluster by running the following command:</p> </li> </ol> <p><pre><code>kind create cluster\n</code></pre> This command creates a single-node Kubernetes cluster running inside a Docker container named <code>kind-kind</code>.</p> <ol> <li> <p>Installing kubectl Instructions for downloading kubectl can be found in the Kubernetes official documentation.</p> </li> <li> <p>Deploy Argo Rollouts Create a namespace for Argo Rollouts using the following command:</p> </li> </ol> <pre><code>kubectl create namespace argo-rollouts\n</code></pre> <p>Deploy Argo Rollouts using the quick start manifest:</p> <pre><code>kubectl apply -n argo-rollouts -f https://github.com/argoproj/argo-rollouts/releases/download/v1.8.3/install.yaml\n</code></pre> <p>This will install custom resource definitions as well as the Argo Rollouts controller. During this course we use Argo Rollouts in version 1.8.3. We recommend using the same version to ensure consistent results.</p> <p>Verify that Argo Rollouts is installed by running the following command:</p> <pre><code>kubectl get pods -n argo-rollouts\n</code></pre> <ol> <li>Install Rollouts kubectl Plugin</li> </ol> <p>Unlike Argo CD and Argo Workflows, Argo Rollouts uses a kubectl plugin as its CLI client. Download the latest Argo Rollouts kubectl plugin version from</p> <p>https://github.com/argoproj/argo-rollouts/releases/latest/.</p> <p>On Ubuntu 24.04, you can install the CLI using the following commands:</p> <pre><code>wget https://github.com/argoproj/argo-rollouts/releases/download/v1.8.3/kubectl-argo-rollouts-linux-amd64 -O kubectl-argo-rollouts\nchmod +x kubectl-argo-rollouts\nsudo mv kubectl-argo-rollouts /usr/local/bin/\n</code></pre> <p>More detailed installation instructions can be found via the CLI installation documentation.</p> <p>This is also available in Mac, Linux and WSL Homebrew. Use the following command:</p> <pre><code>brew install argoproj/tap/kubectl-argo-rollouts\n</code></pre> <p>Verify that the argo CLI is installed correctly by running the following command:</p> <pre><code>kubectl argo rollouts version\n</code></pre> <ol> <li>UI Dashboard For the sake of completeness it needs to be mentioned that Argo Rollouts ships with a fully fledged UI Dashboard. It can be accessed via the kubectl argo rollouts dashboard command and provides a nice overview and basic commands for administration.</li> </ol> <pre><code>kubectl argo rollouts dashboard\n</code></pre> <p>Output:</p> <pre><code>INFO[0000] Argo Rollouts Dashboard is now available at\nhttp://localhost:3100/rollouts\nINFO[0000] [core] [Channel #1 SubChannel #2]grpc:\naddrConn.createTransport failed to connect to {Addr: \"0.0.0.0:3100\",\nServerName: \"0.0.0.0:3100\", }. Err: connection error: desc =\n\"transport: Error while dialing: dial tcp 0.0.0.0:3100: connect:\nconnection refused\"\n</code></pre> <p>Despite any ‚Äúconnection refused‚Äù errors, you can now access it via the UI at http://localhost:3100 or your VM‚Äôs public IP address at port 3100.</p> <p>As the Dashboard is self-explanatory, we will not discuss it in detail during this course. The Argo Rollouts Dashboard displaying a sample app ‚Äúrollout-bluegreen‚Äù NOTE: If no rollout resources are in place, the dashboard will display ‚ÄúLoading‚Ä¶‚Äù.</p> <ol> <li> <p>Optional: Shell Auto-Completion To get easy access to Argo Rollout resources, the CLI can add shell completion code for several shells. For bash, you can use the following command: source &lt;(kubectl-argo-rollouts completion bash) Other shells are supported as well. Please refer to the completion command documentation for more details.</p> </li> <li> <p>Using Argo Rollouts There are a wide variety of commands that you can use to control Argo Rollouts via the CLI, as described in the -h output for the kubectl argo rollouts command. As a kubectl plugin, it uses the Kubernetes API to perform all management tasks. Here is a list of the most common commands to operate with Argo Rollouts:</p> </li> </ol> <pre><code>kubectl get rollout\nkubectl argo rollouts get rollout\nkubectl argo rollouts promote\nkubectl argo rollouts undo\n</code></pre>","path":["ARGO","ARGO ROLLOUTS","Labs","Installing Argo Rollouts"],"tags":[]},{"location":"argo/argo_rollouts/labs/labs02/","level":1,"title":"Argo Rollouts Blue-Green","text":"<p>Let‚Äôs dig into it by creating a blue-green deployment scenario. It enables us to verify a version upgrade before the live traffic hits our service. It is easy to understand and therefore one of the most commonly used ways to roll out new versions of software without any downtime.</p>","path":["ARGO","ARGO ROLLOUTS","Labs","Argo Rollouts Blue-Green"],"tags":[]},{"location":"argo/argo_rollouts/labs/labs02/#objective","level":2,"title":"Objective","text":"<p>This lab aims to give a reader an idea of the ‚Äúlook and feel‚Äù of Argo Rollouts. It will demonstrate how to realize a simple blue-green scenario with Argo Rollouts. As blue-green is the most basic deployment pattern that rollout supports, this is a great introduction to the fundamental functionality of Argo Rollouts.</p>","path":["ARGO","ARGO ROLLOUTS","Labs","Argo Rollouts Blue-Green"],"tags":[]},{"location":"argo/argo_rollouts/labs/labs02/#prerequisites","level":2,"title":"Prerequisites","text":"<ul> <li>Kubernetes cluster with the argo-rollouts controller</li> <li>kubectl with the argo-rollouts plugin (optional)</li> </ul>","path":["ARGO","ARGO ROLLOUTS","Labs","Argo Rollouts Blue-Green"],"tags":[]},{"location":"argo/argo_rollouts/labs/labs02/#creating-blue-green-deployments-with-argo-rollouts","level":2,"title":"Creating Blue-Green Deployments with Argo Rollouts","text":"<ol> <li>Install Resources For the beginning, let‚Äôs check for existing rollouts using this command:</li> </ol> <pre><code>kubectl get rollout\n</code></pre> <p>Output: No resources found in default namespace. As expected, there are no rollouts (yet) to be found in our cluster. Let's create one with the following command:</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: argoproj.io/v1alpha1\nkind: Rollout\nmetadata:\n  name: rollout-bluegreen\nspec:\n  replicas: 2\n  revisionHistoryLimit: 2\n  selector:\n    matchLabels:\n      app: rollout-bluegreen\n  template:\n    metadata:\n      labels:\n        app: rollout-bluegreen\n    spec:\n      containers:\n      - name: rollouts-demo\n      image: argoproj/rollouts-demo:blue\n      imagePullPolicy: Always\n      ports:\n      - containerPort: 8080\n  strategy:\n    blueGreen:\n      activeService: rollout-bluegreen-active\n      previewService: rollout-bluegreen-preview\n      autoPromotionEnabled: false\nEOF\n</code></pre> <p>Output: rollout.argoproj.io/rollout-bluegreen created Check whether the Rollout resource has been created running the command below: $ kubectl get rollout Output: NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE rollout-bluegreen 2 75s It has been created, but is not ready yet. Let‚Äôs explore the reasons behind using the Argo Rollouts kubectl plugin to see if we can better understand its functionality.</p> <p>The plugin enables us to get status information of a specific rollout and can be queried in the form: kubectl argo rollouts get ro . Command: $ kubectl argo rollouts get ro rollout-bluegreen Output: Name: rollout-bluegreen Namespace: default Status: ‚úñ Degraded Message: InvalidSpec: The Rollout \"rollout-bluegreen\" is invalid: spec.strategy.blueGreen.activeService: Invalid value: \"rollout-bluegreen-active\": service \"rollout-bluegreen-active\" not found Strategy: BlueGreen Replicas: Desired: 2 Current: 0 Updated: 0 Ready: 0 Available: 0 NAME KIND STATUS AGE INFO ‚ü≥ rollout-bluegreen Rollout ‚úñ Degraded 9s The resource status is degraded, as not all requirements are met: If we look closely in our rollout manifest we see that we defined two services activeService and previewService. We need to make sure that the named services are available. Let's create them with the command below: cat &lt;&lt;EOF | kubectl apply -f - apiVersion: v1 kind: Service <p>metadata: creationTimestamp: null labels: app: rollout-bluegreen-active name: rollout-bluegreen-active spec: ports: - name: \"80\" port: 80 protocol: TCP targetPort: 80 selector: app: rollout-bluegreen type: ClusterIP status: loadBalancer: {} EOF As mentioned, the rollouts resource references a second service. A so-called ‚Äúpreview‚Äù service. A preview service enables a preview stack to be reachable by an administrator. It does so without serving public traffic. If we want the preview to go live, we need to \"promote\" the rollout. \"Promotion\" refers to setting a service live. Therefore, we will create a preview service resource to be able to check our application before promotion (aka setting it live). Run the command below: cat &lt;&lt;EOF | kubectl apply -f - apiVersion: v1 kind: Service metadata: creationTimestamp: null labels: app: rollout-bluegreen-preview name: rollout-bluegreen-preview spec: ports: - name: \"80\" port: 80 protocol: TCP targetPort: 80 selector: app: rollout-bluegreen type: ClusterIP</p> <p>status: loadBalancer: {} EOF If we now check our rollout, we will eventually see a Healthy status. Run the command below: $ kubectl argo rollouts get ro rollout-bluegreen Output: Name: rollout-bluegreen Namespace: default Status: ‚úî Healthy Strategy: BlueGreen Images: argoproj/rollouts-demo:blue (stable, active) Replicas: Desired: 2 Current: 2 Updated: 2 Ready: 2 Available: 2 NAME KIND STATUS AGE INFO ‚ü≥ rollout-bluegreen Rollout ‚úî Healthy 50s ‚îî‚îÄ‚îÄ# revision:1 ‚îî‚îÄ‚îÄ‚ßâ rollout-bluegreen-5ffd47b8d4 ReplicaSet ‚úî Healthy 14s stable,active ‚îú‚îÄ‚îÄ‚ñ° rollout-bluegreen-5ffd47b8d4-mqc25 Pod ‚úî Running 4s ready:1/1 ‚îî‚îÄ‚îÄ‚ñ° rollout-bluegreen-5ffd47b8d4-q4bgf Pod ‚úî Running 3s ready:1/1 2. Perform an Upgrade Now that we have a running application, let's try to perform a version upgrade using the blue-green method. Therefore, we‚Äôll adjust our image to deploy argoproj/rollouts-demo:green instead of blue. Run the command below: cat &lt;&lt;EOF | kubectl apply -f - apiVersion: argoproj.io/v1alpha1 kind: Rollout metadata: name: rollout-bluegreen</p> <p>spec: replicas: 2 revisionHistoryLimit: 2 selector: matchLabels: app: rollout-bluegreen template: metadata: labels: app: rollout-bluegreen spec: containers: - name: rollouts-demo image: argoproj/rollouts-demo:green imagePullPolicy: Always ports: - containerPort: 8080 strategy: blueGreen: activeService: rollout-bluegreen-active previewService: rollout-bluegreen-preview autoPromotionEnabled: false EOF The Rollout status moves from ‚ÄúHealthy‚Äù to ‚ÄúPaused‚Äù, indicating that a rollout is in progress and waits for further action. Please note, that we explicitly set autoPromotionEnabled to false - we can skip the pausing phase and directly promote by setting this value to true. Run the command below: $ kubectl argo rollouts get ro rollout-bluegreen Output: Name: rollout-bluegreen Namespace: default Status: ‡•• Paused Message: BlueGreenPause Strategy: BlueGreen Images: argoproj/rollouts-demo:blue (stable, active) argoproj/rollouts-demo:green (preview) Replicas: Desired: 2 Current: 4 Updated: 2</p> <p>Ready: 2 Available: 2 NAME KIND STATUS AGE INFO ‚ü≥ rollout-bluegreen Rollout ‡•• Paused 91s ‚îú‚îÄ‚îÄ# revision:2 ‚îÇ ‚îî‚îÄ‚îÄ‚ßâ rollout-bluegreen-75695867f ReplicaSet ‚úî Healthy 6s preview ‚îÇ ‚îú‚îÄ‚îÄ‚ñ° rollout-bluegreen-75695867f-m6pxh Pod ‚úî Running 6s ready:1/1 ‚îÇ ‚îî‚îÄ‚îÄ‚ñ° rollout-bluegreen-75695867f-nr2rh Pod ‚úî Running 6s ready:1/1 ‚îî‚îÄ‚îÄ# revision:1 ‚îî‚îÄ‚îÄ‚ßâ rollout-bluegreen-5ffd47b8d4 ReplicaSet ‚úî Healthy 55s stable,active ‚îú‚îÄ‚îÄ‚ñ° rollout-bluegreen-5ffd47b8d4-mqc25 Pod ‚úî Running 45s ready:1/1 ‚îî‚îÄ‚îÄ‚ñ° rollout-bluegreen-5ffd47b8d4-q4bgf Pod ‚úî Running 44s ready:1/1 Let's investigate the rollout a little further and check replicasets with the command below: $ kubectl get replicaset Output: NAME DESIRED CURRENT READY AGE rollout-bluegreen-5ffd47b8d4 2 2 2 80s rollout-bluegreen-75695867f 2 2 2 31s Argo rollout created a second replicaset, which is used to manage the different pod versions. Lets promote the new version.</p> <p>Command:</p> <p><pre><code>kubectl argo rollouts promote rollout-bluegreen\n</code></pre> Output:</p> <p>rollout 'rollout-bluegreen' promoted</p> <p>Command:</p> <pre><code>kubectl argo rollouts get ro rollout-bluegreen\n</code></pre> <p>Output: Name: rollout-bluegreen Namespace: default Status: ‚úî Healthy Strategy: BlueGreen Images: argoproj/rollouts-demo:green (stable, active) Replicas: Desired: 2 Current: 2 Updated: 2 Ready: 2 Available: 2 NAME KIND STATUS AGE INFO ‚ü≥ rollout-bluegreen Rollout ‚úî Healthy 2m40s ‚îú‚îÄ‚îÄ# revision:2 ‚îÇ ‚îî‚îÄ‚îÄ‚ßâ rollout-bluegreen-75695867f ReplicaSet ‚úî Healthy 75s stable,active ‚îÇ ‚îú‚îÄ‚îÄ‚ñ° rollout-bluegreen-75695867f-m6pxh Pod ‚úî Running 75s ready:1/1 ‚îÇ ‚îî‚îÄ‚îÄ‚ñ° rollout-bluegreen-75695867f-nr2rh Pod ‚úî Running 75s ready:1/1 ‚îî‚îÄ‚îÄ# revision:1 ‚îî‚îÄ‚îÄ‚ßâ rollout-bluegreen-5ffd47b8d4 ReplicaSet ‚Ä¢ ScaledDown 2m4s ‚îú‚îÄ‚îÄ‚ñ° rollout-bluegreen-5ffd47b8d4-mqc25 Pod ‚óå Terminating 114s ready:1/1 ‚îî‚îÄ‚îÄ‚ñ° rollout-bluegreen-5ffd47b8d4-q4bgf Pod ‚óå Terminating 113s ready:1/1 Our new revision changed from ‚Äúpreview‚Äù to ‚Äústable,active‚Äù - indicating that the new revision is live. You may also see that the first revision will display ‚Äúdelay‚Äù followed by a counter. Eventually, it will go into a ‚ÄúScaledDown‚Äù status. We can even see this by checking our service with the command below: $ kubectl describe svc rollout-bluegreen-active Output:</p> <p>Name: rollout-bluegreen-active Namespace: default Labels: app=rollout-bluegreen-active Annotations: argo-rollouts.argoproj.io/managed-by-rollouts: rollout-bluegreen Selector: app=rollout-bluegreen,rollouts-pod-template-hash=75695867f Type: ClusterIP IP Family Policy: SingleStack IP Families: IPv4 IP: 10.96.227.100 IPs: 10.96.227.100 Port: 80 80/TCP TargetPort: 80/TCP Endpoints: 10.244.0.43:80,10.244.0.44:80 Session Affinity: None Internal Traffic Policy: Cluster Events:  Note that the Selector rollouts-pod-template-hash has the same value as the new ReplicaSet. We just successfully performed a deployment using blue-green methodology. 3. Perform a Rollback Let‚Äôs assume we want to roll back from the new green to the old blue image. <p>Command:</p> <p><pre><code>kubectl argo rollouts undo rollout-bluegreen\n</code></pre> Output:</p> <p>rollout 'rollout-bluegreen' undo</p> <p>Command:</p> <pre><code>kubectl argo rollouts get ro rollout-bluegreen\n</code></pre> <p>Output:</p> <p>Name: rollout-bluegreen Namespace: default Status: ‡•• Paused Message: BlueGreenPause Strategy: BlueGreen</p> <p>Images: argoproj/rollouts-demo:blue (preview) argoproj/rollouts-demo:green (stable, active) Replicas: Desired: 2 Current: 4 Updated: 2 Ready: 2 Available: 2 NAME KIND STATUS AGE INFO ‚ü≥ rollout-bluegreen Rollout ‡•• Paused 3m52s ‚îú‚îÄ‚îÄ# revision:3 ‚îÇ ‚îî‚îÄ‚îÄ‚ßâ rollout-bluegreen-5ffd47b8d4 ReplicaSet ‚úî Healthy 3m16s preview ‚îÇ ‚îú‚îÄ‚îÄ‚ñ° rollout-bluegreen-5ffd47b8d4-2lvcq Pod ‚úî Running 3s ready:1/1 ‚îÇ ‚îî‚îÄ‚îÄ‚ñ° rollout-bluegreen-5ffd47b8d4-k8sqf Pod ‚úî Running 3s ready:1/1 ‚îî‚îÄ‚îÄ# revision:2 ‚îî‚îÄ‚îÄ‚ßâ rollout-bluegreen-75695867f ReplicaSet ‚úî Healthy 2m27s stable,active ‚îú‚îÄ‚îÄ‚ñ° rollout-bluegreen-75695867f-m6pxh Pod ‚úî Running 2m27s ready:1/1 ‚îî‚îÄ‚îÄ‚ñ° rollout-bluegreen-75695867f-nr2rh Pod ‚úî Running 2m27s ready:1/1 Note, that ‚Äúundo‚Äù alone did not set the blue image active. The rollout is now again in the pausing phase, waiting for promotion of the rollout. Run the following command: $ kubectl argo rollouts promote rollout-bluegreen Output: rollout 'rollout-bluegreen' promoted Checking our rollout and active service once again, we see, that the selector changed back to our old ReplicaSet. Command: $ kubectl argo rollouts get ro rollout-bluegreen Output:</p> <p>Name: rollout-bluegreen Namespace: default Status: ‚úî Healthy Strategy: BlueGreen Images: argoproj/rollouts-demo:blue (stable, active) argoproj/rollouts-demo:green Replicas: Desired: 2 Current: 4 Updated: 2 Ready: 2 Available: 2 NAME KIND STATUS AGE INFO ‚ü≥ rollout-bluegreen Rollout ‚úî Healthy 4m17s ‚îú‚îÄ‚îÄ# revision:3 ‚îÇ ‚îî‚îÄ‚îÄ‚ßâ rollout-bluegreen-5ffd47b8d4 ReplicaSet ‚úî Healthy 3m41s stable,active ‚îÇ ‚îú‚îÄ‚îÄ‚ñ° rollout-bluegreen-5ffd47b8d4-2lvcq Pod ‚úî Running 28s ready:1/1 ‚îÇ ‚îî‚îÄ‚îÄ‚ñ° rollout-bluegreen-5ffd47b8d4-k8sqf Pod ‚úî Running 28s ready:1/1 ‚îî‚îÄ‚îÄ# revision:2 ‚îî‚îÄ‚îÄ‚ßâ rollout-bluegreen-75695867f ReplicaSet ‚úî Healthy 2m52s delay:23s ‚îú‚îÄ‚îÄ‚ñ° rollout-bluegreen-75695867f-m6pxh Pod ‚úî Running 2m52s ready:1/1 ‚îî‚îÄ‚îÄ‚ñ° rollout-bluegreen-75695867f-nr2rh Pod ‚úî Running 2m52s ready:1/1</p> <p>Command:</p> <pre><code>kubectl describe svc rollout-bluegreen-active\n</code></pre> <p>Output:</p> <p>Name: rollout-bluegreen-active Namespace: default Labels: app=rollout-bluegreen-active Annotations: argo-rollouts.argoproj.io/managed-by-rollouts: rollout-bluegreen Selector: app=rollout-bluegreen,rollouts-pod-template-hash=5ffd47b8d4</p> <p>Type: ClusterIP IP Family Policy: SingleStack IP Families: IPv4 IP: 10.96.227.100 IPs: 10.96.227.100 Port: 80 80/TCP TargetPort: 80/TCP Endpoints: 10.244.0.45:80,10.244.0.46:80 Session Affinity: None Internal Traffic Policy: Cluster Events:  <ol> <li>Clean Up Resources We successfully used the blue-green deployment pattern to deploy an application and even performed a rollback. To keep our working cluster nice and clean, we are going to clean up resources we created.</li> </ol> <p>Command:</p> <pre><code>kubectl delete rollout rollout-bluegreen\n</code></pre> <p>Output:</p> <pre><code>rollout.argoproj.io \"rollout-bluegreen\" deleted\n</code></pre> <p>Command:</p> <pre><code>kubectl delete svc rollout-bluegreen-active\n</code></pre> <p>Output:</p> <pre><code>rollout-bluegreen-preview\nservice \"rollout-bluegreen-active\" deleted\nservice \"rollout-bluegreen-preview\" deleted\n</code></pre>","path":["ARGO","ARGO ROLLOUTS","Labs","Argo Rollouts Blue-Green"],"tags":[]},{"location":"argo/argo_rollouts/labs/labs03/","level":1,"title":"Migrating an Existing Deployment to Argo Rollouts","text":"<p>Chances are, that you are not starting with a fresh Kubernetes installation but already have a running cluster with deployed workloads. Argo Rollouts has this scenario in mind and provides a migration path to migrate Deployments to Rollout resources.</p>","path":["ARGO","ARGO ROLLOUTS","Labs","Migrating an Existing Deployment to Argo Rollouts"],"tags":[]},{"location":"argo/argo_rollouts/labs/labs03/#objective","level":2,"title":"Objective","text":"<p>Migrate a vanilla Kubernetes Deployment to an Argo Rollout resource.</p>","path":["ARGO","ARGO ROLLOUTS","Labs","Migrating an Existing Deployment to Argo Rollouts"],"tags":[]},{"location":"argo/argo_rollouts/labs/labs03/#prerequisites","level":2,"title":"Prerequisites","text":"<ul> <li>Kubernetes cluster with an argo-rollouts controller.</li> <li>kubectl with an argo-rollouts plugin (optional).</li> </ul>","path":["ARGO","ARGO ROLLOUTS","Labs","Migrating an Existing Deployment to Argo Rollouts"],"tags":[]},{"location":"argo/argo_rollouts/labs/labs03/#transitioning-to-argo-rollouts","level":2,"title":"Transitioning to Argo Rollouts","text":"<ol> <li>Preparing resources</li> </ol> <p>For this lab, we will create an NGINX deployment‚Äîa task you may have already undertaken numerous times. Run the command below:</p> <pre><code>kubectl create deploy nginx-deployment --image=nginx --replicas=3\ndeployment.apps/nginx-deployment created\n</code></pre> <p>Now check our running pods and deployments using the following command:</p> <pre><code>kubectl get pods,deployment\nNAME                                    READY   STATUS    RESTARTS   AGE\npod/nginx-deployment-6ff797d4c9-ftwcc   1/1     Running   0          57s\npod/nginx-deployment-6ff797d4c9-nbxf5   1/1     Running   0          57s\npod/nginx-deployment-6ff797d4c9-pw2cl   1/1     Running   0          57s\n\nNAME                               READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/nginx-deployment   3/3     3            3           58s\n</code></pre> <ol> <li>Convert Deployment to Rollout Now we want to use the deployment definition to reference it in a new rollout.  Run the command below:</li> </ol> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: argoproj.io/v1alpha1\nkind: Rollout\nmetadata:\n  name: nginx-rollout\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx-deployment\n  workloadRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: nginx-deployment\n  strategy:\n    canary:\n      steps:\n        - setWeight: 20\n        - pause: {duration: 10s}\nEOF\n\nrollout.argoproj.io/nginx-rollout created\n</code></pre> <p>Note</p> <p>The field <code>workloadRef</code>, which references the nginx-deployment resource. As a result, we have 6 nginx instances running, 3 managed by our vanilla deployment, 3 by the newly created rollout. </p> <p>Run the command below:</p> <pre><code>kubectl get rollout,deployment,pod\nNAME                                DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nrollout.argoproj.io/nginx-rollout   3         3         3            3           2m16s\n\nNAME                               READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/nginx-deployment   3/3     3            3           26m\n\nNAME                                    READY   STATUS    RESTARTS   AGE\npod/nginx-deployment-6ff797d4c9-ftwcc   1/1     Running   0          26m\npod/nginx-deployment-6ff797d4c9-nbxf5   1/1     Running   0          26m\npod/nginx-deployment-6ff797d4c9-pw2cl   1/1     Running   0          26m\npod/nginx-rollout-6d7df6cfcb-bt78w      1/1     Running   0          2m16s\npod/nginx-rollout-6d7df6cfcb-z6srf      1/1     Running   0          2m16s\npod/nginx-rollout-6d7df6cfcb-zmm7v      1/1     Running   0          2m16s\n</code></pre> <ol> <li>Scale Down Deployment To finish the migration, we now need to manually scale down the deployment.  Run the following command:</li> </ol> <pre><code>kubectl scale deployment/nginx-deployment --replicas=0\n</code></pre> <p>Output:</p> <pre><code>kubectl get rollouts,deployments,pods\nNAME                                DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nrollout.argoproj.io/nginx-rollout   3         3         3            3           4m9s\n\nNAME                               READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/nginx-deployment   0/0     0            0           28m\n\nNAME                                 READY   STATUS    RESTARTS   AGE\npod/nginx-rollout-6d7df6cfcb-bt78w   1/1     Running   0          4m9s\npod/nginx-rollout-6d7df6cfcb-z6srf   1/1     Running   0          4m9s\npod/nginx-rollout-6d7df6cfcb-zmm7v   1/1     Running   0          4m9s\n</code></pre> <p>This leaves you with an up-and-running workload, managed by a rollout resource!</p> <p>The step of scaling down the deployment once referenced by the Rollout resource can be taken over by the Argo Rollout controller. A special scaleDown parameter exists that enables administrators to specify how the deployment should be scaled down (never, onsuccess, progressively).</p> <p>After confirming the deployment is scaled down, scale it up one more time with the following command:</p> <pre><code>kubectl scale deployment/nginx-deployment --replicas=3\n</code></pre> <p>And apply the following new rollout spec which includes the scaleDown parameter: <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: argoproj.io/v1alpha1\nkind: Rollout\nmetadata:\n  name: nginx-rollout\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx-deployment\n  workloadRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: nginx-deployment\n    scaleDown: onsuccess\n  strategy:\n    canary:\n      steps:\n        - setWeight: 20\n        - pause: {duration: 10s}\nEOF\n</code></pre></p> <p>This will provide the same result as before, except this time there was no need for any manual intervention!</p> <p>Command:</p> <pre><code>kubectl get rollouts,deployments,pods\nNAME                                DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nrollout.argoproj.io/nginx-rollout   3         3         3            3           126m\n\nNAME                               READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/nginx-deployment   0/0     0            0           150m\n\nNAME                                 READY   STATUS    RESTARTS   AGE\npod/nginx-rollout-6d7df6cfcb-bt78w   1/1     Running   0          126m\npod/nginx-rollout-6d7df6cfcb-z6srf   1/1     Running   0          126m\npod/nginx-rollout-6d7df6cfcb-zmm7v   1/1     Running   0          126m\n</code></pre> <p>More details on the feature can be found on the Rollout Migration documentation page.</p> <ol> <li>Clean Up Resources Make sure to leave the cluster nice and clean.</li> </ol> <p>Command:</p> <pre><code>$ kubectl delete rollout nginx-rollout\n</code></pre> <p>Output:</p> <pre><code>rollout.argoproj.io \"nginx-rollout\" deleted\n</code></pre> <p>Command:</p> <pre><code>$ kubectl delete deployment nginx-deployment\n</code></pre> <p>Output:</p> <pre><code>deployment.apps \"nginx-deployment\" deleted\n</code></pre>","path":["ARGO","ARGO ROLLOUTS","Labs","Migrating an Existing Deployment to Argo Rollouts"],"tags":[]},{"location":"argo/argo_workflows/architecture/","level":1,"title":"Argo Workflows Architecture","text":"","path":["ARGO","ARGO WORKFLOWS","Argo Workflows Architecture"],"tags":[]},{"location":"argo/argo_workflows/architecture/#defining-argo-workflows-and-its-components","level":2,"title":"Defining Argo Workflows and Its Components","text":"<p>Argo Workflows is an open source workflow orchestration platform designed for Kubernetes. It enables users to define, run, and manage complex workflows using Kubernetes as the underlying execution environment. All of Argo Workflows‚Äô components are independent of other Argo projects and are usually deployed into their own Kubernetes namespace in a cluster.</p> <p>Argo Server The Argo Server is a central component that manages the overall workflow resources, state, and interactions. It exposes a REST API for workflow submission, monitoring, and management. The server maintains the state of workflows and their execution and interacts with the Kubernetes API server to create and manage resources.</p> <p>Workflow Controller The Argo Workflows Controller is a critical component within the Argo Workflows system. It is responsible for managing the lifecycle of workflows, interacting with the Kubernetes API server, and ensuring the execution of workflows according to their specifications. The Argo Workflows Controller continuously watches the Kubernetes API server for changes related to Argo Workflows Custom Resources (CRs). The primary CR involved is the Workflow, which defines the workflow structure and steps. Upon detecting the creation or modification of a Workflow CR, the controller initiates the corresponding workflow execution. The controller is responsible for managing the complete lifecycle of a workflow, including its creation, execution, monitoring, and completion. It also resolves dependencies between steps within a workflow. It ensures that steps are executed in the correct order, based on dependencies specified in the workflow definition.</p> <p>Argo UI The Argo UI is a web-based user interface for visually monitoring and managing workflows. It allows users to view workflow status, logs, and artifacts, as well as submit new workflows.</p> <p>Both the Workflow Controller and Argo Server run in the argo namespace. We can opt for one of the cluster or namespaced installations, however, the generated Workflows and the Pods will be run in the respective namespace.</p> <p>The diagram below shows an overview of a Workflow and also details of a namespace with generated pods.</p> <p>https://d36ai2hkxl16us.cloudfront.net/course-uploads/e0df7fbf-a057-42af-8a1f-590912be5460/utgl42dbrozx-LFS256_CourseTrainingGraphics-3.png</p> <p>Argo Workflow building blocks</p> <p>A user defines a workflow using YAML or JSON files, specifying the sequence of steps, dependencies, inputs, outputs, parameters, and any other relevant configurations. Then the workflow definition file is submitted to the Kubernetes cluster where Argo Workflows is deployed. This submission can be done via the Argo CLI, Argo UI, or programmatically through Kubernetes API clients.</p> <p>The Workflow Controller component of Argo Workflows continuously monitors the Kubernetes cluster for new workflow submissions or updates to existing workflows. When a new workflow is submitted, the Workflow Controller parses the workflow definition to validate its syntax and semantics. If there are any errors or inconsistencies, the Workflow Controller reports them to the user for correction.</p> <p>Once the workflow definition is validated, the Workflow Controller creates the necessary Kubernetes resources to represent the workflow, such as Workflow CRDs (Custom Resource Definitions) and associated Pods, Services, ConfigMaps, and Secrets.</p> <p>Finally, Workflow Controller begins executing the steps defined in the workflow. Each step may involve running containers, executing scripts, or performing other actions specified by the user. Argo Workflows ensures that steps are executed in the correct order based on dependencies defined in the workflow.</p>","path":["ARGO","ARGO WORKFLOWS","Argo Workflows Architecture"],"tags":[]},{"location":"argo/argo_workflows/architecture/#argo-workflow-overview","level":2,"title":"Argo Workflow Overview","text":"<p>Each Step and DAG causes the generation of a Pod which comprises three containers:</p> <ul> <li>init: a template that contains an init container that performs initialization tasks. In this case, it echoes a message and sleeps for 30 seconds, but you can replace these commands with your actual initialization steps.</li> <li>main: a template contains the main container that executes the primary process once the initialization is complete.</li> <li>wait: a container that executes tasks such as clean up, saving off parameters, and artifacts. To learn more about Experiments, please consult the official documentation.</li> </ul>","path":["ARGO","ARGO WORKFLOWS","Argo Workflows Architecture"],"tags":[]},{"location":"argo/argo_workflows/concepts/","level":1,"title":"Argo Workflows Core Concepts","text":"","path":["ARGO","ARGO WORKFLOWS","Argo Workflows Core Concepts"],"tags":[]},{"location":"argo/argo_workflows/concepts/#workflow","level":2,"title":"Workflow","text":"<p>A workflow is a series of tasks, processes, or steps that are executed in a specific sequence to achieve a particular goal or outcome. Workflows are prevalent in various domains, including business, software development, and project management. In the context of Argo and other DevOps tools, a workflow specifically refers to a sequence of automated steps involved in the deployment, testing, and promotion of software applications.</p> <p>In Argo, the term Workflow is a Kubernetes Custom Resource that represents a sequence of tasks or steps that are defined and orchestrated to achieve a specific goal. It is a higher-level abstraction that allows users to describe complex processes, dependencies, and conditions in a structured and declarative manner. A Workflow also maintains the state of a workflow.</p> <p>Next, we will take a look at the specs of a simple Workflow.</p> <p>The main part of a Workflow spec contains an entrypoint and list of templates, as shown in the example below:</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: hello-world-\nspec:\n  entrypoint: whalesay\n  templates:\n- name: whalesay\n  container:\n    image: docker/whalesay\n    command: [cowsay]\n    args: [\"hello world\"]\n</code></pre> <p>A Workflow spec has two core parts:</p> <p>Entrypoint: Specifies the name of the template that serves as the entrypoint for the workflow. It defines the starting point of the workflow execution. Templates: A template represents a step or task in the workflow that should be executed. There are six types of templates that we will introduce next.</p>","path":["ARGO","ARGO WORKFLOWS","Argo Workflows Core Concepts"],"tags":[]},{"location":"argo/argo_workflows/concepts/#template-types","level":2,"title":"Template Types","text":"<p>Un template peut √™tre un container, un script, un DAG (Directed-Acyclic Graph), ou d'autres types selon la structure du workflow. Il se divise en deux groupes : les template definitions (qui d√©finissent les t√¢ches √† accomplir) et les template invocators (qui appellent d'autres templates et g√®rent le contr√¥le d'ex√©cution).</p> <p>Il existe 9 types de templates, r√©partis en deux cat√©gories diff√©rentes.</p>","path":["ARGO","ARGO WORKFLOWS","Argo Workflows Core Concepts"],"tags":[]},{"location":"argo/argo_workflows/concepts/#template-definitions","level":3,"title":"Template Definitions","text":"<p>Container Un container est le type de template le plus courant et repr√©sente une √©tape dans le workflow qui ex√©cute un container. Il est adapt√© pour l'ex√©cution d'applications ou de scripts containeris√©s.</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: whalesay-\nspec:\n  entrypoint: whalesay\n  templates:\n  - name: whalesay\n    container:\n      image: docker/whalesay\n      command: [cowsay]\n      args: [\"hello world\"]\n</code></pre> <p>Resource Une resource repr√©sente un template pour cr√©er, modifier ou supprimer des ressources Kubernetes. Elle est utile pour effectuer des op√©rations sur les objets Kubernetes dans le cluster o√π les workflows sont actuellement ex√©cut√©s.</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: kubernetes-resource-\nspec:\n  entrypoint: kubernetes-resource\n  templates:\n  - name: kubernetes-resource\n    resource:\n      action: create\n      manifest: |\n        apiVersion: v1\n        kind: ConfigMap\n        metadata:\n          generateName: app-production-\n        data:\n          some: value\n</code></pre> <p>Script Un script est similaire au template container, mais permet de sp√©cifier le script directement, sans faire r√©f√©rence √† une image de container externe. Il peut √™tre utilis√© pour des scripts simples ou des commandes en une seule ligne.</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: gen-random-int-\nspec:\n  entrypoint: gen-random-int\n  templates:\n  - name: gen-random-int\n    script:\n      image: python:alpine3.6\n      command: [python]\n      source: |\n        import random\n        i = random.randint(1, 100)\n        print(i)\n</code></pre> <p>TODO</p> <p>TODO: Ajouter un exemple plus parlant</p> <p>Suspend Un suspend est un template qui suspend l'ex√©cution, soit pendant une dur√©e d√©termin√©e, soit jusqu'√† ce qu'il soit repris manuellement. Il peut √™tre repris via CLI, API ou l'UI.</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: delay-\nspec:\n  entrypoint: delay\n  templates:\n  - name: delay\n    suspend:\n      duration: \"30s\"\n</code></pre> <p>TODO</p> <p>Plugin Un plugin permet d'int√©grer des actions avec des services externes, comme Slack. Par exemple, il peut √™tre utilis√© pour envoyer des notifications dans un canal Slack directement depuis ton workflow.</p> PLUGIN<pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: argocd-example-\nspec:\n  entrypoint: main\n  templates:\n  - name: main\n    plugin:\n      argocd:\n        serverUrl: https://argocd.mathod.io/\n        actions:\n          - sync:\n              project: platform\n              apps:\n                - crossplane\n                - kyverno\n</code></pre> <p>Container Set <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: container-set-template-\nspec:\n  entrypoint: main\n  templates:\n    - name: main\n      containerSet:\n        containers:\n          - name: a\n            image: rancher/cowsay\n            command: [cowsay]\n            args: [\"container A\"]\n          - name: b\n            image: rancher/cowsay\n            command: [cowsay]\n            args: [\"container B\"]\n</code></pre></p> <p>HTTP <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: http-template-example-\nspec:\n  entrypoint: fetch-todo-item\n  templates:\n  - name: fetch-todo-item\n    http:\n      url: \"https://mathod.io/todos/1\"\n      method: \"GET\"\n      headers:\n        - name: \"Content-Type\"\n          value: \"application/json\"\n</code></pre></p>","path":["ARGO","ARGO WORKFLOWS","Argo Workflows Core Concepts"],"tags":[]},{"location":"argo/argo_workflows/concepts/#template-invocators","level":3,"title":"Template Invocators","text":"<p>DAG A DAG (Directed-Acyclic Graph) allows defining our tasks as a graph of dependencies. It is beneficial for workflows with complex dependencies and conditional execution. </p> <pre><code>- name: diamond\n  dag:\n    tasks:\n    - name: A\n      template: echo\n    - name: B\n      dependencies: [A]\n      template: echo\n    - name: C\n      dependencies: [A]\n      template: echo\n    - name: D\n      dependencies: [B, C]\n      template: echo\n</code></pre> <p>Note</p> <p>Dans cet exemple, A s'ex√©cute en premier. Une fois termin√©, B et C s'ex√©cuteront en parall√®le, et une fois qu'ils auront tous les deux termin√©, D s'ex√©cutera</p> <p>Steps Steps are defining multiple steps within a template as several steps need to be executed sequentially or in parallel.</p> <pre><code>- name: hello-hello-hello\n  steps:\n  - - name: step1\n      template: prepare-data\n  - - name: step2a\n      template: run-data-first-half\n    - name: step2b\n      template: run-data-second-half\n</code></pre>","path":["ARGO","ARGO WORKFLOWS","Argo Workflows Core Concepts"],"tags":[]},{"location":"argo/argo_workflows/concepts/#outputs","level":2,"title":"Outputs","text":"<p>In Argo Workflows, the outputs section within a step template allows you to define and capture outputs that can be accessed by subsequent steps or referenced in the workflow definition. Outputs are useful when you want to pass data, values, or artifacts from one step to another. Here's an overview of how outputs work in Argo Workflows. The Output comprises two key concepts:</p> <p>Defining Outputs: You define outputs within a step template using the outputs section. Each output has a name and a path within the container where the data or artifact is produced. Accessing Outputs: You can reference the outputs of a step using templating expressions in subsequent steps or the workflow definition. Let‚Äôs consider a simple example where one step generates an output parameter and an output artifact, and another step consumes them:</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: artifact-passing-\nspec:\n  entrypoint: artifact-example\n  templates:\n  - name: artifact-example\n    steps:\n    - - name: generate-artifact\n        template: whalesay\n    - - name: consume-artifact\n        template: print-message\n        arguments:\n          artifacts:\n          - name: message\n            from: \"{{steps.generate-artifact.outputs.artifacts.hello-art}}\"\n\n  - name: whalesay\n    container:\n      image: docker/whalesay:latest\n      command: [sh, -c]\n      args: [\"cowsay hello world | tee /tmp/hello_world.txt\"]\n    outputs:\n      artifacts:\n    - name: hello-art\n      path: /tmp/hello_world.txt\n\n  - name: print-message\n    inputs:\n      artifacts:\n      - name: message\n        path: /tmp/message\n    container:\n      image: alpine:latest\n      command: [sh, -c]\n      args: [\"cat /tmp/message\"]\n</code></pre> <p>First the <code>whalesay</code> template creates a file name <code>/tmp/hello-world.txt</code> by using the cowsay command. Next, it outputs this file as an artifact called hello-art. The <code>artifact-example</code> template provides the generated hello-art artifact as an output of the generate-artifact step. Finally, the <code>print-message</code> template takes an input artifact called message and consumes it by unpacking it in <code>/tmp/message</code> path and using the cat command to print it into standard output.</p>","path":["ARGO","ARGO WORKFLOWS","Argo Workflows Core Concepts"],"tags":[]},{"location":"argo/argo_workflows/concepts/#workflowtemplate","level":2,"title":"WorkflowTemplate","text":"<p>In Argo Workflows, a WorkflowTemplate is a resource that defines a reusable and shareable workflow template, allowing users to encapsulate workflow logic, parameters, and metadata. This abstraction promotes modularity and reusability, enabling the creation of complex workflows from pre-defined templates.</p> <p>Here is an example of a simple WorkflowTemplate definition in Argo Workflows:</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: WorkflowTemplate\nmetadata:\n  name: sample-template\nspec:\n  templates:\n   - name: hello-world\n     inputs:\n       parameters:\n         - name: msg\n           value: \"Hello World!\"\n     container:\n       image: docker/whalesay\n       command: [cowsay]\n       args: [\"{{inputs.parameters.msg}}\"]\n</code></pre> <p>In this example:</p> <ul> <li>The WorkflowTemplate is named <code>sample-template</code></li> <li>It contains a template: <code>hello-world</code></li> <li>The <code>hello-world</code> template takes a parameter message (with a default value of \"Hello, World!\") and generates a file with the specified message. Once defined, this WorkflowTemplate can be referenced and instantiated within multiple workflows. For example:</li> </ul> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: hello-world-\nspec:\nentrypoint: whalesay\ntemplates:\n  - name: whalesay\n    steps:\n      - - name: hello-world\n          templateRef:\n            name: sample-template\n            template: hello-world\n</code></pre> <p>This workflow references the WorkflowTemplate named sample-template effectively inheriting the structure and logic defined in the template.</p> <p>Using WorkflowTemplates is beneficial when you want to standardize and reuse specific workflow patterns, making it easier to manage, maintain, and share workflow definitions within your organization. They also help in enforcing consistency and reducing redundancy across multiple workflows.</p>","path":["ARGO","ARGO WORKFLOWS","Argo Workflows Core Concepts"],"tags":[]},{"location":"argo/argo_workflows/draft/","level":1,"title":"Draft","text":"","path":["ARGO","DRAFT","Draft"],"tags":[]},{"location":"argo/argo_workflows/draft/#template-invocators","level":4,"title":"Template Invocators","text":"<p>Ces templates sont utilis√©s pour invoquer/appeler d'autres templates et fournir un contr√¥le d'ex√©cution.</p> <p>TODO</p> <p>TODO: Principe de synchronization et when</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: simple-output-example-\nspec:\n  entrypoint: main\n  templates:\n  - name: main\n    steps:\n    - - name: generate\n        template: producer\n    - - name: consume\n        template: consumer\n        arguments:\n          parameters:\n          - name: outputMessage\n            from: \"{{steps.generate.outputs.result}}\"\n\n  - name: producer\n    script:\n      image: busybox:1.28\n      command: [sh, -c]\n      args: echo \"hello from the producer step\"\n  - name: consumer\n    inputs:\n      parameters:\n      - name: message\n    container:\n      image: busybox:1.28\n      command: [sh, -c]\n      args: [\" Received: '{{inputs.parameters.outputMessage}}'\"]\n</code></pre> <p>Expliquer argument parameters et artefact</p> <p>In Argo Workflows, the terms parameters, arguments, and artifacts are related but play different roles. Here‚Äôs a clear, practical breakdown with examples.</p> <ol> <li>Parameters What they are</li> </ol> <p>Small, simple values (strings, numbers, booleans, JSON-as-string)</p> <p>Used to control logic or configuration</p> <p>Passed by value</p> <p>Where they live</p> <p>Defined in a template</p> <p>Referenced inside that template</p> <p>Example templates: - name: greet   inputs:     parameters:     - name: name   container:     image: alpine     command: [sh, -c]     args: [\"echo Hello {{inputs.parameters.name}}\"]</p> <p>Think of parameters as</p> <p>Command-line flags or function parameters</p> <ol> <li>Arguments What they are</li> </ol> <p>Concrete values you pass to a template‚Äôs inputs</p> <p>Can include parameters and artifacts</p> <p>They are the instance of an input, not the definition</p> <p>Where they live</p> <p>At the workflow level or when calling another template</p> <p>Example workflow:   entrypoint: greet   arguments:     parameters:     - name: name       value: Alice</p> <p>or when calling a template from another template: - name: call-greet   template: greet   arguments:     parameters:     - name: name       value: Bob</p> <p>Think of arguments as</p> <p>The actual values you pass when calling a function</p> <ol> <li>Artifacts What they are</li> </ol> <p>Files or directories, often large</p> <p>Used for data exchange between steps</p> <p>Stored in artifact repositories (S3, GCS, MinIO, etc.)</p> <p>Passed by reference, not by value</p> <p>Where they live</p> <p>Defined as inputs or outputs of templates</p> <p>Example templates: - name: generate-file   container:     image: alpine     command: [sh, -c]     args: [\"echo hello &gt; /tmp/hello.txt\"]   outputs:     artifacts:     - name: greeting-file       path: /tmp/hello.txt</p> <p>Passing the artifact to another step: - name: consume-file   template: read-file   arguments:     artifacts:     - name: greeting-file       from: \"{{steps.generate-file.outputs.artifacts.greeting-file}}\"</p> <p>Think of artifacts as</p> <p>Files passed via shared storage</p> <p>Side-by-Side Comparison ConceptPurposeSizePassed HowTypical UseParameterControl behavior/configSmallBy valueFlags, names, countsArgumentAssign input valuesN/AN/ACalling templatesArtifactShare files/data between stepsLargeBy referenceModels, datasets, reports</p> <p>Mental Model (Most Helpful) If Argo were a programming language:</p> <p>Template = function definition</p> <p>Parameters = function parameters</p> <p>Arguments = function call arguments</p> <p>Artifacts = files passed via shared storage</p> <p>If you want, I can:</p> <p>Show a full end-to-end workflow example</p> <p>Explain when to use parameter vs artifact</p> <p>Compare artifacts vs volumes vs outputs</p> <p>‚úÖ Parameters vs. Arguments</p> <p>The docs explain that:</p> <p>A Workflow provides arguments, which are the values you supply when you run or call a template.</p> <p>A template defines inputs, and those inputs include parameters. When you call that template, you pass arguments to match those input parameters.  Argo Workflows</p> <p>Key points from docs:</p> <p>arguments are what the workflow or another caller supplies to a template.</p> <p>parameters are defined by the template as named values it expects.  Argo Workflows</p> <p>This aligns with the analogy:</p> <p>Parameters = function parameter definitions Arguments = actual values supplied at call time</p> <p>‚úÖ Artifacts</p> <p>The documentation also clearly distinguishes artifacts from parameters:</p> <p>Artifacts are files or directories passed between steps, typically stored in or retrieved from an artifact repository.</p> <p>In the workflow spec, artifacts are part of the inputs or outputs of templates, similarly to parameters, but they represent large data objects, not simple values.  Argo Workflows</p> <p>For example, in an arguments: block you see both:</p> <p>arguments:   parameters:     - name: ...       value: ...   artifacts:     - name: ...       from: ...</p> <p>Which matches how Argo separates values (parameters) from file artifacts.  Argo Workflows</p> <p>üìå Summary with Doc Support Term    Meaning in Argo Docs    Example in Spec Parameters  Named values templates expect   inputs: parameters:  Argo Workflows</p> <p>Arguments   Values passed to templates  arguments: parameters:  Argo Workflows</p> <p>Artifacts   Files/large data passed between steps   arguments: artifacts:  Argo Workflows +1</p> <p>If you want, I can point you to the exact sections of the official docs that cover each concept (‚ÄúWorkflow Inputs‚Äù, ‚ÄúArtifacts‚Äù, and ‚ÄúOutput Parameters‚Äù) so you can see the definitions side-by-side.</p>","path":["ARGO","DRAFT","Draft"],"tags":[]},{"location":"argo/argo_workflows/draft/#steps","level":5,"title":"Steps","text":"<p>Les steps permettent de d√©finir plusieurs √©tapes dans un template, avec des t√¢ches ex√©cut√©es s√©quentiellement ou en parall√®le. </p> <p>Un steps template suit une structure de list of lists:</p> <ul> <li>Les listes ext√©rieures s‚Äôex√©cutent s√©quentiellement  </li> <li>Les listes int√©rieures en parall√®le.</li> </ul> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: steps-\nspec:\n  entrypoint: hello-hello-hello\n\n  # This spec contains two templates: hello-hello-hello and print-message\n  templates:\n  - name: hello-hello-hello\n    # Instead of just running a container\n    # This template has a sequence of steps\n    steps:\n    - - name: hello1            # hello1 is run before the following steps\n        template: print-message\n        arguments:\n          parameters:\n          - name: message\n            value: \"hello1\"\n    - - name: hello2a           # double dash =&gt; run after previous step\n        template: print-message\n        arguments:\n          parameters:\n          - name: message\n            value: \"hello2a\"\n      - name: hello2b           # single dash =&gt; run in parallel with previous step\n        template: print-message\n        arguments:\n          parameters:\n          - name: message\n            value: \"hello2b\"\n\n  # This is the same template as from the previous example\n  - name: print-message\n    inputs:\n      parameters:\n      - name: message\n    container:\n      image: busybox\n      command: [echo]\n      args: [\"{{inputs.parameters.message}}\"]\n</code></pre> <p>Note</p> <pre><code>argo -n argo get hello-hello-hello\nSTEP            TEMPLATE           PODNAME                 DURATION  MESSAGE\n‚úî steps-z2zdn  hello-hello-hello\n‚îú‚îÄ‚îÄ‚îÄ‚úî hello1   print-message      steps-z2zdn-27420706    2s\n‚îî‚îÄ‚î¨‚îÄ‚úî hello2a  print-message      steps-z2zdn-2006760091  3s\n  ‚îî‚îÄ‚úî hello2b  print-message      steps-z2zdn-2023537710  3s\n</code></pre> <p>Note</p> <p>Dans cet exemple, hello1 s'ex√©cute en premier. Une fois termin√©, hello2a et hello2b s'ex√©cuteront en parall√®le</p>","path":["ARGO","DRAFT","Draft"],"tags":[]},{"location":"argo/argo_workflows/draft/#walk-through","level":2,"title":"Walk Through","text":"<p>Quote</p>","path":["ARGO","DRAFT","Draft"],"tags":[]},{"location":"argo/argo_workflows/draft/#parameters","level":3,"title":"Parameters","text":"<pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: hello-world-parameters-\nspec:\n  # invoke the print-message template with \"hello world\" as the argument to the message parameter\n  entrypoint: print-message\n  arguments:\n    parameters:\n    - name: message\n      value: hello world\n\n  templates:\n  - name: print-message\n    inputs:\n      parameters:\n      - name: message       # parameter declaration\n    container:\n      # run echo with that message input parameter as args\n      image: busybox\n      command: [echo]\n      args: [\"{{inputs.parameters.message}}\"]\n</code></pre> <pre><code>argo submit arguments-parameters.yaml -p message=\"goodbye world\"\n</code></pre>","path":["ARGO","DRAFT","Draft"],"tags":[]},{"location":"argo/argo_workflows/draft/#parameters-file","level":4,"title":"Parameters file","text":"<p>In case of multiple parameters that can be overridden, the argo CLI provides a command to load parameters files in YAML or JSON format. Here is an example of that kind of parameter file:</p> <p>params.yaml<pre><code>message: goodbye world\n</code></pre> To run use following command:</p> <pre><code>argo submit arguments-parameters.yaml --parameter-file params.yaml\n</code></pre>","path":["ARGO","DRAFT","Draft"],"tags":[]},{"location":"argo/argo_workflows/draft/#entrypoint","level":4,"title":"Entrypoint ???","text":"<p>FROM THE UI you can override the parameters</p>","path":["ARGO","DRAFT","Draft"],"tags":[]},{"location":"argo/argo_workflows/draft/#workflowtemplate","level":2,"title":"WorkflowTemplate","text":"<p>A reusable recipe for workflows, defined once and used across multiple workflows</p> <p>Benefits:   - central, version-controlled library   - promote consistency across workflows   - cleaner and easier to manage   - reduce duplication and maintenance effort</p> workflow-template-library.yaml<pre><code>apiVersion: argoproj.io/v1alpha1\nkind: WorkflowTemplate\nmetadata:\n  name: cowsay-template\n  namespace: argo\nspec:\n  entrypoint: cowsay\n  templates:\n  - name: cowsay\n    inputs:\n      parameters:\n        - name: message\n    container:\n      image: rancher/cowsay\n      command: [cowsay]\n      args: [\"{{inputs.parameters.message}}\"]\n</code></pre> <pre><code>argo -n argo template create workflow-template-library.yaml\n</code></pre> templateRef.yaml<pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: use-single-template-\nspec:\n  entrypoint: my-custom-workflow\n  templates:\n  - name: my-custom-workflow\n    steps:\n      - - name: first-step\n          templateRef:\n            name: cowsay-template\n            template: cowsay\n          arguments:\n            parameters:\n              - name: message\n                values: \"I called this from another workflow!\"\n</code></pre> templateRef.yaml<pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: run-whole-template-\n  namespace: argo\nspec:\n  arguments:\n    parameters:\n      - name: message\n        value: \"Hello from WorkflowTemplate!\"\n  workflowTemplateRef:\n    name: cowsay-template\n</code></pre>","path":["ARGO","DRAFT","Draft"],"tags":[]},{"location":"argo/argo_workflows/draft/#cluster-workflow-template","level":3,"title":"Cluster Workflow Template","text":"<p>cluster-wide version of a WorkflowTemplate cluster-scoped: Usable across all namespaces Managed by DevOps Provides a central library of reusable templates and promotes consistency</p> <p>Donc pour de la cr√©ation de resources clusterworkflowtemplate et pour des commande de check de pods etc workflowTemplate tous court</p> <p>https://learn.kodekloud.com/user/courses/certified-argo-project-associate-capa/module/44223b5d-ccc3-4dcb-8292-66036e2ea023/lesson/7fd60c67-290c-4eb8-a6d6-47ad6a9ea72c?autoplay=true <pre><code>clusterScope: true\n</code></pre></p> <pre><code>spec:\n  entrypoint: main\n  arguments:\n    parameters:\n      - name: environment\n        value: \"production\"\n  templates:\n    - name: main\n      steps:\n        - - name: build\n        - - template: build-step\n        - - name: test\n        - - template: test-step\n        - - when: \"{{workflow.parameters.environment}} != production\"\n        - - name: deploy\n        - - template: deploy-step\n\n    - name: build-step\n        container:\n          image: alpine\n          command: [sh, -c]\n          args: [\"echo 'Building application...'\"]\n    - name: test-step\n      container:\n        image: alpine\n        command: [sh, -c]\n        args: [\"echo 'Running tests...'\"]\n    - name: deploy-step\n      container:\n        image: alpine\n        command: [sh, -c]\n        args: [\"echo 'Deploying to {{workflow.parameters.environment}}'\"]\n</code></pre>","path":["ARGO","DRAFT","Draft"],"tags":[]},{"location":"argo/argo_workflows/draft/#when","level":4,"title":"when","text":"","path":["ARGO","DRAFT","Draft"],"tags":[]},{"location":"argo/argo_workflows/draft/#daemon","level":4,"title":"daemon","text":"","path":["ARGO","DRAFT","Draft"],"tags":[]},{"location":"argo/argo_workflows/draft/#activedeadlinesecondes","level":4,"title":"activeDeadlineSecondes","text":"<p>Timeouts</p> <p>Retrying Failed or Errored Steps</p>","path":["ARGO","DRAFT","Draft"],"tags":[]},{"location":"argo/argo_workflows/draft/#onexit","level":4,"title":"onExit","text":"<p>You can use the field activeDeadlineSeconds to limit the elapsed time for a workflow:</p> <p><pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: timeouts-\nspec:\n  activeDeadlineSeconds: 10 # terminate workflow after 10 seconds\n  entrypoint: sleep\n  templates:\n  - name: sleep\n    container:\n      image: alpine:3.23\n      command: [sh, -c]\n      args: [\"echo sleeping for 1m; sleep 60; echo done\"]\n</code></pre> You can limit the elapsed time for a specific template as well:</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: timeouts-\nspec:\n  entrypoint: sleep\n  templates:\n  - name: sleep\n    activeDeadlineSeconds: 10 # terminate container template after 10 seconds\n    container:\n      image: alpine:3.23\n      command: [sh, -c]\n      args: [\"echo sleeping for 1m; sleep 60; echo done\"]\n</code></pre>","path":["ARGO","DRAFT","Draft"],"tags":[]},{"location":"argo/argo_workflows/draft/#retrystrategy","level":4,"title":"retryStrategy","text":"<p>You can specify a retryStrategy that will dictate how failed or errored steps are retried:</p> <pre><code># This example demonstrates the use of retry back offs\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: retry-backoff-\nspec:\n  entrypoint: retry-backoff\n  templates:\n  - name: retry-backoff\n    retryStrategy:\n      limit: 10\n      retryPolicy: \"Always\"\n      backoff:\n        duration: \"1\"      # Must be a string. Default unit is seconds. Could also be a Duration, e.g.: \"2m\", \"6h\", \"1d\"\n        factor: 2\n        maxDuration: \"1m\"  # Must be a string. Default unit is seconds. Could also be a Duration, e.g.: \"2m\", \"6h\", \"1d\"\n      affinity:\n        nodeAntiAffinity: {}\n    container:\n      image: python:alpine3.23\n      command: [\"python\", -c]\n      # fail with a 66% probability\n      args: [\"import random; import sys; exit_code = random.choice([0, 1, 1]); sys.exit(exit_code)\"]\n</code></pre> <p>limit is the maximum number of times the container will be retried. retryPolicy specifies if a container will be retried on failure, error, both, or only transient errors (e.g. i/o or TLS handshake timeout). \"Always\" retries on both errors and failures. Also available: OnFailure (default), \"OnError\", and \"OnTransientError\" (available after v3.0.0-rc2). backoff is an exponential back-off nodeAntiAffinity prevents running steps on the same host. Current implementation allows only empty nodeAntiAffinity (i.e. nodeAntiAffinity: {}) and by default it uses label kubernetes.io/hostname as the selector. Providing an empty retryStrategy (i.e. retryStrategy: {}) will cause a container to retry until completion.</p>","path":["ARGO","DRAFT","Draft"],"tags":[]},{"location":"argo/argo_workflows/draft/#successcondition","level":4,"title":"successCondition","text":"","path":["ARGO","DRAFT","Draft"],"tags":[]},{"location":"argo/argo_workflows/introduction/","level":1,"title":"Introduction","text":"","path":["ARGO","ARGO WORKFLOWS","Introduction"],"tags":[]},{"location":"argo/argo_workflows/introduction/#chapter-overview-and-objectives","level":2,"title":"Chapter Overview and Objectives","text":"<p>In this chapter, we will explore the details of Argo Workflows, an extension of Argo, a popular GitOps tool designed for declarative continuous delivery of Kubernetes applications. Argo Workflows allows you to define and manage complex workflows as code, providing a way to orchestrate and automate multi-step processes within the Kubernetes environment.</p> <p>By the end of this chapter, you should be able to comprehend the basics and architecture of Argo Workflows. This involves understanding its key components, how they interact, and the fundamental concepts that govern the execution of workflows. Here are learning objectives for gaining proficiency in Argo Workflows:</p> <ul> <li>Define and explain the structure of an Argo Workflow.</li> <li>Recognize key elements such as metadata, spec, entrypoint, and templates.</li> <li>Understand the role of templates in workflows.</li> <li>Identify and explain the primary components of Argo Workflows, including the Workflow Controller, and UI.</li> <li>Understand how Argo Workflows schedules and executes tasks.</li> <li>Dive into the responsibilities of the Workflow Controller.</li> </ul>","path":["ARGO","ARGO WORKFLOWS","Introduction"],"tags":[]},{"location":"argo/argo_workflows/useCase/","level":1,"title":"Use Cases for Argo Workflow","text":"","path":["ARGO","ARGO WORKFLOWS","Use Cases for Argo Workflow"],"tags":[]},{"location":"argo/argo_workflows/useCase/#examples","level":2,"title":"Examples","text":"<p>Argo Workflows is a versatile tool with a wide range of use cases in the context of Kubernetes and containerized environments. Here are some common use cases where Argo Workflows can be beneficial:</p> <ul> <li>To orchestrate end-to-end data processing pipelines, including data extraction, transformation, and loading (ETL) tasks.</li> <li>In machine learning projects, Argo Workflows can orchestrate tasks such as data preprocessing, model training, evaluation, and deployment.</li> <li>Argo Workflows can serve as the foundation for continuous integration and continuous deployment (CI/CD) pipelines. It enables the automation of building, testing, and deploying applications in a Kubernetes environment.</li> <li>For batch processing and periodic tasks, Argo Workflows can be configured to run at specified intervals or based on cron schedules. This is useful for automating routine tasks, report generation, and other scheduled jobs.</li> </ul>","path":["ARGO","ARGO WORKFLOWS","Use Cases for Argo Workflow"],"tags":[]},{"location":"argo/argo_workflows/labs/labs01/","level":1,"title":"Installing Argo Workflows","text":"","path":["ARGO","ARGO WORKFLOWS","Labs","Installing Argo Workflows"],"tags":[]},{"location":"argo/argo_workflows/labs/labs02/","level":1,"title":"A Simple DAG Workflow","text":"","path":["ARGO","ARGO WORKFLOWS","Labs","A Simple DAG Workflow"],"tags":[]},{"location":"argo/argo_workflows/labs/labs03/","level":1,"title":"CI/CD Using Argo Workflows","text":"","path":["ARGO","ARGO WORKFLOWS","Labs","CI/CD Using Argo Workflows"],"tags":[]},{"location":"argo/gitops/introduction/","level":1,"title":"GitOps Introduction","text":"","path":["ARGO","GitOps Introduction"],"tags":[]},{"location":"argo/gitops/introduction/#what-is-gitops","level":2,"title":"What is GitOps","text":"<p>GitOps is an operational framework that leverages Git as the single source of truth for managing both infrastructure and application code. It extends the principles of Infrastructure as Code, enabling automated deployments and rollbacks by controlling the entire code delivery pipeline through Git version control.</p>","path":["ARGO","GitOps Introduction"],"tags":[]},{"location":"argo/gitops/introduction/#gitops-workflow","level":3,"title":"GitOps Workflow","text":"<p>Developers begin by committing their changes to a centralized Git repository. Typically, they work in feature branches created as copies of the main codebase. These branches allow teams to develop new features in isolation until they are deemed ready. A Continuous Integration (CI) service automatically builds the application and runs unit tests on the new code. Once tests pass, the changes undergo a review and approval process by relevant team members before being merged into the central repository.</p> <p>The final step in the pipeline is Continuous Deployment (CD), where changes from the repository are automatically released to Kubernetes clusters.</p> <p>The image illustrates the GitOps workflow, showing the integration of infrastructure, configuration, and application code into a Git repository, followed by continuous integration (CI) and continuous deployment (CD) processes to a Kubernetes cluster. It also depicts a branching and merging process in Git.</p> <p>At the heart of GitOps is the concept of a declaratively defined state. This involves maintaining your infrastructure, application configurations, and related components within one or more Git repositories. An automated process continuously verifies that the state stored in Git matches the actual state in the production environment. This synchronization is managed by a GitOps operator running within a Kubernetes cluster. The operator monitors the repository for updates and applies the desired changes to the cluster‚Äîor even to other clusters as needed.</p> <p>When a developer merges new code into the application repository, a series of automated steps is triggered: unit tests are run, the application is built, a Docker image is created and pushed to a container registry, and finally, the Kubernetes manifests in another Git repository are updated.</p> <p>The image illustrates a GitOps workflow, showing the process from application code merging and continuous integration to deploying Kubernetes manifests, with GitOps operators ensuring the desired state matches the actual state in production environments.</p> <p>The GitOps operator continuously compares the desired state (as defined in Git) with the actual state in the Kubernetes cluster. If discrepancies are found, the operator pulls the necessary changes to ensure that the production environment remains aligned with the desired configuration.</p> <p>The image illustrates a GitOps workflow, showing the process from application code repository through continuous integration to Kubernetes deployment, highlighting the synchronization between desired and actual states.</p> <p>Ease of Rollbacks</p> <p>One of the key benefits of GitOps is the seamless rollback process. Since the entire configuration is maintained in Git, reverting to a previous state is as simple as executing a git revert command. The GitOps operator detects this change and automatically rolls back the production environment to match the desired state.</p> <p>The image illustrates a GitOps workflow, showing the process from application code repository through continuous integration to Kubernetes deployment, highlighting the synchronization between desired and actual states.</p>","path":["ARGO","GitOps Introduction"],"tags":[]},{"location":"argo/gitops/introduction/#gitops-principles","level":2,"title":"GitOps Principles","text":"<p>GitOps Principles In this lesson, we will explore the core principles of GitOps‚Äîan approach to continuous deployment that leverages Git as the single source of truth for infrastructure and application state. The GitOps methodology is built upon four foundational principles.</p> <p>Remember</p> <p>GitOps ensures system consistency and reduces human error by enforcing a declarative model of infrastructure management.</p> <p>Declarative vs. Imperative Approach: The first principle stresses a declarative methodology over an imperative one. In the declarative model, the entire system‚Äîincluding both infrastructure and application manifests‚Äîis described in a desired state. This contrasts with the imperative approach, where specific commands are executed sequentially to change the system state. Relying on the imperative style can complicate reconciliation since it does not maintain a comprehensive record of the system's intended state.</p> <p>Storing the Desired State in Git: The second principle mandates that all declarative files, which represent the desired state of the system, be stored in a Git repository. Git not only offers powerful version control capabilities but also preserves immutability. Storing the desired state in Git makes it the definitive source of truth for system configuration. Any changes pushed to Git are automatically recognized and applied across the system.</p> <p>Automated Application of the Desired State via GitOps Operators: The third principle involves using GitOps operators‚Äîsoftware agents that continuously monitor Git for updates. Once they detect changes, these operators automatically retrieve the desired state from the repository and apply it across one or more clusters or environments. Consider the following deployment manifest example that a GitOps operator might manage:</p> deployment.yaml<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n</code></pre> <p>This operator can run in a single cluster and propagate configuration changes to other clusters as necessary, ensuring uniformity and scalability.</p> <p>Reconciliation and Self-Healing: The final principle centers on continuous reconciliation. GitOps operators maintain a self-healing system by constantly checking for discrepancies between the actual state of the system and the desired state stored in Git. They execute this process through three key steps:</p> <ul> <li>Observe: Monitor the Git repository for updates.</li> <li>Diff: Compare the desired state from Git with the current state of the cluster.</li> <li>Act: Reconcile any differences by updating the system to reflect the declared desired state.</li> </ul> <p>This ongoing reconciliation loop minimizes the risk of configuration drift and helps maintain a robust, error-resistant system.</p> <p>By understanding and applying these GitOps principles, you can ensure your infrastructure remains consistent, scalable, and resilient to changes.</p> <p>Thank you.</p>","path":["ARGO","GitOps Introduction"],"tags":[]},{"location":"argo/gitops/introduction/#devops-vs-gitops","level":2,"title":"DevOps vs GitOps","text":"<p>This lesson dives into the contrasting approaches of DevOps and GitOps‚Äîtwo methodologies that share common goals but differ significantly in execution and toolsets.</p> <p>GitOps leverages containerization technologies such as OpenShift and Kubernetes. It uses Git as the single source of truth for both infrastructure and deployments. In comparison, DevOps is a broader methodology that can be applied to diverse application environments and workflows.</p> <p>DevOps Pipeline A typical DevOps pipeline operates as follows:</p> <p>A developer writes code in an Integrated Development Environment (IDE) and commits it to a source code management system. A Continuous Integration (CI) process detects the commit, runs tests, and builds the necessary artifacts. The pipeline then creates a container image and publishes it to a container repository. Finally, the Continuous Deployment (CD) process connects to a Kubernetes cluster and uses a command-line tool such as kubectl (with imperative commands) to push updates directly to the cluster. Key Point</p> <p>In DevOps, the deployment is initiated by pushing changes directly into the cluster.</p> <p>GitOps Pipeline While the CI processes in a GitOps pipeline mirror those of DevOps up to the point of publishing the container image, the deployment process is distinct:</p> <p>Two separate Git repositories are maintained: one dedicated to application code and another for Kubernetes manifests. Once the image is published, the manifest repository is cloned and updated‚Äîtypically the new image name is specified. These changes are then committed and pushed. The pipeline automatically raises a pull request for the manifest repository. A team member reviews the pull request, suggests adjustments if necessary, and merges the changes upon approval. A GitOps operator, running within the Kubernetes cluster, continuously monitors the repository. When changes are detected, it synchronizes the cluster state to match the repository configuration. Takeaway</p> <p>In a GitOps pipeline, the deployment operator pulls changes from the repository and applies them to the cluster, contrasting with the push-based approach in traditional DevOps workflows.</p> <p>The image below illustrates a side-by-side comparison of the CI/CD pipelines in both DevOps and GitOps. It highlights the key steps‚Äîranging from code development to deployment‚Äîand emphasizes the differences in update management and application.</p> <p>The image compares DevOps and GitOps CI/CD pipelines, illustrating the processes of continuous integration and deployment for each approach. It highlights the steps from code development to deployment, showing differences in how updates are managed and applied.</p> <p>Conclusion This article compared DevOps and GitOps pipelines by detailing the key stages of each process. Understanding these differences is essential for choosing the right methodology for your projects. In the next lesson, we will explore the advantages and challenges associated with each approach.</p>","path":["ARGO","GitOps Introduction"],"tags":[]},{"location":"argo/gitops/introduction/#push-vs-pull","level":2,"title":"Push vs Pull","text":"<p>In this article, we explore the differences between push-based and pull-based deployment strategies for Kubernetes clusters. We will examine their benefits, challenges, and use cases, helping you determine the best approach for your environment.</p> <p>Push-Based Deployment Push-based deployment is commonly used in CI/CD pipelines. With this approach, the application code goes through various stages within the CI pipeline before updates are pushed directly to the Kubernetes cluster.</p> <p>Key Characteristics The CI system requires read-write access to the Kubernetes cluster, which means Kubernetes credentials are stored in the CI system outside the cluster. This arrangement may introduce potential security risks. Typically, the CI system has read-only access to the Git repository and read-write access to the container registry, while the Kubernetes cluster itself only has read-only access to the registry. Deployments can leverage a variety of plugins and tools. For instance, Jenkins can use multiple plugins or approaches, and Helm plugins further simplify the deployment of Helm charts. Security Consideration</p> <p>Storing Kubernetes credentials in the CI system exposes a potential security risk, as these credentials grant read-write access to the cluster.</p> <p>Challenges The deployment configuration is tightly coupled with the CI system. Migrating from one CI platform to another (for example, switching from Jenkins to a different platform) often requires reworking many deployment configurations. Embedding cluster credentials in the CI system increases the risk of unauthorized access if the CI system is compromised. Pull-Based Deployment Pull-based deployment, frequently associated with GitOps, employs an operator running within the Kubernetes cluster. This operator monitors for changes‚Äîeither in a container registry for new images or in a Git repository for updated manifests‚Äîand then autonomously deploys those changes.</p> <p>Key Characteristics The CI/CD system only needs read-write access to the container registry, without requiring direct access to the Kubernetes cluster. Deployments are executed internally from within the cluster, enhancing security by minimizing external access. GitOps operators are particularly supportive of multi-tenant environments, allowing teams to manage multiple repositories and namespaces. For example, different teams can maintain separate Git repositories and corresponding namespaces for their deployments. Secrets can be securely managed by encrypting them using tools like HashiCorp Vault or Bitnami Sealed Secrets. These encrypted secrets are stored in Git or decrypted during the deployment process. GitOps operators can monitor container registries for newer image versions and automatically trigger deployments of the latest images. Secret Management</p> <p>While GitOps encourages declarative management‚Äîincluding secrets‚Äîin Git, the process often requires additional tools and steps (e.g., encryption and decryption) to ensure security, especially with Helm chart deployments.</p> <p>Challenges Managing secrets and configurations can be more complex compared to the push-based model. Although GitOps principles promote a declarative approach, handling encrypted credentials adds an extra layer of complexity. Visual Comparison The image compares push-based and pull-based deployment methods for Kubernetes, highlighting their processes, access permissions, and advantages or disadvantages.</p> <p>The image compares push-based and pull-based deployment methods for Kubernetes, highlighting their processes, advantages, and disadvantages. It includes diagrams and lists of pros and cons for each approach.</p> <p>Summary Deployment Strategy Pros    Cons Push-Based  - Direct integration with CI/CD pipelines- Flexible deployment configurations using various tools and plugins   - Requires CI system to have cluster credentials- Tightly coupled to specific CI systems, making migrations challenging Pull-Based (GitOps) - Enhanced security by limiting external access- Supports multi-tenant environments and automated image updates - More complex secret management- Additional tools required for encrypting and decrypting configurations In summary, push-based deployment strategies simplify certain aspects of automation but may lead to inflexibility and potential security issues. In contrast, pull-based (GitOps) deployments enhance internal management and security at the cost of added complexity in handling secrets and configuration management.</p> <p>Explore more about these methodologies in the Kubernetes Documentation and learn how GitOps can revolutionize your deployment pipeline.</p>","path":["ARGO","GitOps Introduction"],"tags":[]},{"location":"argo/gitops/introduction/#gitops-feature-set","level":2,"title":"GitOps Feature Set","text":"<p>GitOps Feature Set This article provides an in-depth overview of GitOps features and their associated use cases, demonstrating how storing every configuration declaratively in a Git repository can transform your deployment workflows.</p> <p>Every configuration is stored declaratively in Git, which serves as the single source of truth containing the full desired state of the system. This approach not only simplifies application rollbacks‚Äîenabling a quick recovery with a simple git revert‚Äîbut also ensures that audit trails are automatically available through pull requests and commit histories.</p> <p>Key Benefit</p> <p>Storing configurations in Git allows teams to effortlessly rollback to a previous state and maintain a complete audit trail for all changes.</p> <p>Automated CI/CD and Continuous Deployment CI/CD automation is a cornerstone of GitOps. By leveraging automation:</p> <p>Building, testing, and deployment tasks are triggered automatically based on the desired state stored in Git. Continuous deployment becomes seamless and consistent, as applications are deployed automatically to clusters without manual intervention. Extending GitOps to Infrastructure and Cluster Resources Once GitOps is established for application deployment, extend these practices to manage both cluster resources and Infrastructure as Code. For instance, in Kubernetes environments, you can manage various resources including:</p> <p>Secrets management Networking agents and service mesh configurations Database provisioning Prometheus monitoring The core principle here is automatic reconciliation: the system continuously compares the desired state in Git with the actual state in the cluster. If any unintended changes occur, the system automatically reverts them, ensuring consistency.</p> <p>Automatic Reconciliation</p> <p>GitOps continuously compares Git‚Äôs desired state against the actual runtime state and reverts any drift, maintaining alignment across your infrastructure.</p> <p>Detecting and Preventing Configuration Drift Early detection of configuration drift is a fundamental aspect of GitOps. Identifying drift as soon as it happens allows teams to resolve inconsistencies before they evolve into significant issues. This proactive stance distinguishes GitOps from other deployment methodologies.</p> <p>Multi-Cluster Deployment Made Easy Managing multiple clusters, especially across different geographical locations, can be challenging. GitOps simplifies this process by centralizing cluster state within Git. This means:</p> <p>A single operator can deploy applications across multiple clusters. There is no need to install or set up the operator individually on each cluster. The deployment process is streamlined and significantly more efficient. The image illustrates a GitOps feature set and use cases, showing a workflow involving tools like Helm and Jenkins, Git repositories, and Kubernetes clusters for continuous deployment and automation. It highlights concepts such as single source of truth, everything as code, auditable processes, and multi-cluster deployments.</p> <p>For more details on deploying and managing resources with GitOps, explore additional resources such as:</p> <p>Kubernetes Documentation GitOps Tools Overview Modern CI/CD Practices By leveraging GitOps, teams can achieve high levels of deployment efficiency, improved management across diverse environments, and robust recovery mechanisms, making it an essential strategy for modern infrastructure management.</p>","path":["ARGO","GitOps Introduction"],"tags":[]},{"location":"argo/gitops/introduction/#gitops-benefits-drawbacks","level":2,"title":"GitOps Benefits Drawbacks","text":"<p>GitOps Benefits Drawbacks This article reviews the key advantages and challenges associated with GitOps, providing insights for managing Kubernetes application deployments effectively.</p> <p>Benefits of GitOps GitOps offers several compelling advantages:</p> <p>It is lightweight and vendor-neutral, leveraging the open-source Git protocol to work seamlessly across diverse platforms. GitOps enables faster and safer deployments by ensuring immutable and reproducible environments. In team setups where environmental changes might occur unexpectedly, GitOps prevents unintended modifications. The GitOps operator enforces consistency by disallowing manual updates, thus eliminating configuration drift. In the event of a manual update, the GitOps operator automatically restores the desired state from Git. Developers enjoy the familiarity of using Git and CI/CD tools. The workflow remains straightforward: push the code to the repository, and a CI/CD pipeline handles testing and deployment. Git‚Äôs history tracking allows for easy comparison between declarative file revisions, making it simple to correlate changes with specific change requests. Note</p> <p>For more details on CI/CD integrations with GitOps, refer to the official GitOps Documentation.</p> <p>Challenges of GitOps Despite its advantages, GitOps introduces a few challenges that need to be addressed:</p> <p>Centralized Secret Management: GitOps does not secure secrets by default. Although it recommends storing secrets declaratively in Git repositories, operations teams must integrate additional tools to manage secrets securely.</p> <p>Repository Organization: As the number of microservices and environments grows, organizing Git repositories becomes complex. Decisions need to be made about whether to store source code and manifests in a single repository or use multiple repositories/branches. There is no one-size-fits-all solution‚Äîeach organization must tailor this approach to fit its specific application requirements.</p> <p>Update Conflicts: Frequent application updates in continuous delivery environments can trigger simultaneous CI processes, leading to multiple pull requests. This may result in conflicts when several processes attempt to update the GitOps repository concurrently, often necessitating manual resolution.</p> <p>Governance and Policy Enforcement: Relying on pull requests (PRs) for approval can reduce the effectiveness of enforcing strict company policies after a PR is approved.</p> <p>Configuration Validation: Malformed YAML files or configuration errors can occur. External validation tools are essential for ensuring that manifest files meet the required standards.</p> <p>Warning</p> <p>Ensure that you integrate robust secret management and repository organization strategies when implementing GitOps to mitigate these challenges effectively.</p>","path":["ARGO","GitOps Introduction"],"tags":[]},{"location":"argo/gitops/introduction/#gitops-projects-tools","level":2,"title":"GitOps Projects Tools","text":"<p>In this article, we explore a diverse range of GitOps projects and tools available as of this recording. These solutions have been designed to streamline the management of Kubernetes applications by leveraging GitOps practices through various controllers and automation tools.</p> <p>Overview</p> <p>This guide provides insights into both GitOps controllers and complementary tools that enhance Kubernetes application deployment and management.</p> <p>GitOps Controller: ArgoCD ArgoCD is our primary GitOps controller. It is a declarative continuous deployment tool for Kubernetes that simplifies application management while ensuring your deployment process remains automated and consistent.</p> <p>Additional GitOps Tools Enhance your Kubernetes GitOps workflows with these additional tools:</p> <ul> <li>Atlantis: Automates Terraform workflows by integrating directly with pull requests.</li> <li>AutoApply: Automatically applies configuration changes from a Git repository to your Kubernetes cluster, saving manual intervention.</li> <li>CloudRollout: Provides advanced feature flagging, enabling teams to deploy and iterate rapidly without sacrificing safety.</li> <li>GitOps with FluxCD: Offers continuous and progressive delivery solutions optimized for Kubernetes environments.</li> <li>Helm Operator: Automates the release of Helm charts following GitOps principles.</li> <li>The image lists various GitOps projects and tools, each represented by a logo and name, such as ArgoCD, FluxCD, and JenkinsX.</li> <li>Flagger: A Kubernetes operator focused on progressive delivery. It supports canary releases, A/B testing, and blue-green deployments.</li> <li>Ignite: Functions as a virtual machine manager with a container-like user experience, incorporating built-in GitOps capabilities.</li> <li>Faros: A GitOps controller that utilizes Custom Resource Definitions (CRDs) for streamlined operations.</li> <li>GitKube: Facilitates Docker image building and deployment to Kubernetes clusters through a Git push workflow.</li> <li>Jenkins X: Tailored for Kubernetes, this CI/CD platform provides pipeline automation with integrated GitOps and preview environments.</li> <li>KubeStack: Leverages Terraform to provide a GitOps framework for cloud Kubernetes distributions such as AKS, GKE, and EKS, complete with CI/CD examples.</li> <li>Weave Cloud: An automation and management platform designed to support both development and DevOps teams.</li> <li>PipeCD: A continuous delivery solution built for declarative Kubernetes, serverless, and infrastructure applications.</li> </ul>","path":["ARGO","GitOps Introduction"],"tags":[]},{"location":"aws/","level":1,"title":"Index","text":"","path":["AWS","Index"],"tags":[]},{"location":"aws/#title-aws","level":2,"title":"title: AWS","text":"","path":["AWS","Index"],"tags":[]},{"location":"aws/EKS/","level":1,"title":"Home","text":"<p>Introduction:</p> <ul> <li>Course Introduction</li> </ul> <p>EKS Fundamentals:</p> <ul> <li>What is EKS</li> <li>Common Use Cases</li> <li>Architecture</li> <li>Deployment Options</li> <li>Tools needed for EKS</li> <li>Networking</li> <li>Authentication</li> </ul> <p>EKS Networking:</p> <ul> <li>How networking works</li> <li>Prefix Delegation</li> <li>IPv6</li> <li>Network Policies</li> <li>Network Policies Demo</li> </ul> <p>EKS Storage:</p> <ul> <li>EKS EBSElastic Block Store</li> <li>EKS EFSElastic File System</li> <li>EKS Other Storage</li> </ul> <p>EKS Secrets:</p> <ul> <li>EKS Secrets Intro</li> <li>Kubernetes Secrets Options</li> </ul> <p>Load Balancers:</p> <ul> <li>LoadBalancers Intro</li> <li>Gateway Ingress</li> <li>VPC Lattice</li> </ul> <p>Compute Scaling:</p> <ul> <li>Fargate</li> <li>EKS Node Groups</li> <li>Karpenter</li> <li>Compute Demo</li> </ul> <p>Redundancy Resiliency:</p> <ul> <li>Cluster Access</li> <li>IRSAIAM Roles for Service Accounts</li> <li>Pod Identity</li> <li>SGSecurity Groups for Prods</li> </ul> <p>Upgrades and Maintenance:</p> <ul> <li>EKS monitoring</li> <li>Upgrade Cycles</li> <li>EKS Upgrades</li> <li>EKS Addon</li> </ul>","path":["AWS","EKS","Home"],"tags":[]},{"location":"aws/IAM/","level":1,"title":"Home","text":"<p>Introduction:</p> <ul> <li>Course Introduction</li> </ul> <p>Introduction to AWS Identity and Access Management:</p> <ul> <li>IAM Overview</li> <li>AWS Account</li> <li>Demo Creating AWS Account</li> <li>IAM Users</li> <li>Demo Create IAM User</li> <li>AWS CLI and SDK</li> <li>Demo IAM Groups</li> <li>IAM Policies and Permissions</li> <li>Demo Identity Policy</li> <li>IAM Resource Based Policy</li> <li>Demo Resource Based Policy</li> <li>IAM Permission Boundaries</li> <li>Demo Permission Boundaries</li> <li>IAM Roles</li> <li>Demo Creating IAM Role</li> <li>IAM Session Policies</li> <li>Demo Session Policies</li> <li>Auditing with CloudTrail</li> <li>Demo CloudTrail</li> </ul> <p>IAM Policies Federation STS and MFA:</p> <ul> <li>Inline vs Managed Policy</li> <li>Demo Inline Policy</li> <li>IAM Policy Building Blocks</li> <li>Demo Policy with Conditions</li> <li>MFA and Password Policies</li> <li>Demo MFA and Password Policies</li> <li>Security Token ServiceSTS</li> <li>AWS Resource Access Manager</li> <li>Identity Federation</li> <li>AWS Private Link</li> </ul> <p>Configure AWS IAM at Scale:</p> <ul> <li>Overview</li> <li>AWS Organizations</li> <li>IAM Cross Account Access</li> <li>Demo Cross Account Access</li> <li>Centralized Logging and Monitoring</li> <li>CloudTrail</li> <li>Monitoring Demo CloudTrail</li> <li>CloudWatch</li> <li>Demo CloudWatch</li> <li>AWS Config</li> <li>Demo AWS Config</li> <li>IAM Anywhere</li> <li>IAM Identity Center</li> </ul>","path":["AWS","IAM","Home"],"tags":[]},{"location":"aws/VPC/","level":1,"title":"VPC","text":"<p>Introduction:</p> <ul> <li>Course Introduction</li> </ul> <p>Core Networking Services:</p> <ul> <li>VPC Overview</li> <li>Custom VPC Demo</li> <li>Default VPC Demo</li> <li>Subnets</li> <li>Subnets Demo</li> <li>Routing in VPC</li> <li>Route Table Demo</li> <li>Internet Gateways VPC</li> <li>Internet Gateway Demo</li> <li>NAT Gateways VPC</li> <li>NAT Gateways VPC Demo</li> <li>Public vs Private Subnets</li> <li>DNS VPC</li> <li>DNS VPC Demo</li> <li>Elastic IP</li> <li>Elastic IP Demo</li> <li>Security Groups NACLs</li> <li>Security Groups Demo</li> <li>NACLs Demo</li> <li>Load Balancers</li> <li>Load Balancers Demo</li> <li>Route 53</li> <li>Route 53 Demo</li> </ul> <p>Transit Networks:</p> <ul> <li>VPN</li> <li>Direct Connect</li> <li>VPC Peering</li> <li>VPC Peering Demo</li> <li>Transit Gateway</li> <li>Privatelink</li> </ul> <p>Edge Networks:</p> <ul> <li>CloudFront</li> <li>CloudFront Demo</li> <li>Cloudfront and LambdaEdge</li> <li>Global Accelerator</li> </ul>","path":["AWS","VPC","Home"],"tags":[]},{"location":"crossplane/","level":1,"title":"Crossplane","text":"","path":["CROSSPLANE","Crossplane"],"tags":["Crossplane","Komoplane"]},{"location":"crossplane/#import-existing-resources","level":2,"title":"Import existing Resources","text":"<p>Import existing resources into your control plane for Crossplane to manage</p> <p>If you have resources that are already provisioned in a Provider, you can import them as managed resources and let Crossplane manage them. A managed resource‚Äôs managementPolicies field enables importing external resources into Crossplane.</p>","path":["CROSSPLANE","Crossplane"],"tags":["Crossplane","Komoplane"]},{"location":"crossplane/#import-resources-in-observe-only-mode","level":3,"title":"Import resources in observe only mode","text":"<p>Start by importing external resources with an Observe management policy.</p> <p>Crossplane imports observe only resources but never changes or deletes the resources.</p> <p>Warning</p> <p>The managed resource <code>managementPolicies</code> option is a beta feature.</p> <p>The Provider determines support for management policies. Refer to the Provider's documentation to see if the Provider supports management policies.</p>","path":["CROSSPLANE","Crossplane"],"tags":["Crossplane","Komoplane"]},{"location":"crossplane/#apply-the-observe-management-policy","level":3,"title":"Apply the Observe management policy","text":"<ol> <li>Create a new managed resource matching the apiVersion and kind of the resource to import and add managementPolicies: <code>Observe</code> to the spec For example, to import an AWS EC2 Instance, Create a new resource with the managementPolicies: <code>Observe</code> set.</li> <li>Add the crossplane.io/external-name annotation for the resource. This name must match the ID inside the Provider. For example, for an AWS EC2 Instance use the <code>Instance ID</code>, apply the crossplane.io/external-name annotation with the value <code>i-037556a7512bd1f4b</code>.</li> <li>Create a name to use for the Kubernetes object. For example, name the Kubernetes object imported-ec2-instance.</li> <li>If more than one resource inside the Provider shares the same name, identify the specific resource with a unique spec.forProvider field. For example, only import the GCP SQL database in the us-central1 region.</li> </ol> imported-ec2-instance.yaml<pre><code>apiVersion: ec2.aws.m.upbound.io/v1beta1\nkind: Instance\nmetadata:\n  name: imported-ec2-instance\n  annotations:\n    crossplane.io/external-name: i-037556a7512bd1f4b\nspec:\n  managementPolicies: [\"Observe\"]\n  forProvider:\n    region: eu-west-3\n  providerConfigRef:\n    name: default\n    kind: ProviderConfig \n</code></pre> <p>Warning</p> <p>If u dont precise providerConfigRef it try to lookup for ClusterProviderConfig </p>","path":["CROSSPLANE","Crossplane"],"tags":["Crossplane","Komoplane"]},{"location":"crossplane/#apply-the-managed-resource","level":3,"title":"Apply the managed resource","text":"<p>Apply the new managed resource. Crossplane syncs the status of the external resource in the cloud with the newly created managed resource.</p> <pre><code>kubectl apply -f imported-ec2.yaml\ninstance.ec2.aws.m.upbound.io/imported-ec2-instance created\n</code></pre>","path":["CROSSPLANE","Crossplane"],"tags":["Crossplane","Komoplane"]},{"location":"crossplane/#view-the-discovered-resource","level":3,"title":"View the discovered resource","text":"<p>Crossplane discovers the managed resource and populates the status.atProvider fields with the values from the external resource.</p> <pre><code>apiVersion: ec2.aws.m.upbound.io/v1beta1\nkind: Instance\nmetadata:\n  annotations:\n    crossplane.io/external-name: i-037556a7512bd1f4b\n    kubectl.kubernetes.io/last-applied-configuration: |\n      {\"apiVersion\":\"ec2.aws.m.upbound.io/v1beta1\",\"kind\":\"Instance\",\"metadata\":{\"annotations\":{\"crossplane.io/external-name\":\"i-037556a7512bd1f4b\"},\"name\":\"imported-ec2-instance\",\"namespace\":\"crossplane-system\"},\"spec\":{\"forProvider\":{\"region\":\"eu-west-3\"},\"managementPolicies\":[\"Observe\"],\"providerConfigRef\":{\"kind\":\"ProviderConfig\",\"name\":\"default\"}}}\n  creationTimestamp: \"2025-11-25T04:34:45Z\"\n  finalizers:\n  - finalizer.managedresource.crossplane.io\n  generation: 2\n  name: imported-ec2-instance\n  namespace: crossplane-system\n  resourceVersion: \"62176\"\n  uid: e0aecc43-1878-41ea-a58f-e9f2557f2a3e\nspec:\n  forProvider:\n    region: eu-west-3\n  initProvider: {}\n  managementPolicies:\n  - Observe\n  providerConfigRef:\n    kind: ProviderConfig\n    name: default\nstatus:\n  atProvider:\n    ami: ami-00769f46ca3bb4381\n    arn: arn:aws:ec2:eu-west-3:478084574796:instance/i-037556a7512bd1f4b\n    associatePublicIpAddress: false\n    availabilityZone: eu-west-3c\n    capacityReservationSpecification:\n      capacityReservationPreference: open\n      capacityReservationTarget: {}\n    cpuOptions:\n      amdSevSnp: \"\"\n      coreCount: 1\n      threadsPerCore: 2\n    creditSpecification:\n      cpuCredits: unlimited\n    disableApiStop: false\n    disableApiTermination: false\n    ebsOptimized: true\n    enclaveOptions:\n      enabled: false\n    getPasswordData: false\n    hibernation: false\n    hostId: \"\"\n    iamInstanceProfile: \"\"\n    id: i-037556a7512bd1f4b\n    instanceInitiatedShutdownBehavior: stop\n    instanceLifecycle: \"\"\n    instanceMarketOptions: {}\n    instanceState: stopped\n    instanceType: t3.micro\n    ipv6AddressCount: 0\n    keyName: \"\"\n    launchTemplate: {}\n    maintenanceOptions:\n      autoRecovery: default\n    metadataOptions:\n      httpEndpoint: enabled\n      httpProtocolIpv6: disabled\n      httpPutResponseHopLimit: 2\n      httpTokens: required\n      instanceMetadataTags: disabled\n    monitoring: false\n    outpostArn: \"\"\n    passwordData: \"\"\n    placementGroup: \"\"\n    placementGroupId: \"\"\n    placementPartitionNumber: 0\n    primaryNetworkInterface:\n      deleteOnTermination: true\n      networkInterfaceId: eni-05970f9b52533b159\n    primaryNetworkInterfaceId: eni-05970f9b52533b159\n    privateDns: ip-172-31-42-191.eu-west-3.compute.internal\n    privateDnsNameOptions:\n      enableResourceNameDnsARecord: true\n      enableResourceNameDnsAaaaRecord: false\n      hostnameType: ip-name\n    privateIp: 172.31.42.191\n    publicDns: \"\"\n    publicIp: \"\"\n    region: eu-west-3\n    rootBlockDevice:\n      deleteOnTermination: true\n      deviceName: /dev/xvda\n      encrypted: false\n      iops: 3000\n      kmsKeyId: \"\"\n      throughput: 125\n      volumeId: vol-03fdca6cb6fd07a3f\n      volumeSize: 8\n      volumeType: gp3\n    securityGroups:\n    - launch-wizard-1\n    sourceDestCheck: true\n    spotInstanceRequestId: \"\"\n    subnetId: subnet-0df5c09839230c14c\n    tags:\n      Name: app1\n    tagsAll:\n      Name: app1\n    tenancy: default\n    vpcSecurityGroupIds:\n    - sg-0d69b6ea8692119d8\n  conditions:\n  - lastTransitionTime: \"2025-11-25T04:34:51Z\"\n    observedGeneration: 2\n    reason: ReconcileSuccess\n    status: \"True\"\n    type: Synced\n  - lastTransitionTime: \"2025-11-25T04:34:51Z\"\n    reason: Available\n    status: \"True\"\n    type: Ready\n  - lastTransitionTime: \"2025-11-25T04:34:51Z\"\n    reason: Success\n    status: \"True\"\n    type: LastAsyncOperation\n</code></pre>","path":["CROSSPLANE","Crossplane"],"tags":["Crossplane","Komoplane"]},{"location":"crossplane/#control-imported-observe-only-resources","level":2,"title":"Control imported observe only resources","text":"<p>Crossplane can take active control of observe only imported resources by changing the managementPolicies after import. Change the managementPolicies field of the managed resource to [\"*\"]. Copy any required parameter values from status.atProvider and provide them in spec.forProvider.</p> <p>Warning</p> <p>Manually copy the important <code>spec.atProvider</code> values to <code>spec.forProvider</code>.</p> <pre><code>apiVersion: sql.gcp.upbound.io/v1beta1\nkind: DatabaseInstance\nmetadata:\n  name: my-imported-database\n  annotations:\n    crossplane.io/external-name: my-external-database\nspec:\n  managementPolicies: [\"*\"]\n  forProvider:\n    databaseVersion: POSTGRES_14\n    region: us-central1\n    settings:\n    - diskSize: 100\n      tier: db-custom-4-26624\nstatus:\n  atProvider:\n    databaseVersion: POSTGRES_14\n    region: us-central1\n    # Removed for brevity\n    settings:\n    - diskSize: 100\n      tier: db-custom-4-26624\n      # Removed for brevity\n  conditions:\n    - lastTransitionTime: \"2023-02-22T07:16:51Z\"\n      reason: Available\n      status: \"True\"\n      type: Ready\n    - lastTransitionTime: \"2023-02-22T11:16:45Z\"\n      reason: ReconcileSuccess\n      status: \"True\"\n      type: Synced\n</code></pre> <p>Crossplane now fully manages the imported resource. Crossplane applies any changes to the managed resource in the Provider's external resource.</p> <p>We can by now specify new tag </p> Apply new tag on your manifest<pre><code>apiVersion: ec2.aws.m.upbound.io/v1beta1\nkind: Instance\nmetadata:\n  name: imported-ec2-instance\n  annotations:\n    crossplane.io/external-name: i-037556a7512bd1f4b\nspec:\n  managementPolicies: [\"*\"]\n  forProvider:\n    region: eu-west-3\n    tags:\n      company: mathod\n      project: app1\n      environment: production\n  providerConfigRef:\n    name: default\n    kind: ProviderConfig \n</code></pre> New tag on instance<pre><code>Name:         imported-ec2-instance\nNamespace:    crossplane-system\nLabels:       &lt;none&gt;\nAnnotations:  crossplane.io/external-name: i-037556a7512bd1f4b\nAPI Version:  ec2.aws.m.upbound.io/v1beta1\nKind:         Instance\nMetadata:\n  Creation Timestamp:  2025-11-25T04:34:45Z\n  Finalizers:\n    finalizer.managedresource.crossplane.io\n  Generation:        6\n  Resource Version:  62949\n  UID:               e0aecc43-1878-41ea-a58f-e9f2557f2a3e\nSpec:\n  For Provider:\n    Ami:                ami-00769f46ca3bb4381\n    Availability Zone:  eu-west-3c\n    Capacity Reservation Specification:\n      Capacity Reservation Preference:  open\n    Credit Specification:\n      Cpu Credits:  unlimited\n    Ebs Optimized:  true\n    Enclave Options:\n    Instance Initiated Shutdown Behavior:  stop\n    Instance Type:                         t3.micro\n    Maintenance Options:\n      Auto Recovery:  default\n    Metadata Options:\n      Http Endpoint:                enabled\n      httpProtocolIpv6:             disabled\n      Http Put Response Hop Limit:  2\n      Http Tokens:                  required\n      Instance Metadata Tags:       disabled\n    Primary Network Interface:\n      Network Interface Id:  eni-05970f9b52533b159\n    Private Dns Name Options:\n      Enable Resource Name Dns A Record:  true\n      Hostname Type:                      ip-name\n    Region:                               eu-west-3\n    Tags:\n      Company:                      mathod\n      Crossplane - Kind:            instance.ec2.aws.m.upbound.io\n      Crossplane - Name:            imported-ec2-instance\n      Crossplane - Providerconfig:  default\n      Environment:                  production\n      Project:                      app1\n    Tenancy:                        default\n  Init Provider:\n  Management Policies:\n    *\n  Provider Config Ref:\n    Kind:  ProviderConfig\n    Name:  default\nStatus:\n  At Provider:\n    Ami:                          ami-00769f46ca3bb4381\n    Arn:                          arn:aws:ec2:eu-west-3:478084574796:instance/i-037556a7512bd1f4b\n    Associate Public Ip Address:  false\n    Availability Zone:            eu-west-3c\n    Capacity Reservation Specification:\n      Capacity Reservation Preference:  open\n      Capacity Reservation Target:\n    Cpu Options:\n      Amd Sev Snp:       \n      Core Count:        1\n      Threads Per Core:  2\n    Credit Specification:\n      Cpu Credits:            unlimited\n    Disable API Stop:         false\n    Disable API Termination:  false\n    Ebs Optimized:            true\n    Enclave Options:\n      Enabled:                             false\n    Force Destroy:                         false\n    Get Password Data:                     false\n    Hibernation:                           false\n    Host Id:                               \n    Iam Instance Profile:                  \n    Id:                                    i-037556a7512bd1f4b\n    Instance Initiated Shutdown Behavior:  stop\n    Instance Lifecycle:                    \n    Instance Market Options:\n    Instance State:    stopped\n    Instance Type:     t3.micro\n    ipv6AddressCount:  0\n    Key Name:          \n    Launch Template:\n    Maintenance Options:\n      Auto Recovery:  default\n    Metadata Options:\n      Http Endpoint:                enabled\n      httpProtocolIpv6:             disabled\n      Http Put Response Hop Limit:  2\n      Http Tokens:                  required\n      Instance Metadata Tags:       disabled\n    Monitoring:                     false\n    Outpost Arn:                    \n    Password Data:                  \n    Placement Group:                \n    Placement Group Id:             \n    Placement Partition Number:     0\n    Primary Network Interface:\n      Delete On Termination:       true\n      Network Interface Id:        eni-05970f9b52533b159\n    Primary Network Interface Id:  eni-05970f9b52533b159\n    Private Dns:                   ip-172-31-42-191.eu-west-3.compute.internal\n    Private Dns Name Options:\n      Enable Resource Name Dns A Record:     true\n      Enable Resource Name Dns Aaaa Record:  false\n      Hostname Type:                         ip-name\n    Private Ip:                              172.31.42.191\n    Public Dns:                              \n    Public Ip:                               \n    Region:                                  eu-west-3\n    Root Block Device:\n      Delete On Termination:  true\n      Device Name:            /dev/xvda\n      Encrypted:              false\n      Iops:                   3000\n      Kms Key Id:             \n      Throughput:             125\n      Volume Id:              vol-03fdca6cb6fd07a3f\n      Volume Size:            8\n      Volume Type:            gp3\n    Security Groups:\n      launch-wizard-1\n    Source Dest Check:         true\n    Spot Instance Request Id:  \n    Subnet Id:                 subnet-0df5c09839230c14c\n    Tags:\n      Name:                         app1\n      Company:                      mathod\n      Crossplane - Kind:            instance.ec2.aws.m.upbound.io\n      Crossplane - Name:            imported-ec2-instance\n      Crossplane - Providerconfig:  default\n      Environment:                  production\n      Project:                      app1\n    Tags All:\n      Name:                         app1\n      Company:                      mathod\n      Crossplane - Kind:            instance.ec2.aws.m.upbound.io\n      Crossplane - Name:            imported-ec2-instance\n      Crossplane - Providerconfig:  default\n      Environment:                  production\n      Project:                      app1\n    Tenancy:                        default\n    User Data Replace On Change:    false\n    Vpc Security Group Ids:\n      sg-0d69b6ea8692119d8\n  Conditions:\n    Last Transition Time:  2025-11-25T04:40:18Z\n    Observed Generation:   6\n    Reason:                ReconcileSuccess\n    Status:                True\n    Type:                  Synced\n    Last Transition Time:  2025-11-25T04:34:51Z\n    Reason:                Available\n    Status:                True\n    Type:                  Ready\n    Last Transition Time:  2025-11-25T04:34:51Z\n    Reason:                Success\n    Status:                True\n    Type:                  LastAsyncOperation\nEvents:\n  Type    Reason                   Age                From                                                 Message\n  ----    ------                   ----               ----                                                 -------\n  Normal  UpdatedExternalResource  6s (x2 over 114s)  managed/ec2.aws.m.upbound.io/v1beta1, kind=instance  Successfully requested update of external resource\n</code></pre> <p><pre><code>kubectl get instance\nNAME                    SYNCED   READY   EXTERNAL-NAME         AGE\nimported-ec2-instance   True     True    i-037556a7512bd1f4b   7m55s\n\nkubectl get managed\nNAME                                                  SYNCED   READY   EXTERNAL-NAME         AGE\ninstance.ec2.aws.m.upbound.io/imported-ec2-instance   True     True    i-037556a7512bd1f4b   8m25s\n\nkubectl get instance.ec2.aws.m.upbound.io/imported-ec2-instance\nNAME                    SYNCED   READY   EXTERNAL-NAME         AGE\nimported-ec2-instance   True     True    i-037556a7512bd1f4b   8m31s\n\nkubectl delete instance.ec2.aws.m.upbound.io/imported-ec2-instance\ninstance.ec2.aws.m.upbound.io \"imported-ec2-instance\" deleted from crossplane-system namespace\n</code></pre> The instances as been removed from AWS since we deleted the resources !</p> <p>DOCUMENTATION</p> <pre><code>url: https://docs.crossplane.io/latest\nimage: https://avatars.githubusercontent.com/u/45158470?s=280&amp;v=4\n</code></pre> <pre><code>url: https://github.com/DevOpsHiveHQ/awesome-crossplane\n</code></pre> <pre><code>url: https://docs.crossplane.io/latest/guides/import-existing-resources/\n</code></pre> <pre><code>url: https://docs.crossplane.io/v2.1/guides/upgrade-to-crossplane-v2/\n</code></pre> <pre><code>url: https://docs.crossplane.io/v2.1/whats-new/\n</code></pre>","path":["CROSSPLANE","Crossplane"],"tags":["Crossplane","Komoplane"]},{"location":"crossplane/resources/Organization/","level":1,"title":"Index","text":"<p>User Defined Values</p> <p>{{{user-defined-values}}}</p>","path":["CROSSPLANE","Resources","Index"],"tags":["Crossplane","AWS","Organization"]},{"location":"crossplane/resources/Organization/#account","level":2,"title":"Account","text":"<pre><code>apiVersion: organizations.aws.m.upbound.io/v1beta1\nkind: Account\nmetadata:\n  annotations:\n    meta.upbound.io/example-id: organizations/v1beta1/account\n  labels:\n    testing.upbound.io/example-name: account\n  name: account\n  namespace: upbound-system\nspec:\n  forProvider:\n    email: &lt;new-account-email&gt;\n    name: my_new_account\n</code></pre>","path":["CROSSPLANE","Resources","Index"],"tags":["Crossplane","AWS","Organization"]},{"location":"crossplane/resources/Organization/#delegatedadministrator","level":2,"title":"DelegatedAdministrator","text":"","path":["CROSSPLANE","Resources","Index"],"tags":["Crossplane","AWS","Organization"]},{"location":"crossplane/resources/Organization/#organization","level":2,"title":"Organization","text":"<pre><code>apiVersion: organizations.aws.m.upbound.io/v1beta1\nkind: Organization\nmetadata:\n  name: org\n  namespace: upbound-system\nspec:\n  forProvider:\n    awsServiceAccessPrincipals: #(1)!\n      - cloudtrail.amazonaws.com\n      - config.amazonaws.com\n    featureSet: ALL #(2)!\n</code></pre> <ol> <li> <p>List of AWS service principal names for which you want to enable integration with your organization. </p> <p>This is typically in the form of a URL, such as service-abbreviation.amazonaws.com. </p> <p>Organization must have feature_set set to ALL. </p> <p>Some services do not support enablement via this endpoint, see warning in aws docs.</p> </li> <li> <p>Specify <code>ALL</code> (default) or <code>CONSOLIDATED_BILLING</code>.</p> </li> </ol>","path":["CROSSPLANE","Resources","Index"],"tags":["Crossplane","AWS","Organization"]},{"location":"crossplane/resources/Organization/#organizationalunit","level":2,"title":"OrganizationalUnit","text":"<pre><code>apiVersion: organizations.aws.m.upbound.io/v1beta1\nkind: OrganizationalUnit\nmetadata:\n  annotations:\n    meta.upbound.io/example-id: organizations/v1beta1/organizationalunit\n  labels:\n    testing.upbound.io/example-name: example\n  name: example\n  namespace: upbound-system\nspec:\n  forProvider:\n    name: example\n    parentId: &lt;parent-id&gt;\n</code></pre>","path":["CROSSPLANE","Resources","Index"],"tags":["Crossplane","AWS","Organization"]},{"location":"crossplane/resources/Organization/#policy","level":2,"title":"Policy","text":"","path":["CROSSPLANE","Resources","Index"],"tags":["Crossplane","AWS","Organization"]},{"location":"crossplane/resources/Organization/#policyattachment","level":2,"title":"PolicyAttachment","text":"","path":["CROSSPLANE","Resources","Index"],"tags":["Crossplane","AWS","Organization"]},{"location":"crossplane/resources/S3/","level":1,"title":"Index","text":"<p>User Defined Values</p> <p>{{{user-defined-values}}}</p>","path":["CROSSPLANE","Resources","Index"],"tags":["Crossplane","S3","AWS"]},{"location":"crossplane/resources/S3/#bucket","level":2,"title":"Bucket","text":"Bucket with User Defined ValuesBucket Bucket with User Defined Values<pre><code>apiVersion: s3.aws.m.upbound.io/v1beta1\nkind: Bucket\nmetadata:\n  namespace: NAMESPACE\n  name: APPNAME-SERVICE-ENVIRONMENT-bucket\nspec:\n  forProvider:\n    region: REGION\n    tags:\n      company: COMPANY\n      appName: APPNAME\n      service: SERVICE\n      environment: ENVIRONMENT\n      team: TEAM\n      kind: bucket\n      region: REGION\n      account: ACCOUNT_ID\n      createdBy: mathod\n      createdAt: 2026-06-01\n      provisionedBy: crossplane\n  providerConfigRef:\n    name: default\n    kind: ProviderConfig\n</code></pre> Bucket with fixed name<pre><code>apiVersion: s3.aws.m.upbound.io/v1beta1\nkind: Bucket\nmetadata:\n  name: app1-bucket-mathod\n  namespace: crossplane-system\nspec:\n  forProvider:\n    region: eu-west-3\n    tags:\n      company: mathod\n      project: app1\n      environment: production\n  providerConfigRef:\n    name: default\n    kind: ProviderConfig\n</code></pre> Bucket with autogenerated name prefix<pre><code>apiVersion: s3.aws.m.upbound.io/v1beta1\nkind: Bucket\nmetadata:\n  generateName: app1-bucket-mathod-\n  namespace: crossplane-system\nspec:\n  forProvider:\n    region: eu-west-3\n    tags:\n      company: mathod\n      project: app1\n      environment: production\n  providerConfigRef:\n    name: default\n    kind: ProviderConfig\n</code></pre>","path":["CROSSPLANE","Resources","Index"],"tags":["Crossplane","S3","AWS"]},{"location":"crossplane/resources/S3/#bucketaccelerateconfiguration","level":2,"title":"BucketAccelerateConfiguration","text":"","path":["CROSSPLANE","Resources","Index"],"tags":["Crossplane","S3","AWS"]},{"location":"crossplane/resources/S3/#bucketacl","level":2,"title":"BucketACL","text":"","path":["CROSSPLANE","Resources","Index"],"tags":["Crossplane","S3","AWS"]},{"location":"crossplane/resources/S3/#bucketanalyticsconfiguration","level":2,"title":"BucketAnalyticsConfiguration","text":"","path":["CROSSPLANE","Resources","Index"],"tags":["Crossplane","S3","AWS"]},{"location":"crossplane/resources/S3/#bucketcorsconfiguration","level":2,"title":"BucketCorsConfiguration","text":"","path":["CROSSPLANE","Resources","Index"],"tags":["Crossplane","S3","AWS"]},{"location":"crossplane/resources/S3/#bucketintelligenttieringconfiguration","level":2,"title":"BucketIntelligentTieringConfiguration","text":"","path":["CROSSPLANE","Resources","Index"],"tags":["Crossplane","S3","AWS"]},{"location":"crossplane/resources/S3/#bucketinventory","level":2,"title":"BucketInventory","text":"","path":["CROSSPLANE","Resources","Index"],"tags":["Crossplane","S3","AWS"]},{"location":"crossplane/resources/S3/#bucketlifecycleconfiguration","level":2,"title":"BucketLifecycleConfiguration","text":"","path":["CROSSPLANE","Resources","Index"],"tags":["Crossplane","S3","AWS"]},{"location":"crossplane/resources/S3/#bucketlogging","level":2,"title":"BucketLogging","text":"","path":["CROSSPLANE","Resources","Index"],"tags":["Crossplane","S3","AWS"]},{"location":"crossplane/resources/S3/#bucketmetric","level":2,"title":"BucketMetric","text":"","path":["CROSSPLANE","Resources","Index"],"tags":["Crossplane","S3","AWS"]},{"location":"crossplane/resources/S3/#bucketnotification","level":2,"title":"BucketNotification","text":"","path":["CROSSPLANE","Resources","Index"],"tags":["Crossplane","S3","AWS"]},{"location":"crossplane/resources/S3/#bucketobject","level":2,"title":"BucketObject","text":"","path":["CROSSPLANE","Resources","Index"],"tags":["Crossplane","S3","AWS"]},{"location":"crossplane/resources/S3/#bucketobjectlockconfiguration","level":2,"title":"BucketObjectLockConfiguration","text":"","path":["CROSSPLANE","Resources","Index"],"tags":["Crossplane","S3","AWS"]},{"location":"crossplane/resources/S3/#bucketownershipcontrols","level":2,"title":"BucketOwnershipControls","text":"","path":["CROSSPLANE","Resources","Index"],"tags":["Crossplane","S3","AWS"]},{"location":"crossplane/resources/S3/#bucketpolicy","level":2,"title":"BucketPolicy","text":"","path":["CROSSPLANE","Resources","Index"],"tags":["Crossplane","S3","AWS"]},{"location":"crossplane/resources/S3/#bucketpublicaccessblock","level":2,"title":"BucketPublicAccessBlock","text":"","path":["CROSSPLANE","Resources","Index"],"tags":["Crossplane","S3","AWS"]},{"location":"crossplane/resources/S3/#bucketreplicationconfiguration","level":2,"title":"BucketReplicationConfiguration","text":"","path":["CROSSPLANE","Resources","Index"],"tags":["Crossplane","S3","AWS"]},{"location":"crossplane/resources/S3/#bucketrequestpaymentconfiguration","level":2,"title":"BucketRequestPaymentConfiguration","text":"","path":["CROSSPLANE","Resources","Index"],"tags":["Crossplane","S3","AWS"]},{"location":"crossplane/resources/S3/#bucketserversideencryptionconfiguration","level":2,"title":"BucketServerSideEncryptionConfiguration","text":"<pre><code>apiVersion: s3.aws.m.upbound.io/v1beta1\nkind: BucketServerSideEncryptionConfiguration\nmetadata:\n  namespace: dev-environment\n  name: my-secure-bucket-encryption\nspec:\n  forProvider:\n    bucket: my-secure-bucket\n    rules:\n      - applyServerSideEncryptionByDefault:\n          sseAlgorithm: AES256\n    tags:\n      kind: BucketServerSideEncryptionConfiguration\n  providerConfigRef:\n    name: aws-provider\n    kind: ProviderConfig\n</code></pre>","path":["CROSSPLANE","Resources","Index"],"tags":["Crossplane","S3","AWS"]},{"location":"crossplane/resources/S3/#bucketversioning","level":2,"title":"BucketVersioning","text":"","path":["CROSSPLANE","Resources","Index"],"tags":["Crossplane","S3","AWS"]},{"location":"crossplane/resources/S3/#bucketwebsiteconfiguration","level":2,"title":"BucketWebsiteConfiguration","text":"","path":["CROSSPLANE","Resources","Index"],"tags":["Crossplane","S3","AWS"]},{"location":"crossplane/resources/S3/#directorybucket","level":2,"title":"DirectoryBucket","text":"","path":["CROSSPLANE","Resources","Index"],"tags":["Crossplane","S3","AWS"]},{"location":"crossplane/resources/S3/#object","level":2,"title":"Object","text":"","path":["CROSSPLANE","Resources","Index"],"tags":["Crossplane","S3","AWS"]},{"location":"crossplane/resources/S3/#objectcopy","level":2,"title":"ObjectCopy","text":"<p>Documentation</p> <pre><code>url: https://marketplace.upbound.io/providers/upbound/provider-aws-s3/latest/resources/s3.aws.m.upbound.io/Bucket/v1beta1\nimage: https://raw.githubusercontent.com/cncf/artwork/9e203aa38643bbf0fcb081dbaa80abbd0f6f0698/projects/crossplane/stacked/color/crossplane-stacked-color.svg\n</code></pre>","path":["CROSSPLANE","Resources","Index"],"tags":["Crossplane","S3","AWS"]},{"location":"kubernetes/affinity/","level":1,"title":"Affinity","text":"","path":["KUBERNETES","RESOURCES","Affinity"],"tags":["Kubernetes"]},{"location":"kubernetes/affinity/#kubernetes-affinity","level":2,"title":"Kubernetes Affinity","text":"<p>In Kubernetes, orchestrating containers is not just about keeping applications running; it‚Äôs about doing so in the most efficient, reliable, and intelligent way possible. This is where Kubernetes affinity rules step into the spotlight, offering a sophisticated mechanism for influencing how pods are distributed across a cluster. This article aims to explain the concepts of Kubernetes Node Affinity and Pod Affinity.</p>","path":["KUBERNETES","RESOURCES","Affinity"],"tags":["Kubernetes"]},{"location":"kubernetes/affinity/#understanding-kubernetes-affinity","level":3,"title":"Understanding Kubernetes Affinity","text":"<p>At its core, Kubernetes affinity is about setting rules that dictate how pods are placed relative to nodes and other pods. These rules allow users to influence the scheduling decisions of the Kubernetes scheduler, ensuring that pods are deployed on the most appropriate nodes based on various criteria, such as hardware requirements, software needs, or even geographical location. Affinity can be broadly categorized into two types: Node Affinity and Pod Affinity/Anti-Affinity.</p>","path":["KUBERNETES","RESOURCES","Affinity"],"tags":["Kubernetes"]},{"location":"kubernetes/affinity/#node-affinity","level":3,"title":"Node Affinity:","text":"<p>Node Affinity is the successor to the simpler <code>nodeSelector</code> feature, offering a more expressive syntax that allows for more nuanced selection logic. It specifies constraints or preferences that affect how pods are placed relative to nodes. These constraints can be \"hard\" (required) or \"soft\" (preferred), providing a balance between strict requirements and desirable attributes.</p> <ul> <li>Required Node Affinity ensures that pods are only scheduled on nodes that meet specific criteria, such as a particular type of hardware.</li> <li>Preferred Node Affinity attempts to place pods on nodes that meet certain preferences, but it will not prevent the pod from being scheduled if the criteria aren‚Äôt met.</li> </ul>","path":["KUBERNETES","RESOURCES","Affinity"],"tags":["Kubernetes"]},{"location":"kubernetes/affinity/#pod-affinity-and-anti-affinity","level":3,"title":"Pod Affinity and Anti-Affinity","text":"<p>While Node Affinity focuses on the relationship between pods and nodes, Pod Affinity and Anti-Affinity govern how pods are placed relative to one another. These rules can be used to ensure that certain pods are co-located in the same node, zone, or region, or to keep them separated for redundancy and fault tolerance.</p> <ul> <li>Pod Affinity encourages the scheduler to place pods together based on specified labels. This is useful for performance reasons or when pods need to be close to each other for communication purposes.</li> <li>Pod Anti-Affinity ensures that pods are not placed on the same node, zone, or region. This is crucial for eliminating single points of failure and achieving high availability.</li> </ul>","path":["KUBERNETES","RESOURCES","Affinity"],"tags":["Kubernetes"]},{"location":"kubernetes/affinity/#why-affinity-matters","level":3,"title":"Why Affinity Matters?","text":"<p>Affinity rules are essential for optimizing applications for various scenarios:</p> <ul> <li>Performance Optimization: By placing interdependent pods close to each other, network latency can be minimized, enhancing performance.</li> <li>High Availability: Pod Anti-Affinity ensures that critical components are spread across different nodes or zones, reducing the risk of simultaneous failures.</li> <li>Compliance and Data Sovereignty: Node Affinity can restrict pods to nodes in specific geographic locations, adhering to legal requirements.</li> <li>Efficient Resource Utilization: Affinity rules can help ensure that pods are placed on nodes with the appropriate resources, avoiding underutilization or overloading.</li> </ul>","path":["KUBERNETES","RESOURCES","Affinity"],"tags":["Kubernetes"]},{"location":"kubernetes/affinity/#implementing-affinity-in-kubernetes","level":3,"title":"Implementing Affinity in Kubernetes","text":"<p>Implementing Node and Pod Affinity requires defining rules in your pod specifications. Here‚Äôs a brief look at how you can specify Node and Pod Affinity:</p> <pre><code>affinity:\n  nodeAffinity:\n    requiredDuringSchedulingIgnoredDuringExecution:\n      nodeSelectorTerms:\n        - matchExpressions:\n            - key: \"hardware-type\"\n              operator: In\n              values:\n                - GPU\n    preferredDuringSchedulingIgnoredDuringExecution:\n      - weight: 1\n        preference:\n          matchExpressions:\n            - key: \"disk-type\"\n              operator: In\n              values:\n                - ssd\n  podAffinity:\n    requiredDuringSchedulingIgnoredDuringExecution:\n      - labelSelector:\n          matchExpressions:\n            - key: \"app\"\n              operator: In\n              values:\n                - webserver\n        topologyKey: \"kubernetes.io/hostname\"\n  podAntiAffinity:\n    preferredDuringSchedulingIgnoredDuringExecution:\n      - weight: 100\n        podAffinityTerm:\n          labelSelector:\n            matchExpressions:\n              - key: \"app\"\n                operator: In\n                values:\n                  - database\n          topologyKey: \"kubernetes.io/hostname\"\n</code></pre> <p>This above YAML snippet outlines basic Node Affinity, Pod Affinity, and Pod Anti-Affinity rules. It demonstrates the flexibility and power of Kubernetes‚Äô scheduling capabilities, allowing for sophisticated deployment strategies that can dramatically improve the resilience, efficiency, and performance of your applications.</p> <p>Kubernetes‚Äô Affinity features provide a powerful set of tools for fine-tuning the placement of pods across the cluster. By understanding and implementing Node Affinity and Pod Affinity/Anti-Affinity, you can ensure that your applications are not only resilient and highly available but also optimized for performance and compliance with regulatory requirements. As with any powerful tool, it‚Äôs important to use Affinity judiciously, balancing the desires for specificity and flexibility to maintain the overall health and efficiency of your cluster.</p>","path":["KUBERNETES","RESOURCES","Affinity"],"tags":["Kubernetes"]},{"location":"kubernetes/autoscaler/","level":1,"title":"Autoscaler","text":"<p>title: Kubernetes Autoscaling: HPA, VPA, and Cluster Autoscaler date: 2026-01-22 categories: Kubernetes tags:   - Kubernetes   - AutoScaling   - HPA   - VPA   - ClusterAutoscaler source: https://thekubeguy.com/kubernetes-autoscaling-hpa-vpa-and-cluster-autoscaler-d4a89f0daaec</p> <p>Welcome back to Kubernetes adventure series. This article delves into Kubernetes‚Äô autoscaling capabilities, focusing on the Horizontal Pod Autoscaler (HPA), Vertical Pod Autoscaler (VPA), and Cluster Autoscaler. We‚Äôll explore their differences, use cases, and guide you through configuring them for optimal performance.</p> <p>Kubernetes Autoscaling</p>","path":["Autoscaler"],"tags":[]},{"location":"kubernetes/autoscaler/#understanding-kubernetes-autoscaling","level":3,"title":"Understanding Kubernetes Autoscaling","text":"<p>Kubernetes autoscaling can be broadly categorized into three types: HPA, VPA, and Cluster Autoscaler. Each serves a unique purpose and operates at different layers of the Kubernetes architecture.</p>","path":["Autoscaler"],"tags":[]},{"location":"kubernetes/autoscaler/#horizontal-pod-autoscaler-hpa","level":3,"title":"Horizontal Pod Autoscaler (HPA)","text":"<p>The Horizontal Pod Autoscaler automatically adjusts the number of pods in a deployment, replication controller, stateful set, or replica set based on observed CPU utilization or other select metrics provided by custom metrics support.</p>","path":["Autoscaler"],"tags":[]},{"location":"kubernetes/autoscaler/#how-hpa-works","level":3,"title":"How HPA Works:","text":"<p>HPA operates by increasing or decreasing the count of replica pods to meet the target metric value you specify. For instance, if the target CPU utilization is set at 50% and the current utilization exceeds this threshold, HPA will deploy more pods to distribute the load.</p>","path":["Autoscaler"],"tags":[]},{"location":"kubernetes/autoscaler/#use-cases","level":3,"title":"Use Cases:","text":"<ul> <li>Scalable Web Applications: Ideal for handling varying traffic loads.</li> <li>Background Processing: Scaling workers in response to job queues.</li> </ul>","path":["Autoscaler"],"tags":[]},{"location":"kubernetes/autoscaler/#configuring-hpa","level":3,"title":"Configuring HPA:","text":"<p>HPA can be configured via the Kubernetes CLI <code>kubectl</code> by specifying the target metric and thresholds. For example, setting up HPA for a deployment to scale based on CPU usage involves creating an HPA resource linked to the deployment.</p>","path":["Autoscaler"],"tags":[]},{"location":"kubernetes/autoscaler/#vertical-pod-autoscaler-vpa","level":3,"title":"Vertical Pod Autoscaler (VPA)","text":"<p>Unlike HPA, the Vertical Pod Autoscaler adjusts the CPU and memory reservations of pods in a deployment, stateful set, or replica set. VPA aims to match the supply of resources to the demand as closely as possible, with minimal waste.</p>","path":["Autoscaler"],"tags":[]},{"location":"kubernetes/autoscaler/#how-vpa-works","level":3,"title":"How VPA Works:","text":"<p>VPA periodically adjusts the CPU and memory limits of pods to the optimal values based on usage history and patterns. This ensures that pods have enough resources to perform efficiently without hogging unnecessary resources.</p>","path":["Autoscaler"],"tags":[]},{"location":"kubernetes/autoscaler/#use-cases_1","level":3,"title":"Use Cases:","text":"<ul> <li>Resource-Intensive Applications: For applications whose resource requirements vary significantly over time.</li> <li>Optimizing Resource Utilization: Reducing waste in over-provisioned containers.</li> </ul>","path":["Autoscaler"],"tags":[]},{"location":"kubernetes/autoscaler/#configuring-vpa","level":3,"title":"Configuring VPA:","text":"<p>VPA is set up by applying a VPA resource to your Kubernetes cluster. This resource specifies the target deployment and the resource policy (auto, manual, or off).</p>","path":["Autoscaler"],"tags":[]},{"location":"kubernetes/autoscaler/#cluster-autoscaler","level":3,"title":"Cluster Autoscaler","text":"<p>The Cluster Autoscaler automatically adjusts the size of a Kubernetes cluster based on the demands of the workloads and the availability of resources.</p>","path":["Autoscaler"],"tags":[]},{"location":"kubernetes/autoscaler/#how-it-works","level":3,"title":"How It Works:","text":"<p>It increases the number of nodes during high demand and decreases the count when resources are underutilized, ensuring that your cluster is neither over-provisioned nor under-resourced.</p>","path":["Autoscaler"],"tags":[]},{"location":"kubernetes/autoscaler/#use-cases_2","level":3,"title":"Use Cases:","text":"<ul> <li>Dynamic Workloads: Perfect for clusters running jobs that vary greatly in resource demands.</li> <li>Cost Optimization: Ensures you‚Äôre only paying for the resources you need.</li> </ul>","path":["Autoscaler"],"tags":[]},{"location":"kubernetes/autoscaler/#configuring-cluster-autoscaler","level":3,"title":"Configuring Cluster Autoscaler:","text":"<p>Cluster Autoscaler is typically configured on the cloud provider side, requiring access to manage the underlying virtual machines or instances.</p>","path":["Autoscaler"],"tags":[]},{"location":"kubernetes/autoscaler/#combining-autoscalers-for-optimal-performance","level":3,"title":"Combining Autoscalers for Optimal Performance","text":"<p>In practice, HPA, VPA, and Cluster Autoscaler are often used in tandem to achieve both efficient resource utilization and responsive scaling. However, it‚Äôs crucial to understand their interactions:</p> <ul> <li>HPA and VPA: Should be used with caution together as they can conflict; for example, HPA might try to add more pods when VPA recommends increasing resources to existing pods.</li> <li>HPA and Cluster Autoscaler: Complement each other well, as HPA adjusts the number of pods and Cluster Autoscaler adjusts the number of nodes to accommodate the pods.</li> <li>VPA and Cluster Autoscaler: Can be used together to ensure pods have enough resources and that nodes are added or removed based on overall demand.</li> </ul>","path":["Autoscaler"],"tags":[]},{"location":"kubernetes/autoscaler/#conclusion","level":2,"title":"Conclusion","text":"<p>Kubernetes‚Äô autoscaling features, HPA, VPA, and Cluster Autoscaler, offer powerful tools for managing application scalability and resource efficiency. By understanding their mechanisms, use cases, and how to configure them, you can ensure your applications remain responsive and cost-effective under varying loads. As you adopt these autoscaling strategies, keep in mind the importance of monitoring and fine-tuning settings to match your specific workload requirements and achieve optimal performance in your Kubernetes environment.</p>","path":["Autoscaler"],"tags":[]},{"location":"kubernetes/cheatsheet/","level":1,"title":"Cheat sheet","text":"Voir les labels du deployment knative-operator<pre><code>kubectl get deployment knative-operator -n knative-operator --show-labels\n</code></pre> Voir les labels du deployment operator-webhook<pre><code>kubectl get deployment operator-webhook -n knative-operator --show-labels\n</code></pre> Voir les deux en m√™me temps avec leurs labels<pre><code>kubectl get deployments -n knative-operator --show-labels\n</code></pre> Plus de d√©tails en format YAML pour voir tous les labels<pre><code>kubectl get deployment knative-operator -n knative-operator -o yaml | grep -A 5 \"labels:\"\nkubectl get deployment operator-webhook -n knative-operator -o yaml | grep -A 5 \"labels:\"\n</code></pre> Format tableau avec les labels sp√©cifiques<pre><code>kubectl get deployments -n knative-operator \\\n  -o custom-columns=NAME:.metadata.name,APP:.metadata.labels.app,ENV:.metadata.labels.environment\n</code></pre> Voir si les labels app et environment existent<pre><code>kubectl get deployments -n knative-operator -o json | jq '.items[].metadata.labels | {name: .name, app: .app, environment: .environment}'\n</code></pre> Version plus simple pour voir tous les labels<pre><code>kubectl get deployments -n knative-operator -o yaml | yq '.items[].metadata | {\"name\": .name, \"labels\": .labels}'\n</code></pre>","path":["KUBERNETES","Cheat sheet"],"tags":["Kubernetes","CheatSheet"]},{"location":"kubernetes/containerRuntime/","level":1,"title":"Container Runtime","text":"<p>Kubernetes has become the go-to platform for container orchestration, and while most of us are comfortable deploying and managing applications on it, understanding the underlying magic can sometimes feel like deciphering hieroglyphs. One such concept is the container runtime. What the heck is it, and why should you care? Let‚Äôs break it down from the basics</p> <p>What is a Container runtime?</p>","path":["Container Runtime"],"tags":["Kubernetes"]},{"location":"kubernetes/containerRuntime/#what-is-a-container-runtime","level":3,"title":"What is a Container Runtime?","text":"<p>In simple terms, a container runtime is a piece of software responsible for running containers. It does the heavy lifting of pulling container images, starting and stopping containers, and managing the container lifecycle. Think of it as the engine under the hood of your Kubernetes car ‚Äî you might not see it directly, but without it, you‚Äôre not going anywhere.</p>","path":["Container Runtime"],"tags":["Kubernetes"]},{"location":"kubernetes/containerRuntime/#real-life-analogy","level":3,"title":"Real-Life Analogy","text":"<p>Imagine you‚Äôre at a fast-food restaurant. You place your order (the container image), and the chef (container runtime) prepares your meal and serves it on a tray (running container). You don‚Äôt really care what‚Äôs happening behind the counter, as long as your burger shows up, but without the chef, you‚Äôre just staring at an empty tray.</p>","path":["Container Runtime"],"tags":["Kubernetes"]},{"location":"kubernetes/containerRuntime/#why-do-we-need-container-runtimes","level":3,"title":"Why Do We Need Container Runtimes?","text":"<p>Container runtimes abstract away the complexity of managing isolated processes. Kubernetes uses container runtimes to keep workloads consistent, efficient, and portable. Different container runtimes can impact performance, compatibility, and security.</p>","path":["Container Runtime"],"tags":["Kubernetes"]},{"location":"kubernetes/containerRuntime/#popular-container-runtimes-in-kubernetes","level":3,"title":"Popular Container Runtimes in Kubernetes","text":"<p>Let‚Äôs take a look at some popular container runtimes and understand how they fit into the Kubernetes ecosystem.</p>","path":["Container Runtime"],"tags":["Kubernetes"]},{"location":"kubernetes/containerRuntime/#docker","level":3,"title":"Docker","text":"<p>The OG of container runtimes. It‚Äôs like that old, reliable car that‚Äôs been around forever. Kubernetes initially relied on Docker as its default runtime. However, as Kubernetes evolved, Docker was swapped out for something more streamlined (like ditching your old sedan for a sporty new coupe).</p>","path":["Container Runtime"],"tags":["Kubernetes"]},{"location":"kubernetes/containerRuntime/#containerd","level":3,"title":"containerd","text":"<p>containerd is the runtime that grew out of Docker itself and became the preferred runtime for Kubernetes. It‚Äôs leaner, faster, and focuses purely on running containers without the extra baggage Docker carried.</p>","path":["Container Runtime"],"tags":["Kubernetes"]},{"location":"kubernetes/containerRuntime/#cri-o","level":3,"title":"CRI-O","text":"<p>Designed explicitly for Kubernetes, CRI-O is a lightweight runtime compatible with Kubernetes‚Äô Container Runtime Interface (CRI). It‚Äôs like upgrading to a hybrid car ‚Äî efficient, purpose-built, and fully compatible with modern standards.</p>","path":["Container Runtime"],"tags":["Kubernetes"]},{"location":"kubernetes/containerRuntime/#how-kubernetes-uses-runtimes","level":3,"title":"How Kubernetes Uses Runtimes?","text":"<p>Kubernetes interacts with container runtimes via the Container Runtime Interface (CRI). This allows Kubernetes to work with different runtimes without being tied to a specific one. The kubelet on each node communicates with the runtime to manage the containers.</p>","path":["Container Runtime"],"tags":["Kubernetes"]},{"location":"kubernetes/containerRuntime/#choosing-the-right-runtime","level":3,"title":"Choosing the Right Runtime","text":"<p>When choosing a container runtime, consider factors like performance, security, compatibility, and community support. While containerd is often the default, CRI-O‚Äôs Kubernetes-centric design makes it a great choice for pure K8s environments, and gVisor and Kata are better for security-focused setups.</p>","path":["Container Runtime"],"tags":["Kubernetes"]},{"location":"kubernetes/containerRuntime/#conclusion","level":3,"title":"Conclusion","text":"<p>Container runtimes might not be the flashy part of Kubernetes, but they‚Äôre essential to making your workloads actually run. Think of them as the unsung heroes keeping your clusters humming. Next time you‚Äôre digging into your Kubernetes setup, give a little nod of respect to the container runtime doing the heavy lifting.</p>","path":["Container Runtime"],"tags":["Kubernetes"]},{"location":"kubernetes/containerRuntimeInterface/","level":1,"title":"Container Runtime Interface","text":"<p>So, you‚Äôve wrapped your head around container runtimes and how they‚Äôre the unsung heroes of Kubernetes. Great! But wait ‚Äî how exactly does Kubernetes talk to these runtimes? Enter the Container Runtime Interface (CRI), the Kubernetes equivalent of a universal remote control. In this blog, we‚Äôll break down the CRI from basics to intermediate##</p>","path":["Container Runtime Interface"],"tags":["Kubernetes"]},{"location":"kubernetes/containerRuntimeInterface/#what-is-the-container-runtime-interface-cri","level":3,"title":"What Is the Container Runtime Interface (CRI)?","text":"<p>The CRI is an API that Kubernetes uses to communicate with container runtimes. It‚Äôs like the translator between Kubernetes (speaking in orchestration) and container runtimes (speaking in execution). Before the CRI, Kubernetes directly integrated with Docker, which was fine until people wanted to use other runtimes. CRI makes Kubernetes more flexible by allowing it to work with multiple runtimes without a hitch.</p> <p>What is CRI?</p>","path":["Container Runtime Interface"],"tags":["Kubernetes"]},{"location":"kubernetes/containerRuntimeInterface/#real-life-analogy","level":3,"title":"Real-Life Analogy","text":"<p>Think of CRI as a universal power adapter when traveling. You have your laptop (Kubernetes) that needs to charge, but every country (container runtime) has a different plug. Instead of buying a new charger every time, you use an adapter (CRI) that lets your laptop connect to any outlet. Convenient and way less of a headache.</p>","path":["Container Runtime Interface"],"tags":["Kubernetes"]},{"location":"kubernetes/containerRuntimeInterface/#why-was-cri-introduced","level":3,"title":"Why Was CRI Introduced?","text":"<p>Initially, Kubernetes was tightly coupled with Docker. This worked well, but as other runtimes like containerd and CRI-O gained popularity, it became clear that Kubernetes needed a more modular approach. By introducing the CRI, Kubernetes could plug into any compatible runtime without rewriting the entire engine.</p>","path":["Container Runtime Interface"],"tags":["Kubernetes"]},{"location":"kubernetes/containerRuntimeInterface/#benefits-of-cri","level":3,"title":"Benefits of CRI:","text":"<ol> <li>Flexibility: Kubernetes can switch between different runtimes without major rewrites.</li> <li>Efficiency: Runtimes can be specialized and optimized without affecting Kubernetes.</li> <li>Modularity: Developers can experiment with new runtimes while still leveraging Kubernetes.</li> </ol>","path":["Container Runtime Interface"],"tags":["Kubernetes"]},{"location":"kubernetes/containerRuntimeInterface/#how-does-cri-work","level":3,"title":"How Does CRI Work?","text":"<p>The CRI has two primary components:</p> <ol> <li>Image Service API: Handles pulling, listing, and managing container images.</li> <li>Runtime Service API: Manages the lifecycle of the containers themselves (start, stop, delete, etc.).</li> </ol> <p>The kubelet communicates with the container runtime through these APIs. The runtime itself needs to implement the CRI, ensuring compatibility with Kubernetes without worrying about specific runtime details.</p>","path":["Container Runtime Interface"],"tags":["Kubernetes"]},{"location":"kubernetes/containerRuntimeInterface/#cri-implementations","level":3,"title":"CRI Implementations","text":"<p>Now that you get the concept, let‚Äôs look at a few popular CRI implementations:</p>","path":["Container Runtime Interface"],"tags":["Kubernetes"]},{"location":"kubernetes/containerRuntimeInterface/#containerd","level":3,"title":"containerd","text":"<p>containerd is a core component of Docker and is designed to be CRI-compliant. It‚Äôs like the workhorse that got promoted to the manager‚Äôs seat after Kubernetes ditched Docker.</p>","path":["Container Runtime Interface"],"tags":["Kubernetes"]},{"location":"kubernetes/containerRuntimeInterface/#cri-o","level":3,"title":"CRI-O","text":"<p>This lightweight runtime is tailor-made for Kubernetes. It‚Äôs like buying a suit that‚Äôs custom-fitted rather than off the rack. CRI-O is efficient, lean, and follows Kubernetes standards to the letter.</p>","path":["Container Runtime Interface"],"tags":["Kubernetes"]},{"location":"kubernetes/containerRuntimeInterface/#other-implementations","level":3,"title":"Other Implementations","text":"<p>There are a few others, like Mirantis Container Runtime and the gVisor CRI plugin, which add security or niche features. But the principle is the same: conform to the CRI to keep Kubernetes happy.</p>","path":["Container Runtime Interface"],"tags":["Kubernetes"]},{"location":"kubernetes/containerRuntimeInterface/#debugging-cri-issues","level":3,"title":"Debugging CRI Issues","text":"<p>You know how it goes ‚Äî things work great until they don‚Äôt. CRI issues can crop up when the runtime doesn‚Äôt properly communicate with the kubelet. Common problems include version mismatches and compatibility issues.</p>","path":["Container Runtime Interface"],"tags":["Kubernetes"]},{"location":"kubernetes/containerRuntimeInterface/#quick-debug-tip","level":3,"title":"Quick Debug Tip:","text":"<ul> <li>Check the kubelet logs: <code>journalctl -u kubelet</code> to see if it‚Äôs complaining about CRI connectivity.</li> <li>Make sure your CRI plugin is properly installed and configured.</li> <li>Restart the kubelet after making changes to see if issues resolve.</li> </ul>","path":["Container Runtime Interface"],"tags":["Kubernetes"]},{"location":"kubernetes/containerRuntimeInterface/#conclusion","level":3,"title":"Conclusion","text":"<p>The Container Runtime Interface is like Kubernetes‚Äô universal translator, allowing it to work seamlessly with different runtimes. Whether you‚Äôre team containerd or team CRI-O, the CRI ensures that Kubernetes doesn‚Äôt play favorites. Next time your cluster‚Äôs humming along happily, just know that the CRI is doing its thing quietly behind the scenes.</p>","path":["Container Runtime Interface"],"tags":["Kubernetes"]},{"location":"kubernetes/cronJob/","level":1,"title":"Kubernetes CronJob","text":"<p>In our previous article, we explored the world of Kubernetes Jobs and how they enable us to run one-off tasks within our containerized applications. However, one critical aspect of managing containerized workloads that we intentionally skipped for a more in-depth discussion is Kubernetes CronJobs. In this article, we‚Äôll delve deeper into CronJobs, the scheduling powerhouse of Kubernetes, and discover how they allow you to automate and orchestrate recurring tasks with precision and ease.</p> <p>Kubernetes Cron jobs</p>","path":["Kubernetes CronJob"],"tags":["kubernetes"]},{"location":"kubernetes/cronJob/#what-are-kubernetes-cronjobs","level":3,"title":"What Are Kubernetes CronJobs?","text":"<p>In the world of Kubernetes, a CronJob is a resource type that defines a time-based job. It enables you to run tasks periodically at specified intervals, just like the traditional cron utility on Linux systems. With CronJobs, you can automate various tasks within your containerized applications, such as database backups, log rotation, or data syncing.</p>","path":["Kubernetes CronJob"],"tags":["kubernetes"]},{"location":"kubernetes/cronJob/#enabling-scheduled-tasks","level":3,"title":"Enabling Scheduled Tasks","text":"<p>CronJobs offer several advantages for managing scheduled tasks in Kubernetes:</p> <p>Automation: CronJobs automate repetitive tasks, reducing the need for manual intervention and human error.</p> <p>Scalability: Kubernetes scales CronJobs according to your cluster‚Äôs capacity, ensuring tasks are executed efficiently.</p> <p>Flexibility: You can define the schedule for tasks down to the minute, making it easy to customize when and how often jobs run.</p>","path":["Kubernetes CronJob"],"tags":["kubernetes"]},{"location":"kubernetes/cronJob/#jobs-vs-cron-jobs","level":3,"title":"Jobs Vs Cron Jobs","text":"<p>Kubernetes Jobs are designed for running one-time, batch-style tasks. They execute a task to completion once and then terminate, making them suitable for operations that should occur only once, such as a database migration or a data import task. On the other hand, CronJobs excel at automating recurring tasks. They repeatedly run jobs based on a predefined schedule, ensuring that periodic operations, like regular data backups, log rotation, or daily reports, are executed consistently and on time.</p> <p>Now, let‚Äôs dive into creating and managing CronJobs in Kubernetes.</p>","path":["Kubernetes CronJob"],"tags":["kubernetes"]},{"location":"kubernetes/cronJob/#creating-a-kubernetes-cronjob","level":3,"title":"Creating a Kubernetes CronJob","text":"<p>Creating a CronJob in Kubernetes involves defining a manifest file, which is written in YAML format. Here‚Äôs a step-by-step guide to creating a basic CronJob:</p> <p>Define a YAML manifest: Create a YAML file, e.g., my-cronjob.yaml, and specify the CronJob‚Äôs details, including the schedule and the container image to run. Here‚Äôs a minimal example:</p> <pre><code>apiVersion: batch/v1\nkind: CronJob\nmetadata:\n name: my-cronjob\nspec:\n schedule: ‚Äú*/5 * * * *‚Äù\n jobTemplate:\n spec:\n template:\n spec:\n containers:\n ‚Äî name: my-cronjob-container\n image: my-image:latest\n</code></pre> <p>In this example, the CronJob runs every 5 minutes, using the my-image:latest container.</p> <p>Apply the manifest: Use the kubectl apply command to create the CronJob resource:</p> <pre><code>kubectl apply -f my-cronjob.yaml\n</code></pre> <p>Verify the CronJob: You can check the status and details of your CronJob with the following command:</p> <pre><code>kubectl get cronjob my-cronjob\n</code></pre>","path":["Kubernetes CronJob"],"tags":["kubernetes"]},{"location":"kubernetes/cronJob/#managing-kubernetes-cronjobs","level":3,"title":"Managing Kubernetes CronJobs","text":"<p>Once you‚Äôve created a CronJob, you can manage it using various Kubernetes commands.</p> <p>Here are some common operations:</p> <p>List CronJobs: To see all the CronJobs in your cluster, use the kubectl get cronjobs command.</p> <p>Inspect CronJob: To view the details of a specific CronJob, use kubectl describe cronjob . <p>Pause and Resume CronJobs: You can temporarily stop a CronJob with kubectl suspend cronjob  and resume it with kubectl resume cronjob . <p>Manually Run CronJob: To manually run a CronJob outside its schedule, use kubectl create job ‚Äî from=cronjob/ .","path":["Kubernetes CronJob"],"tags":["kubernetes"]},{"location":"kubernetes/cronJob/#conclusion","level":3,"title":"Conclusion","text":"<p>Kubernetes CronJobs are a powerful tool for automating and managing scheduled tasks within your containerized applications. By defining the schedule and job details in a simple YAML manifest, you can easily create, deploy, and manage CronJobs in your Kubernetes cluster. Whether it‚Äôs performing backups, generating reports, or cleaning up resources, CronJobs can streamline your workflow and enhance the reliability of your applications in a Kubernetes environment.</p>","path":["Kubernetes CronJob"],"tags":["kubernetes"]},{"location":"kubernetes/customResourceDefinitions/","level":1,"title":"Custom Resource Definitions","text":"<p>With great infrastructure we need a great level of customisation, this is where Kubernetes Custom Resource Definitions (CRDs) come into play, offering a powerful way to extend Kubernetes capabilities beyond its default set of resources. In this article, we‚Äôll learn about CRDs, breaking down everything you need to know into simple, plain English.</p> <p>Everything You Need to Know About Custom Resource Definitions in Kubernetes</p>","path":["Custom Resource Definitions"],"tags":["Kubernetes","Custom Resource Definitions"]},{"location":"kubernetes/customResourceDefinitions/#what-is-a-kubernetes-crd","level":3,"title":"What is a Kubernetes CRD?","text":"<p>At its core, a CRD is a way to create your own, custom resources in Kubernetes. Imagine you‚Äôre using LEGO bricks to build models. Each LEGO set comes with standard bricks, but what if you need a special piece that‚Äôs not included? CRDs are like creating that special LEGO piece, allowing you to define new resources that work seamlessly within the Kubernetes ecosystem.</p> <p>A Custom Resource Definition enables you to define a new kind of resource, such as <code>VPCs</code>, <code>Firewalls</code>, or even <code>TodoLists</code>, that Kubernetes doesn't offer out of the box. This new resource can then be used to manage your application's specific needs, making Kubernetes even more flexible and powerful.</p>","path":["Custom Resource Definitions"],"tags":["Kubernetes","Custom Resource Definitions"]},{"location":"kubernetes/customResourceDefinitions/#why-use-crds","level":3,"title":"Why Use CRDs?","text":"<p>CRDs allow for the extension of Kubernetes capabilities in ways that were previously not possible with the default set of resources. They offer several benefits:</p> <ul> <li>Customization: Tailor Kubernetes to your specific application needs.</li> <li>Automation: Automate the management of complex services.</li> <li>Integration: Seamlessly integrate external services into your Kubernetes cluster.</li> </ul>","path":["Custom Resource Definitions"],"tags":["Kubernetes","Custom Resource Definitions"]},{"location":"kubernetes/customResourceDefinitions/#how-do-crds-work","level":3,"title":"How Do CRDs Work?","text":"<p>CRDs work by defining a new resource type in your Kubernetes cluster. This process involves creating a YAML file that specifies the new resource‚Äôs attributes and behavior. Once applied to your cluster, the Kubernetes API server recognizes this new resource type and allows you to create, update, delete, and manage instances of it, just like any standard Kubernetes resource.</p>","path":["Custom Resource Definitions"],"tags":["Kubernetes","Custom Resource Definitions"]},{"location":"kubernetes/customResourceDefinitions/#the-crd-lifecycle","level":3,"title":"The CRD Lifecycle","text":"<ol> <li>Define: You start by defining your CRD, specifying its name, version, scope, and the structure of its spec (the desired state of your resource).</li> <li>Deploy: Apply the CRD to your Kubernetes cluster using <code>kubectl apply</code>. This tells the Kubernetes API server about the new resource.</li> <li>Create Custom Resources: With the CRD in place, you can now create custom resources based on it. These custom resources are instances of your CRD, representing the actual objects you want to manage in your cluster.</li> <li>Manage: Use <code>kubectl</code> to manage your custom resources just like you would with built-in resources. You can create, read, update, and delete custom resources, integrating them into your applications and workflows.</li> </ol>","path":["Custom Resource Definitions"],"tags":["Kubernetes","Custom Resource Definitions"]},{"location":"kubernetes/customResourceDefinitions/#creating-a-simple-crd-an-example","level":3,"title":"Creating a Simple CRD: An Example","text":"<p>Let‚Äôs create a simple CRD for a <code>TodoList</code>. This example will help illustrate the process of defining and using a custom resource in Kubernetes.</p> <p>Define the CRD: First, we need to define what a <code>TodoList</code> looks like in YAML format. This definition includes metadata like the name and group of our CRD, and the schema that describes the structure of each <code>TodoList</code> resource.</p> <pre><code>apiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  name: todolists.mydomain.com\nspec:\n  group: mydomain.com\n  versions:\n    - name: v1\n      served: true\n      storage: true\n      schema:\n        openAPIV3Schema:\n          type: object\n          properties:\n            spec:\n              type: object\n              properties:\n                items:\n                  type: array\n                  items:\n                    type: string\n  scope: Namespaced\n  names:\n    plural: todolists\n    singular: todolist\n    kind: TodoList\n    shortNames:\n      - tl\n</code></pre> <p>Deploy the CRD: Apply this definition to your cluster using <code>kubectl apply -f todolist-crd.yaml</code>. This step registers the <code>TodoList</code> custom resource within your Kubernetes cluster.</p> <p>Create a <code>**TodoList**</code> Instance: Now that the CRD is in place, you can create instances of <code>TodoList</code>. Here's an example of a <code>TodoList</code> resource:</p> <pre><code>apiVersion: mydomain.com/v1\nkind: TodoList\nmetadata:\n  name: my-first-todolist\nspec:\n  items:\n    - Learn Kubernetes CRDs\n    - Write a blog post\n</code></pre> <p>Apply this using <code>kubectl apply -f my-todolist.yaml</code> to create your <code>TodoList</code> in Kubernetes.</p> <p>Manage Your <code>TodoList</code>: You can use <code>kubectl</code> commands to manage your <code>TodoList</code> instances, such as viewing, editing, and deleting them.</p>","path":["Custom Resource Definitions"],"tags":["Kubernetes","Custom Resource Definitions"]},{"location":"kubernetes/customResourceDefinitions/#best-practices-for-using-crds","level":3,"title":"Best Practices for Using CRDs","text":"<p>When working with CRDs, it‚Äôs essential to follow best practices to ensure your Kubernetes cluster remains efficient, secure, and easy to manage:</p> <ul> <li>Validation: Use OpenAPI validation schemas to enforce the structure and validity of your custom resources.</li> <li>Versioning: Properly version your CRDs to manage changes and upgrades smoothly.</li> <li>Namespaces: Consider whether your custom resources should be namespaced or cluster-wide and define them accordingly.</li> <li>Documentation: Document your CRDs and custom resources thoroughly for users and future maintainers.</li> </ul> <p>Whether you‚Äôre automating complex workflows or integrating external services, CRDs offer the flexibility and power to make it happen. Remember, the key to success with CRDs lies in thoughtful definition, careful management, and adherence to best practices. Stay tuned and don‚Äôt forget to follow</p>","path":["Custom Resource Definitions"],"tags":["Kubernetes","Custom Resource Definitions"]},{"location":"kubernetes/deployments/","level":1,"title":"Deployments","text":"<p>In our exploration of Kubernetes, we‚Äôve already gained insights into pods and services, understanding their roles in container orchestration and communication. Now, it‚Äôs time to take another significant step in our Kubernetes journey by introducing Kubernetes Deployments. These powerful abstractions make the management of application versions and scaling a breeze. In this blog, we‚Äôll explore what Kubernetes Deployments are, how they ensure the desired number of pod replicas, and how they help with updates and rollbacks.</p>","path":["KUBERNETES","RESOURCES","Deployments"],"tags":["Kubernetes"]},{"location":"kubernetes/deployments/#what-are-kubernetes-deployments","level":3,"title":"What are Kubernetes Deployments?","text":"<p>Kubernetes Deployments are a declarative way to define and manage applications in your Kubernetes cluster. Think of them as a higher-level construct that abstracts away the nitty-gritty details of creating and managing individual pods. Deployments provide a higher level of control and flexibility, enabling you to effortlessly manage your application‚Äôs lifecycle.</p>","path":["KUBERNETES","RESOURCES","Deployments"],"tags":["Kubernetes"]},{"location":"kubernetes/deployments/#ensuring-the-desired-number-of-pod-replicas","level":3,"title":"Ensuring the Desired Number of Pod Replicas","text":"<p>One of the key functions of Kubernetes Deployments is to ensure that a specified number of pod replicas are always running and available. This desired state is declared in the Deployment‚Äôs configuration, and Kubernetes takes care of maintaining it. Here‚Äôs how it works:</p> <p>Desired State Declaration: You specify the desired number of replicas for your application in the Deployment configuration. For example, you might want three replicas of your web application to ensure redundancy and scalability.</p> <p>Control Loop: Kubernetes continuously monitors the actual state of your application and compares it to the desired state declared in the Deployment. If there are fewer pods than desired, Kubernetes automatically creates new ones. If there are more pods than specified, Kubernetes gracefully scales down by terminating excess pods.</p> <p>Rolling Updates and Rollbacks: Deployments also manage updates and rollbacks seamlessly. When you need to update your application, you can make changes to the Deployment configuration. Kubernetes orchestrates the process of gradually replacing old pods with new ones, ensuring zero downtime during updates. In case of issues or unexpected behaviour with the new version, you can easily roll back to the previous version with a simple command.</p>","path":["KUBERNETES","RESOURCES","Deployments"],"tags":["Kubernetes"]},{"location":"kubernetes/deployments/#handling-updates-and-rollbacks","level":3,"title":"Handling Updates and Rollbacks","text":"<p>Kubernetes Deployments shine when it comes to managing updates and rollbacks:</p> <p>1. Updating Your Application</p> <p>When it‚Äôs time to release a new version of your application, you can update the Deployment configuration with the new image or desired changes. Kubernetes will perform a rolling update, ensuring that your application remains available throughout the process. This not only reduces the risk of service interruptions but also allows you to monitor the rollout and intervene if any issues arise.</p> <p>2. Rollbacks</p> <p>Sometimes, an update might introduce unexpected problems. Kubernetes makes it easy to roll back to a previous, stable version of your application. You can simply specify the desired revision in the Deployment configuration, and Kubernetes will gracefully roll back to that state, avoiding service disruptions.</p> <p>Kubernetes Deployments simplify the management of application versions and scaling, making it easier for you to maintain your containerized applications. By defining your application‚Äôs desired state and letting Kubernetes handle the details, you can focus on building and improving your application while ensuring high availability, scalability, and robust update and rollback procedures. As you continue your Kubernetes journey, remember that Deployments are your allies in achieving seamless application management.</p>","path":["KUBERNETES","RESOURCES","Deployments"],"tags":["Kubernetes"]},{"location":"kubernetes/etcd/","level":1,"title":"etcd","text":"<p>Etcd is a distributed key-value store that reliably stores the data for a distributed system. In simpler terms, you can think of etcd as a highly reliable database that keeps track of all the important information your Kubernetes cluster needs to function. Imagine a well-organized filing cabinet where every piece of important data is stored in a specific drawer, making it easy to find and retrieve when needed.</p>","path":["etcd"],"tags":["Kubernetes","etcd"]},{"location":"kubernetes/etcd/#how-does-etcd-work","level":3,"title":"How Does Etcd Work?","text":"<p>Etcd works by storing data in key-value pairs. This means each piece of information (value) is associated with a unique identifier (key). For example, think of a library where each book (value) has a specific call number (key). When you need a book, you use the call number to find it quickly.</p> <p>Etcd ensures that this data is consistently replicated across multiple nodes (computers) in a cluster, so if one node fails, another can take over without losing any data. This is like having multiple copies of your library‚Äôs catalog in different locations, ensuring you can always find the book you need, even if one catalog is lost.</p>","path":["etcd"],"tags":["Kubernetes","etcd"]},{"location":"kubernetes/etcd/#etcd-architecture","level":3,"title":"Etcd Architecture","text":"<p>Etcd‚Äôs architecture is designed to be both simple and powerful. It consists of several key components:</p> <p></p> <p>Image credits: Internet</p> <ol> <li>Cluster: A group of etcd nodes working together. Think of it as a team of librarians managing the same library catalog.</li> <li>Node: An individual instance of etcd. Each librarian in the team represents a node.</li> <li>Leader: The node that handles all the write operations. The head librarian who updates the catalog.</li> <li>Follower: Nodes that replicate the data from the leader. The assistant librarians who keep their copies of the catalog up-to-date.</li> </ol>","path":["etcd"],"tags":["Kubernetes","etcd"]},{"location":"kubernetes/etcd/#leader-follower-concept","level":2,"title":"Leader-Follower Concept","text":"<p>Etcd follows a leader-follower model to ensure consistency. Here‚Äôs how it works:</p> <ol> <li>Leader Election: Among all the nodes, one is elected as the leader. This is like choosing a head librarian who will manage all updates to the catalog.</li> <li>Write Operations: All write requests (changes to data) go to the leader. The head librarian is responsible for making any updates to the catalog.</li> <li>Replication: The leader then replicates these changes to all the follower nodes. The head librarian tells all the assistant librarians about the changes, so they update their catalogs accordingly.</li> <li>Read Operations: Read requests can be handled by any node. You can ask any librarian to find a book using the catalog.</li> </ol> <p>This ensures that even if the leader node fails, the system can elect a new leader from the followers, and no data is lost.</p>","path":["etcd"],"tags":["Kubernetes","etcd"]},{"location":"kubernetes/etcd/#steps-to-get-started-with-etcd","level":3,"title":"Steps to Get Started with Etcd","text":"<p>Getting started with etcd involves a few key steps:</p> <ol> <li>Install Etcd: Download and install etcd on your system. You can find installation instructions in the official etcd documentation.</li> <li>Start Etcd: Start an etcd instance by running the etcd binary.</li> <li>Join Nodes to the Cluster: If you‚Äôre setting up a cluster, start additional etcd instances and join them to the cluster using the appropriate commands.</li> <li>Configure Kubernetes: Point your Kubernetes cluster to use your etcd cluster by updating the configuration files.</li> </ol>","path":["etcd"],"tags":["Kubernetes","etcd"]},{"location":"kubernetes/etcd/#alternatives-to-etcd","level":3,"title":"Alternatives to Etcd","text":"<p>While etcd is widely used in Kubernetes, there are other distributed key-value stores that can serve as alternatives:</p> <ol> <li>Consul: Consul is a distributed, highly available system for service discovery and configuration. It provides similar key-value storage capabilities along with additional features like service health checking.</li> <li>ZooKeeper: Apache ZooKeeper is another distributed coordination service that can be used for configuration management, synchronization, and naming. It‚Äôs known for its strong consistency guarantees.</li> <li>Redis: Redis is an in-memory data structure store used as a database, cache, and message broker. While not typically used as a replacement for etcd in Kubernetes, it can be used for similar key-value storage needs in other contexts.</li> </ol> <p>Each of these alternatives has its own strengths and use cases, so the best choice depends on your specific requirements.</p> <p>In summary, etcd is a critical component of Kubernetes that ensures the reliable storage and replication of data across the cluster</p>","path":["etcd"],"tags":["Kubernetes","etcd"]},{"location":"kubernetes/home/","level":1,"title":"Home","text":"<ul> <li> Container Runtime</li> <li> Container Runtime Interface</li> </ul> <p>Introduction:</p> <ul> <li>Course Introduction</li> <li>Certification</li> </ul> <p>Core Concepts:</p> <ul> <li>Core Concepts Section Introduction</li> <li>Cluster Architecture</li> <li>Docker vs ContainerD</li> <li>A note on Docker Deprecation</li> <li>ETCD for Beginners</li> <li> ETCD in Kubernetes</li> <li>Kube API Server</li> <li>Kube Controller Manager</li> <li> Kube Scheduler</li> <li> Kubelet</li> <li> Kube Proxy</li> <li> Pods</li> <li>Pods with YAML</li> <li>Demo Pods with YAML</li> <li>Practice Test Introduction</li> <li>Solution Pods optional</li> <li>ReplicaSets</li> <li>Solution ReplicaSets optional</li> <li> Deployments</li> <li>Solution Deploymentoptional</li> <li> Services</li> <li> Services Cluster IP</li> <li>Services Loadbalancer</li> <li>Solution Services optional</li> <li>Namespaces</li> <li>Solution Namespaces optional</li> <li>Imperative vs Declarative</li> <li>Solution Imperative Commands optional</li> <li>Kubectl Apply Command</li> <li>A Quick Reminder</li> </ul> <p>Scheduling:</p> <ul> <li>Scheduling Section Introduction</li> <li>Manual Scheduling</li> <li>Solution Manual Scheduling optional</li> <li>Labels and Selectors</li> <li>Solution Labels and Selectors</li> <li>Taints and Tolerations</li> <li>Solution Taints and Toleration Optional</li> <li>Node Selectors</li> <li>Node Affinity</li> <li>Solution Node Affinity Optional</li> <li>Taints and Tolerations vs Node Affinity</li> <li> Resource Limits</li> <li>Solution Resource Limits</li> <li>DaemonSets</li> <li>Solution DaemonSets optional</li> <li>Static Pods</li> <li>Solution Static Pods Optional</li> <li>Priority Classes</li> <li>Multiple Schedulers</li> <li>Solution Multiple Scheduler</li> <li>Configuring Scheduler Profiles</li> <li>Admission Controllers 2025 Updates</li> <li>Solution Admission Controllers 2025 Updates</li> <li>Validating and Mutating Admission Controllers 2025 Updates</li> <li>Solution Validating and Mutating Admission Controllers 2025 Updates</li> </ul> <p>Logging Monitoring:</p> <ul> <li>Logging and Monitoring Section Introduction</li> <li>Monitor Cluster Components</li> <li>Solution Monitor Cluster Components</li> <li>Managing Application Logs</li> <li>Solution Logging Optional</li> </ul> <p>Application Lifecycle Management:</p> <ul> <li>Application Lifecycle Management Section Introduction</li> <li>Rolling Updates and Rollbacks</li> <li>Solution Rolling update</li> <li>Commands and Arguments in Docker</li> <li>Commands and Arguments in Kubernetes</li> <li>Solution Commands and Arguments Optional</li> <li>Configure Environment Variables in Applications</li> <li>Configure ConfigMaps in Applications</li> <li>Solution Env Variables Optional</li> <li>Secrets</li> <li>Solution Secrets Optional</li> <li>Demo Encrypting Secret Data at Rest</li> <li>Multi Container Pods</li> <li>Solution Multi Container Pods Optional</li> <li>Solution Init Containers Optional</li> <li>Introduction to Autoscaling 2025 Updates</li> <li>Horizontal Pod Autoscaler HPA 2025 Updates</li> <li>In place Resize of Pods 2025 Updates</li> <li>Vertical Pod Autoscaling VPA 2025 Updates</li> </ul> <p>Cluster Maintenance:</p> <ul> <li>Cluster Maintenance Section Introduction</li> <li>OS Upgrades</li> <li>Solution OS Upgrades optional</li> <li>Kubernetes Software Versions</li> <li>Cluster Upgrade Introduction</li> <li>Demo Cluster upgrade</li> <li>Solution Cluster Upgrade Process</li> <li>Backup and Restore Methods</li> </ul> <p>Security:</p> <ul> <li>Security Section Introduction</li> <li>Kubernetes Security Primitives</li> <li>Authentication</li> <li>TLS Introduction</li> <li>TLS Basics</li> <li>TLS in Kubernetes</li> <li>TLS in Kubernetes Certificate Creation</li> <li>View Certificate Details</li> <li>Solution View Certification Details</li> <li>Certificates API</li> <li>Solution Certificates API</li> <li>KubeConfig</li> <li>Solution KubeConfig</li> <li>API Groups</li> <li>Authorization</li> <li>Role Based Access Controls</li> <li>Solution Role Based Access Controls</li> <li>Cluster Roles</li> <li>Solution Cluster Roles</li> <li>Service Accounts</li> <li>Solution Service Accounts</li> <li>Image Security</li> <li>Solution Image Security</li> <li>Pre requisite Security in Docker</li> <li>Security Contexts</li> <li>Solution Security Contexts</li> <li>Network Policies</li> <li>Developing network policies</li> <li>Solution Network Policies optional</li> <li> Custom Resource Definition CRD</li> <li>Custom Controllers 2025 Updates</li> <li>Operator Framework 2025 Updates</li> </ul> <p>Storage:</p> <ul> <li>Storage Section Introduction</li> <li>Introduction to Docker Storage</li> <li>Storage in Docker</li> <li>Volume Driver Plugins in Docker</li> <li>Container Storage Interface</li> <li>Volumes</li> <li>Persistent Volumes</li> <li>Persistent Volume Claims</li> <li>Solution Persistent Volumes and Persistent Volume Claims optional</li> <li>Storage Class</li> <li>Solution Storage Class</li> </ul> <p>Networking:</p> <ul> <li>Networking Introduction</li> <li>Prerequisite Switching Routing Gateways CNI in kubernetes</li> <li>Prerequisite DNS</li> <li>Prerequisite Network Namespaces</li> <li>Prerequisite Docker Networking</li> <li>Prerequisite CNI</li> <li>Cluster Networking</li> <li>Solution Explore Environment optional</li> <li>Pod Networking</li> <li>CNI in kubernetes</li> <li>CNI weave</li> <li>Solution Explore CNI optional</li> <li>ipam weave</li> <li>Service Networking</li> <li>Solution Service Networking optional</li> <li>DNS in kubernetes</li> <li>CoreDNS in Kubernetes</li> <li>Solution Explore DNS optional</li> <li>Ingress</li> <li>Solution Ingress Networking 1 optional</li> <li>Solution Ingress Networking 2 optional</li> <li>Introduction to Gateway API 2025 Updates</li> </ul> <p>Design and Install a Kubernetes Cluster:</p> <ul> <li>Design a Kubernetes Cluster</li> <li>Choosing Kubernetes Infrastructure</li> <li>Configure High Availability</li> <li>ETCD in HA</li> </ul> <p>Install Kubernetes the kubeadm way:</p> <ul> <li>Introduction to Deployment with kubeadm</li> <li>Deploy with Kubeadm Provision VMs with Vagrant</li> <li>Demo Deployment with Kubeadm</li> <li>Solution Install a Kubernetes Cluster using kubeadm</li> </ul> <p>Troubleshooting:</p> <ul> <li>Troubleshooting Section Introduction</li> <li>Application Failure</li> <li>Solution Application Failure</li> <li>Control Plane Failure</li> <li>Solution Control Plane Failure</li> <li>Worker Node Failure</li> <li>Solutions Worker Node Failure</li> </ul> <p>Other Topics:</p> <ul> <li>Advanced Kubectl Commands</li> </ul> <p>Mock Exams:</p> <ul> <li>Mock Exam 1 Step by Step Solutions</li> <li>Mock Exam 2 Step by Step Solutions</li> <li>Mock Exam 3 Step by Step Solutions</li> <li>Whats Next</li> </ul> <p>Helm Basics 2025 Updates:</p> <ul> <li>What is Helm</li> <li>Installation and configuration</li> <li>A quick note about Helm2 vs Helm3</li> <li>Helm Components</li> <li>Helm charts</li> <li>Working with Helm basics</li> <li>Customizing chart parameters</li> <li>Lifecycle management with Helm</li> </ul> <p>Kustomize Basics 2025 Updates:</p> <ul> <li>Kustomize Problem Statement idealogy</li> <li>Kustomize vs Helm</li> <li>InstallationSetup</li> <li>kustomization</li> <li>Kustomize Output</li> <li>Kustomize ApiVersion Kind</li> <li>Managing Directories</li> <li>Managing Directories Demo</li> <li>Common Transformers</li> <li>Image Transformers</li> <li>Transformers Demo</li> <li>Patches Intro</li> <li>Different Types of Patches</li> <li>Patches Dictionary</li> <li>Patches list</li> <li>Overlays</li> <li>Components</li> </ul>","path":["KUBERNETES","Home"],"tags":["Kubernetes"]},{"location":"kubernetes/kube-proxy/","level":1,"title":"Kube-Proxy","text":"<p>One of the key challenges in a Kubernetes cluster is enabling communication between containers, or ‚Äúpods,‚Äù as they are called in Kubernetes. This is where Kubernetes Kube-Proxy comes into play. In this article, we‚Äôll demystify Kubernetes Kube-Proxy, explaining its role in Kubernetes networking, how it works, and its importance in enabling communication between pods.</p>","path":["KUBERNETES","CORE","Kube-Proxy"],"tags":["Kubernetes"]},{"location":"kubernetes/kube-proxy/#understanding-kube-proxy","level":3,"title":"Understanding Kube-proxy","text":"<p>Imagine a Kubernetes cluster as a bustling city with multiple buildings, and each building represents a worker node, which is a machine in the cluster. Now, inside these buildings, there are apartments, and each apartment represents a pod containing your containerized application. For the residents of these apartments (pods) to communicate with each other or with the outside world, they need someone to help them navigate through the city (cluster). This ‚Äúsomeone‚Äù is Kubernetes Kube-Proxy.</p> <p>Kube-Proxy serves as the traffic cop of the Kubernetes city. Its primary role is to facilitate network communication between pods, ensuring that they can find each other within the cluster and connect to the services they require. It‚Äôs like the postal service in our city analogy, ensuring that letters (data) get delivered to the right apartment (pod) efficiently.</p>","path":["KUBERNETES","CORE","Kube-Proxy"],"tags":["Kubernetes"]},{"location":"kubernetes/kube-proxy/#how-kube-proxy-works","level":3,"title":"How Kube-Proxy Works?","text":"<p>Now that we understand Kube-Proxy‚Äôs role, let‚Äôs delve into how it accomplishes this task without getting too technical.</p> <p>Service Discovery: Just like a post office helps you find the address of a friend, Kube-Proxy maintains a list of services and their corresponding pods. When one pod wants to talk to another, it asks Kube-Proxy for help in finding the right address (the destination pod).</p> <p>Load Balancing: Imagine a popular restaurant with multiple entrances. Kube-Proxy helps distribute the incoming diners (requests) evenly among the different entrances (pods). This ensures that no single entrance gets overcrowded, making the dining experience smoother.</p> <p>Source IP Preservation: Sometimes, pods need to know who is sending them a message. Kube-Proxy ensures that the source IP address is preserved so that the receiving pod can identify the sender. It‚Äôs like attaching a return address to your letter.</p> <p>Network Routing: Kube-Proxy also manages network routes, making sure that traffic flows correctly between pods, even if they are on different worker nodes.</p>","path":["KUBERNETES","CORE","Kube-Proxy"],"tags":["Kubernetes"]},{"location":"kubernetes/kube-proxy/#why-kube-proxy-is-important","level":3,"title":"Why Kube-Proxy is Important?","text":"<p>Kube-Proxy plays a vital role in ensuring that your applications in Kubernetes can communicate with each other and with the outside world effectively. Here‚Äôs why it‚Äôs crucial:</p> <p>Pod Communication: Without Kube-Proxy, pods would struggle to find each other in the cluster. It makes sure that when one pod wants to talk to another, the communication happens seamlessly.</p> <p>Load Balancing: In the bustling city of Kubernetes, Kube-Proxy ensures that traffic is distributed evenly among the pods that provide the same service. This keeps your applications responsive and reliable, just like ensuring that all restaurant entrances are used efficiently.</p> <p>Network Isolation: Kube-Proxy helps maintain network security by ensuring that pods can only communicate with the services they are allowed to. It‚Äôs like having security personnel who check IDs at the city‚Äôs entrances.</p> <p>Scaling: As your city (cluster) grows, Kube-Proxy scales with it, ensuring that network traffic remains efficient and load-balanced even when you add more buildings (nodes) or apartments (pods).</p>","path":["KUBERNETES","CORE","Kube-Proxy"],"tags":["Kubernetes"]},{"location":"kubernetes/kube-proxy/#conclusion","level":3,"title":"Conclusion","text":"<p>In the vast universe of Kubernetes, understanding networking doesn‚Äôt have to be rocket science. With the insights gained from this article, you‚Äôve taken a significant step toward demystifying Kubernetes networking. Remember, it‚Äôs all about making connections and facilitating communication between your containerized applications. As you continue your Kubernetes journey, keep these simplified networking concepts in your toolkit, and you‚Äôll navigate the Kubernetes cosmos with confidence. Happy networking!</p>","path":["KUBERNETES","CORE","Kube-Proxy"],"tags":["Kubernetes"]},{"location":"kubernetes/kube-scheduler/","level":1,"title":"Kube-Scheduler","text":"","path":["KUBERNETES","CORE","Kube-Scheduler"],"tags":["Kubernetes"]},{"location":"kubernetes/kube-scheduler/#quest-ce-que-kube-scheduler","level":3,"title":"Qu'est-ce que Kube-Scheduler ?","text":"<p>Kube-scheduler est le scheduler par d√©faut de Kubernetes, responsable d'assigner les pods nouvellement cr√©√©s aux nodes appropri√©s au sein du cluster. Il prend ces d√©cisions en fonction de divers facteurs afin d'assurer une performance optimale, une utilisation efficace des ressources et le respect des contraintes ou des politiques d√©finies par l'utilisateur.</p>","path":["KUBERNETES","CORE","Kube-Scheduler"],"tags":["Kubernetes"]},{"location":"kubernetes/kube-scheduler/#key-responsibilities-du-scheduler","level":3,"title":"Key responsibilities du Scheduler :","text":"<ol> <li>Node Selection: Identifier le meilleur node pour chaque pod en fonction des exigences en ressources et de leur disponibilit√©.</li> <li>Resource Allocation: S'assurer que le CPU, la m√©moire et les autres resources sont correctement allou√©s.</li> <li>Constraints Handling: Prendre en compte des constraints telles que l'affinit√© des nodes, les taints et les tolerations.</li> <li>Prioritization: Classer les nodes en fonction de divers crit√®res pour trouver le plus adapt√©.</li> </ol>","path":["KUBERNETES","CORE","Kube-Scheduler"],"tags":["Kubernetes"]},{"location":"kubernetes/kube-scheduler/#example-the-restaurant-table-scheduler","level":3,"title":"Example The Restaurant Table Scheduler","text":"<p>Imagine a busy restaurant where guests arrive without reservations and need to be seated at the appropriate tables. The restaurant has a host (analogous to the kube-scheduler) whose job is to seat guests at the best available table based on several factors. Let‚Äôs explore how this restaurant scenario parallels the functioning of the kube-scheduler.</p>","path":["KUBERNETES","CORE","Kube-Scheduler"],"tags":["Kubernetes"]},{"location":"kubernetes/kube-scheduler/#step-by-step-scheduling-process","level":4,"title":"Step-by-Step Scheduling Process","text":"<ol> <li> <p>Guest Arrival (Pod Creation)</p> <ul> <li>In Kubernetes, a pod represents one or more containers that need to run on a node. When a new pod is created, it‚Äôs similar to a new group of guests arriving at the restaurant.</li> </ul> </li> <li> <p>Checking Table Availability (Node Filtering)</p> <ul> <li>The host first checks which tables are available. Similarly, the kube-scheduler filters out nodes that cannot accommodate the pod due to insufficient resources or other constraints.</li> </ul> </li> <li> <p>Considering Guest Preferences (Node Affinity and Anti-Affinity)</p> <ul> <li>Some guests may prefer to sit in a specific area of the restaurant (near the window, away from the kitchen). The host considers these preferences. In Kubernetes, this is managed through node affinity and anti-affinity rules that guide the scheduler on preferred or avoided nodes.</li> </ul> </li> <li> <p>Matching Table Size to Party Size (Resource Requests)</p> <ul> <li>The host matches the table size with the number of guests. In Kubernetes, the kube-scheduler looks at the resource requests (CPU, memory) specified for the pod and matches them with the available resources on the nodes.</li> </ul> </li> <li> <p>Special Requests (Taints and Tolerations)</p> <ul> <li>Some guests might have special requests, like requiring a high chair or a quiet corner. The host must ensure these needs are met. Similarly, nodes can have taints that only certain pods with matching tolerations can tolerate, ensuring special conditions are respected.</li> </ul> </li> <li> <p>Selecting the Best Table (Node Prioritization)</p> <ul> <li>Once suitable tables are identified, the host prioritizes them based on factors like proximity to the kitchen for quicker service or distance from noisy areas. The kube-scheduler ranks the nodes using various scoring algorithms to choose the best fit for the pod.</li> </ul> </li> <li> <p>Seating the Guests (Binding the Pod)</p> <ul> <li>Finally, the host seats the guests at the selected table. The kube-scheduler assigns the pod to the chosen node, officially binding it.</li> </ul> </li> </ol> <p>Example in Kubernetes Terms</p> <p>Let‚Äôs consider a concrete example in Kubernetes:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: test-pod\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    resources:\n      requests:\n        memory: \"64Mi\"\n        cpu: \"250m\"\n      limits:\n        memory: \"128Mi\"\n        cpu: \"500m\"\n  nodeSelector:\n    disktype: ssd\n  tolerations:\n  - key: \"key1\"\n    operator: \"Equal\"\n    value: \"value1\"\n    effect: \"NoSchedule\"\n</code></pre> <p>Kube-Scheduler Components Scheduling Algorithm The kube-scheduler follows a two-step process: filtering and scoring.</p> <ol> <li> <p>Filtering: The scheduler filters out nodes that do not meet the pod‚Äôs requirements. This includes checks for resource availability, node conditions, taints, and affinity/anti-affinity rules.</p> </li> <li> <p>Scoring: After filtering, the scheduler scores the remaining nodes to find the most suitable one. Various plugins and scoring functions are used, considering factors like resource utilization, pod topology spread, and custom user-defined rules.</p> </li> </ol> <p>Plugins and Extensibility Kube-scheduler is highly extensible, allowing custom scheduling policies and plugins. This flexibility enables users to tailor the scheduling process to meet specific needs and optimize resource allocation and performance.</p>","path":["KUBERNETES","CORE","Kube-Scheduler"],"tags":["Kubernetes"]},{"location":"kubernetes/kube-scheduler/#conclusion","level":3,"title":"Conclusion","text":"<p>Configurer correctement et comprendre le kube-scheduler est essentiel dans plusieurs sc√©narios :</p> <ul> <li>High-Density Clusters: Lors de l'ex√©cution d'un grand nombre de pods, une planification efficace garantit une utilisation optimale des ressources et des performances √©lev√©es.</li> <li>Resource-Constrained Environments: Dans des environnements avec des ressources limit√©es, une planification efficace emp√™che la contention des ressources et assure un fonctionnement stable.</li> <li>Complex Workloads: Les applications ayant des besoins sp√©cifiques en mati√®re de placement, comme les r√®gles affinity/anti-affinity et les contraintes de ressources, b√©n√©ficient de politiques de planification personnalis√©es.</li> </ul>","path":["KUBERNETES","CORE","Kube-Scheduler"],"tags":["Kubernetes"]},{"location":"kubernetes/kubelet/","level":1,"title":"Kubelet","text":"<p>Kubelet is one of the key component in Kubernetes. This article will help you understand what Kubelet is, what it does, and why it‚Äôs important, all explained in simple terms with examples.</p>","path":["Kubelet"],"tags":["Kubernetes","Kubelet"]},{"location":"kubernetes/kubelet/#what-is-kubelet","level":3,"title":"What is Kubelet?","text":"<p>Kubelet is an agent that runs on every node (a machine in the cluster) in a Kubernetes cluster. Its main job is to ensure that containers are running in a Pod as expected. Think of Kubelet as a caretaker that looks after the containers on its node, making sure they are healthy and running smoothly.</p>","path":["Kubelet"],"tags":["Kubernetes","Kubelet"]},{"location":"kubernetes/kubelet/#key-responsibilities-of-kubelet","level":3,"title":"Key Responsibilities of Kubelet","text":"<ol> <li>Pod Management: Kubelet watches for new Pod specifications and ensures they are running as described. A Pod is a group of one or more containers with shared storage and network, and a specification (a YAML or JSON file) tells Kubelet how to manage it.</li> <li>Health Monitoring: Kubelet regularly checks the health of the containers in a Pod. If a container is not running correctly, Kubelet can restart it based on the defined policies.</li> <li>Node Communication: Kubelet communicates with the Kubernetes API server to report the status of the node and its Pods. This helps the entire cluster stay updated on the state of each node.</li> <li>Container Runtime Interface (CRI): Kubelet uses CRI to interact with container runtimes like Docker or containerd. This allows Kubelet to manage the lifecycle of containers efficiently.</li> </ol>","path":["Kubelet"],"tags":["Kubernetes","Kubelet"]},{"location":"kubernetes/kubelet/#how-kubelet-works","level":3,"title":"How Kubelet Works?","text":"<p>Let‚Äôs break down how Kubelet functions with an example. Suppose you have a Kubernetes cluster with three nodes, and you want to deploy a web application.</p> <ol> <li>Defining a Pod: You create a Pod specification file (<code>webapp-pod.yaml</code>) which includes details like the container image (e.g., <code>nginx</code>), ports, and resource limits. <pre><code>apiVersion: v1 kind: Pod metadata:   \nname: webapp \nspec:   \ncontainers:   - name: \nwebapp-container     \nimage: nginx    \nports:     - containerPort: 80\n</code></pre></li> <li>API Server Interaction: You submit this specification to the Kubernetes API server using <code>kubectl apply -f webapp-pod.yaml</code>. The API server stores this configuration and informs the Kubelet on each node.</li> <li>Pod Scheduling: The Kubernetes scheduler decides which node should run the Pod based on resource availability and other constraints.</li> <li>Kubelet Actions: Once a node is selected, Kubelet on that node takes over:</li> <li>Pod Creation: Kubelet reads the Pod specification and interacts with the container runtime (e.g., Docker) to pull the <code>nginx</code> image and start the container.</li> <li>Health Checks: Kubelet continuously monitors the health of the <code>webapp</code> container. If the container crashes, Kubelet restarts it as specified in the Pod's restart policy.</li> <li>Status Reporting: Kubelet updates the API server with the status of the Pod, ensuring that the cluster has the latest information.</li> </ol>","path":["Kubelet"],"tags":["Kubernetes","Kubelet"]},{"location":"kubernetes/kubelet/#why-is-kubelet-important","level":3,"title":"Why is Kubelet Important?","text":"<p>Kubelet is crucial for the following reasons:</p> <ul> <li>Automation: It automates the process of starting, stopping, and monitoring containers, reducing manual intervention.</li> <li>Reliability: Kubelet ensures that applications remain running and healthy, contributing to the overall reliability of the system.</li> <li>Scalability: By managing containers on each node, Kubelet helps Kubernetes scale applications effortlessly across multiple nodes.</li> </ul>","path":["Kubelet"],"tags":["Kubernetes","Kubelet"]},{"location":"kubernetes/kubelet/#kubelet-configuration","level":3,"title":"Kubelet Configuration","text":"<p>Configuration Files: Kubelet is configured using command-line flags, environment variables, and configuration files. These configurations define how Kubelet should behave and interact with other components in the Kubernetes cluster.</p> <p>Example Configuration: Here‚Äôs a sample Kubelet configuration file:</p> <p><pre><code>apiVersion: kubelet.config.k8s.io/v1beta1\nkind: KubeletConfiguration\naddress: 0.0.0.0\nport: 10250\nreadOnlyPort: 10255\nclusterDNS:\n  - 10.96.0.10\nclusterDomain: cluster.local\n</code></pre> - <code>address</code>: The IP address for the Kubelet to serve on. - <code>port</code>: The port for the Kubelet to serve on. - <code>clusterDNS</code>: The DNS server IP addresses. - <code>clusterDomain</code>: The DNS domain for the cluster.</p>","path":["Kubelet"],"tags":["Kubernetes","Kubelet"]},{"location":"kubernetes/kubelet/#kubelets-role-in-the-kubernetes-architecture","level":3,"title":"Kubelet‚Äôs Role in the Kubernetes Architecture","text":"<p>Integration with Other Components: Kubelet interacts with the Kubernetes API server, scheduler, and etcd. It receives Pod specifications from the API server, schedules Pods as directed by the scheduler, and maintains the state of Pods in etcd.</p> <p>Flow of Operations: The typical flow of operations in Kubernetes involves:</p> <ol> <li>Submitting a Pod specification to the API server.</li> <li>The scheduler selects a node for the Pod.</li> <li>Kubelet on the chosen node creates and manages the Pod.</li> </ol>","path":["Kubelet"],"tags":["Kubernetes","Kubelet"]},{"location":"kubernetes/kubelet/#detailed-explanation-of-health-checks","level":3,"title":"Detailed Explanation of Health Checks","text":"<p>Health checks are crucial for maintaining the stability and reliability of your applications in Kubernetes. Kubelet uses two primary types of health checks: liveness probes and readiness probes.</p>","path":["Kubelet"],"tags":["Kubernetes","Kubelet"]},{"location":"kubernetes/kubelet/#liveness-probes","level":3,"title":"Liveness Probes","text":"<p>Liveness probes determine if a container is still running. If a liveness probe fails, Kubelet will restart the container to try to fix the issue.</p> <p>Example:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: liveness-pod\nspec:\n  containers:\n  - name: liveness-container\n    image: nginx\n    livenessProbe:\n      httpGet:\n        path: /healthz\n        port: 80\n      initialDelaySeconds: 3\n      periodSeconds: 3\n</code></pre> <p>In this example, Kubelet checks the <code>/healthz</code> endpoint of the <code>nginx</code> container every 3 seconds after an initial delay of 3 seconds. If this endpoint returns a non-200 status code, Kubelet will restart the container.</p>","path":["Kubelet"],"tags":["Kubernetes","Kubelet"]},{"location":"kubernetes/kubelet/#readiness-probes","level":3,"title":"Readiness Probes","text":"<p>Readiness probes determine if a container is ready to start accepting traffic. If a readiness probe fails, Kubelet will temporarily remove the Pod from the service endpoints.</p> <p>Example:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: readiness-pod\nspec:\n  containers:\n  - name: readiness-container\n    image: nginx\n    readinessProbe:\n      httpGet:\n        path: /ready\n        port: 80\n      initialDelaySeconds: 3\n      periodSeconds: 3\n</code></pre> <p>Here, Kubelet checks the <code>/ready</code> endpoint of the <code>nginx</code> container. If this endpoint fails, the container will not receive any traffic until it passes the readiness check.</p> <p>By understanding and implementing these health checks, you can ensure your applications are more resilient and capable of self-healing in the face of failures.</p>","path":["Kubelet"],"tags":["Kubernetes","Kubelet"]},{"location":"kubernetes/kubelet/#conclusion","level":3,"title":"Conclusion","text":"<p>Kubelet, acts as the node-level agent that ensures containers are running as expected. It handles Pod creation, health monitoring, and communication with the Kubernetes API server, making it an essential part of maintaining the health and efficiency of your Kubernetes cluster.</p>","path":["Kubelet"],"tags":["Kubernetes","Kubelet"]},{"location":"kubernetes/networkPolicies/","level":1,"title":"Network policies","text":"<p>As we near the conclusion of our Kubernetes Networking series, we‚Äôve covered a lot, from understanding pods to working with Cluster IPs, and much more. Now, as we bid farewell to this series, let‚Äôs wrap it up by exploring Network Policies. In plain language, Network Policies serve as the guardians of connections, allowing only those connections that are truly necessary to pass through.</p> <p></p>","path":["Network policies"],"tags":["Kubernetes"]},{"location":"kubernetes/networkPolicies/#understanding-the-role-of-kubernetes-network-policies","level":3,"title":"Understanding the Role of Kubernetes Network Policies:","text":"<p>In Kubernetes, pods are the smallest deployable units, and they often need to communicate with each other for various reasons, like web services, databases, or other microservices. However, not all pod-to-pod communication should be allowed. This is where Network Policies come into play.</p> <p>Network Policies allow you to define rules that specify which pods can talk to each other and how they can communicate. These rules are based on labels and selectors, making it possible to create fine-grained access controls.</p>","path":["Network policies"],"tags":["Kubernetes"]},{"location":"kubernetes/networkPolicies/#practical-use-cases","level":3,"title":"Practical Use Cases:","text":"<p>Let‚Äôs explore some practical scenarios where Network Policies can be extremely helpful:</p> <ul> <li>Isolation: You may have sensitive data stored in one pod, and you want to isolate it from other pods that shouldn‚Äôt have access to it. Network Policies can help restrict access to only the pods that require it.</li> <li>Microservices Security: In a microservices' architecture, different pods serve various functions. Network Policies ensure that only specific pods can communicate, improving security.</li> <li>Segmenting Environments: If you have multiple environments, like development, staging, and production, Network Policies can ensure that pods in one environment can‚Äôt accidentally or maliciously interfere with pods in another environment.</li> </ul>","path":["Network policies"],"tags":["Kubernetes"]},{"location":"kubernetes/networkPolicies/#setting-up-kubernetes-network-policies","level":3,"title":"Setting Up Kubernetes Network Policies","text":"<p>Here‚Äôs a basic step-by-step guide to setting up Network Policies:</p> <p>Step 1: Create a Network Policy YAML File</p> <p>You need to define your Network Policy rules in a YAML file. This file should include things like which pods can talk to other pods, which ports are open, and so on.</p> <p>Step 2: Apply the Network Policy</p> <p>Use the <code>kubectl apply</code> command to apply your Network Policy. For example, if your YAML file is named <code>my-network-policy.yaml</code>, you can run:</p> <pre><code>kubectl apply -f my-network-policy.yaml\n</code></pre> <p>Step 3: Testing</p> <p>Now that you‚Äôve applied the Network Policy, test your setup. Make sure that only the allowed pods can communicate as per your defined rules.</p> <p>Step 4: Debugging</p> <p>If something isn‚Äôt working as expected, you might need to inspect your Network Policy rules or pods‚Äô labels and selectors to identify the issue. Kubectl commands like <code>kubectl describe networkpolicy</code> and <code>kubectl get pods</code> can be helpful.</p> <p>Now, let‚Äôs look at some practical examples of network policies for beginners:</p>","path":["Network policies"],"tags":["Kubernetes"]},{"location":"kubernetes/networkPolicies/#example-1-deny-all-traffic","level":3,"title":"Example 1: Deny All Traffic","text":"<p>This policy denies all incoming and outgoing traffic for a specific set of pods. It‚Äôs like shutting down all communication for these pods.</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: deny-all-policy\nspec:\n  podSelector:\n    matchLabels:\n      app: my-app\n  policyTypes:\n    - Ingress\n    - Egress\n</code></pre>","path":["Network policies"],"tags":["Kubernetes"]},{"location":"kubernetes/networkPolicies/#example-2-allow-communication-between-frontend-and-backend","level":3,"title":"Example 2: Allow Communication Between Frontend and Backend","text":"<p>Suppose you have frontend and backend pods, and you want to allow traffic only from frontend to backend. Here‚Äôs how you could define the network policy:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-frontend-to-backend\nspec:\n  podSelector:\n    matchLabels:\n      app: frontend\n  ingress:\n    - from:\n      - podSelector:\n          matchLabels:\n            app: backend\n</code></pre> <p>This policy allows incoming traffic from pods labelled <code>app: backend</code> to pods labelled <code>app: frontend</code>. In this case, it's like saying, ‚ÄúFrontend pods can talk to backend pods.‚Äù</p>","path":["Network policies"],"tags":["Kubernetes"]},{"location":"kubernetes/networkPolicies/#example-3-isolate-a-database","level":3,"title":"Example 3: Isolate a Database","text":"<p>Suppose you have a database pod, and you want to isolate it from all other pods to protect sensitive data. You could create a network policy like this:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: isolate-database\nspec:\n  podSelector:\n    matchLabels:\n      app: database\n  ingress: []  # No incoming traffic allowed\n  egress: []   # No outgoing traffic allowed\n</code></pre> <p>This policy blocks all incoming and outgoing traffic to the <code>app: database</code> pod. So, it's completely isolated.</p>","path":["Network policies"],"tags":["Kubernetes"]},{"location":"kubernetes/networkPolicies/#example-4-allow-access-to-external-service","level":3,"title":"Example 4: Allow Access to External Service","text":"<p>If you have a pod that needs access to an external service, you can control it with egress rules. Here‚Äôs an example that allows a specific pod to access an external database:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-external-access\nspec:\n  podSelector:\n    matchLabels:\n      app: my-app\n  egress:\n    - to:\n      - ipBlock:\n          cidr: 203.0.113.0/24  # IP range of the external database\n</code></pre> <p>This policy permits the <code>app: my-app</code> pod to communicate with an external database specified by its IP range.</p> <p>By understanding and creating these types of Network Policies, you can control communication within your Kubernetes cluster, enhancing security and ensuring that your applications function as intended while minimizing unnecessary access.</p> <p>With this article, we are giving an end to our Kubernetes networking series. Up next we are going to learn about security in Kubernetes. Stay tuned and make sure you follow</p>","path":["Network policies"],"tags":["Kubernetes"]},{"location":"kubernetes/podDisruptionBudgets/","level":1,"title":"Pod Disruption Budgets","text":"<p>Welcome to Kubernetes adventure series, today we came up with a powerful feature designed to manage these disruptions gracefully ‚Äî Pod Disruption Budgets (PDBs). In this guide, we‚Äôll dive deep into PDBs, exploring their importance, functionality, and how you can leverage them to ensure high availability of your services.</p>","path":["KUBERNETES","RESOURCES","Pods","Pod Disruption Budgets"],"tags":["Kubernetes","PodDisruptionBudgets"]},{"location":"kubernetes/podDisruptionBudgets/#understanding-pod-disruption-budgets-pdbs","level":3,"title":"Understanding Pod Disruption Budgets (PDBs)","text":"<p>Pod Disruption Budgets serve as a safeguard, ensuring that a specified minimum number of pods remain running during voluntary disruptions, such as node maintenance or upgrades. By defining PDBs, you‚Äôre essentially setting a ‚Äúminimum available‚Äù or ‚Äúmaximum unavailable‚Äù threshold for your pods, which Kubernetes will respect when performing operations that could disrupt your service.</p>","path":["KUBERNETES","RESOURCES","Pods","Pod Disruption Budgets"],"tags":["Kubernetes","PodDisruptionBudgets"]},{"location":"kubernetes/podDisruptionBudgets/#the-shift-to-pdbs","level":3,"title":"The Shift to PDBs","text":"<p>Traditionally, maintaining service availability during disruptions was a manual and cumbersome process, often resulting in longer downtimes or even data loss. With PDBs, Kubernetes introduces an automated way to manage these disruptions, significantly reducing the need for manual intervention and minimizing the risk of downtime.</p>","path":["KUBERNETES","RESOURCES","Pods","Pod Disruption Budgets"],"tags":["Kubernetes","PodDisruptionBudgets"]},{"location":"kubernetes/podDisruptionBudgets/#how-pod-disruption-budgets-work","level":3,"title":"How Pod Disruption Budgets Work?","text":"<p>When you create a Pod Disruption Budget, you specify critical parameters such as the minimum number of available replicas (minAvailable) or the maximum number of replicas that can be unavailable (maxUnavailable), along with a selector to apply the policy to a specific set of pods. Kubernetes then uses this configuration to make informed decisions about managing disruptions, ensuring that your specified availability criteria are met before proceeding with pod evictions.</p>","path":["KUBERNETES","RESOURCES","Pods","Pod Disruption Budgets"],"tags":["Kubernetes","PodDisruptionBudgets"]},{"location":"kubernetes/podDisruptionBudgets/#implementing-pdbs-in-your-cluster","level":3,"title":"Implementing PDBs in Your Cluster","text":"<p>To leverage Pod Disruption Budgets, you‚Äôll start by defining a PDB object in your Kubernetes cluster. This involves creating a YAML or JSON file that specifies the PDB‚Äôs parameters and applying it using the kubectl command. Here‚Äôs a simple example to protect a deployment named ‚Äúmy-deployment‚Äù:</p> <pre><code>apiVersion: policy/v1beta1\nkind: PodDisruptionBudget\nmetadata:\n  name: my-pdb\n  namespace: my-namespace\nspec:\n  minAvailable: 2\n  selector:\n    matchLabels:\n      app: my-deployment\n</code></pre> <p>This PDB ensures that at least two replicas of ‚Äúmy-deployment‚Äù remain available at all times, protecting your application from becoming unavailable during disruptions.</p>","path":["KUBERNETES","RESOURCES","Pods","Pod Disruption Budgets"],"tags":["Kubernetes","PodDisruptionBudgets"]},{"location":"kubernetes/podDisruptionBudgets/#advantages-of-pod-disruption-budgets","level":3,"title":"Advantages of Pod Disruption Budgets","text":"<p>Adopting PDBs in your Kubernetes environment brings several benefits:</p> <ul> <li>Enhanced Availability: By ensuring a minimum number of pods are always running, PDBs help maintain the availability of your critical services.</li> <li>Automated Disruption Management: PDBs automate the handling of disruptions, reducing the need for manual oversight and intervention.</li> <li>Improved Cluster Stability: By preventing excessive simultaneous disruptions, PDBs contribute to the overall stability and reliability of your Kubernetes cluster.</li> <li>Cost Efficiency: By minimizing downtime and potential data loss, PDBs can lead to significant cost savings over time.</li> </ul>","path":["KUBERNETES","RESOURCES","Pods","Pod Disruption Budgets"],"tags":["Kubernetes","PodDisruptionBudgets"]},{"location":"kubernetes/podDisruptionBudgets/#conclusion","level":3,"title":"Conclusion","text":"<p>Pod Disruption Budgets are an essential feature for anyone looking to bolster the resilience and availability of their Kubernetes deployments. By understanding and implementing PDBs, you can protect your applications from disruptions, ensuring a seamless experience for both your team and your users. As you continue to explore Kubernetes‚Äô vast ecosystem, remember that PDBs are just one of the many tools at your disposal to build a robust and scalable infrastructure.</p>","path":["KUBERNETES","RESOURCES","Pods","Pod Disruption Budgets"],"tags":["Kubernetes","PodDisruptionBudgets"]},{"location":"kubernetes/pv-pvc/","level":1,"title":"Persistent Volumes (PV) and Persistent Volume Claims (PVC)","text":"<p>In Kubernetes, managing storage efficiently stands out as a crucial aspect of ensuring applications run smoothly and data persists beyond the life of individual pods. This is where Persistent Volumes (PV) and Persistent Volume Claims (PVC) come into play. Today let‚Äôs learn these concepts and understand their significance in the Kubernetes ecosystem.</p>","path":["KUBERNETES","RESOURCES","Persistent Volumes (PV) and Persistent Volume Claims (PVC)"],"tags":["Kubernetes"]},{"location":"kubernetes/pv-pvc/#persistent-volumes-pv","level":3,"title":"Persistent Volumes (PV)","text":"<p>A Persistent Volume (PV) is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using Storage Classes. It‚Äôs a way to abstract the details of how the storage is provided and how it‚Äôs consumed. Think of PV as a physical hard drive in a network storage system, but with a twist. It‚Äôs not tied to any single pod; instead, it exists independently of pod lifecycles, ensuring that data persists across pod restarts and failures.</p> <p>PVs can come from various sources:</p> <ul> <li>Network-attached storage systems like NFS, iSCSI, or cloud-based storage such as AWS EBS, Google Cloud Persistent Disk, or Azure Disk Storage.</li> <li>Local storage on the nodes.</li> </ul> <p>Administrators can define PVs in the cluster without associating them with any particular pod. This way, they can manage storage resources separately from pod-specific configurations.</p>","path":["KUBERNETES","RESOURCES","Persistent Volumes (PV) and Persistent Volume Claims (PVC)"],"tags":["Kubernetes"]},{"location":"kubernetes/pv-pvc/#persistent-volume-claims-pvc","level":3,"title":"Persistent Volume Claims (PVC)","text":"<p>While PVs are the storage volumes available within the cluster, Persistent Volume Claims are essentially requests for storage by a user. A PVC specifies size, and access modes like read-only, read-write, among other criteria. You can think of a PVC as a storage request ticket given to the Kubernetes cluster by a pod that needs storage.</p> <p>The magic of Kubernetes matches a PVC to a PV based on the requirements cited in the claim (size, access modes, etc.). If the cluster has a PV that satisfies the claim, it binds the PV to the PVC, making the storage available to the pod that made the request. If no suitable PV is available, and dynamic provisioning is enabled, the cluster may dynamically create a new PV that matches the claim.</p>","path":["KUBERNETES","RESOURCES","Persistent Volumes (PV) and Persistent Volume Claims (PVC)"],"tags":["Kubernetes"]},{"location":"kubernetes/pv-pvc/#why-pv-and-pvc","level":3,"title":"Why PV and PVC?","text":"<p>This separation of concerns is what makes the PV/PVC model powerful:</p> <ul> <li>Decoupling of storage configuration from the usage: Administrators can prepare a pool of storage (PVs) without knowing the specific needs of each pod, allowing for a more flexible and efficient use of resources.</li> <li>Storage consumption as needed: Pods can claim storage resources as needed without requiring direct access or knowledge of the underlying storage infrastructure.</li> <li>Dynamic provisioning: The ability to dynamically create storage resources based on demand, reducing manual intervention and speeding up deployment processes.</li> </ul>","path":["KUBERNETES","RESOURCES","Persistent Volumes (PV) and Persistent Volume Claims (PVC)"],"tags":["Kubernetes"]},{"location":"kubernetes/pv-pvc/#example-scenario","level":3,"title":"Example Scenario","text":"<p>To put it into perspective, imagine if you‚Äôre moving into a new apartment (pod) and you need a storage unit (PV). You fill out a form specifying your needs (PVC), and the apartment complex matches you with an available unit that fits your criteria. If no units are available, they build a new one just for you. This way, you get the storage you need without worrying about the details of how it‚Äôs provided.</p>","path":["KUBERNETES","RESOURCES","Persistent Volumes (PV) and Persistent Volume Claims (PVC)"],"tags":["Kubernetes"]},{"location":"kubernetes/pv-pvc/#conclusion","level":3,"title":"Conclusion","text":"<p>Persistent Volumes and Persistent Volume Claims are foundational elements in Kubernetes that ensure data persists and is accessible to applications, irrespective of the lifecycle of individual pods. By understanding and leveraging PVs and PVCs, developers can focus on building and deploying applications, leaving the complexities of storage management to Kubernetes.</p> <p>I'll help you sail through the ocean of Kubernetes with minimal efforts</p>","path":["KUBERNETES","RESOURCES","Persistent Volumes (PV) and Persistent Volume Claims (PVC)"],"tags":["Kubernetes"]},{"location":"kubernetes/resourceLimits/","level":1,"title":"Resource Limits","text":"<p>Resource limits in Kubernetes ensures that every container has its fair share of the spotlight without overshadowing others. This article will walk you through the what, why, and how of Kubernetes resource limits, all aimed at keeping your performance smooth and your bills lean.</p>","path":["KUBERNETES","RESOURCES","Pods","Resource Limits"],"tags":["Kubernetes","ResourcesLimits"]},{"location":"kubernetes/resourceLimits/#what-are-resource-limits-in-kubernetes","level":3,"title":"What Are Resource Limits in Kubernetes?","text":"<p>Imagine you‚Äôre at a buffet with a strict limit on how many dishes you can take at once. Kubernetes resource limits are somewhat similar. They‚Äôre rules you set to specify the maximum amount of CPU and memory (RAM) that a container can use. These limits prevent any single container from hogging all the resources on your cluster, ensuring that every application gets its fair share and plays nicely with others.</p>","path":["KUBERNETES","RESOURCES","Pods","Resource Limits"],"tags":["Kubernetes","ResourcesLimits"]},{"location":"kubernetes/resourceLimits/#why-are-resource-limits-important","level":3,"title":"Why Are Resource Limits Important?","text":"<p>Setting resource limits is like having traffic lights on roads; they prevent chaos by managing the flow efficiently. In Kubernetes, these limits help in several ways:</p> <ol> <li>Prevent Resource Hogging: They ensure no single application can monopolize system resources, which could lead to other applications underperforming or crashing due to lack of resources.</li> <li>Improve Stability: By controlling how resources are allocated, you can avoid system overloads and ensure a stable environment for all your applications.</li> <li>Cost Control: Especially in cloud environments, managing resource use effectively can help control costs by preventing overallocation of resources.</li> </ol> <p>Setting Resource Limits</p> <p>Setting resource limits in Kubernetes is straightforward. You define them in your pod‚Äôs YAML configuration file under the <code>spec</code> section for each container. Here's a quick example:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: sample-pod\nspec:\n  containers:\n  - name: sample-container\n    image: nginx\n    resources:\n      limits:\n        cpu: \"1\"\n        memory: \"512Mi\"\n      requests:\n        cpu: \"0.5\"\n        memory: \"256Mi\"\n</code></pre> <p>In this snippet, the <code>limits</code> section sets the maximum CPU and memory the container can use, while <code>requests</code> specify the resources the container needs to start. If the container tries to use more than the allocated limit, Kubernetes takes actions to limit the usage, ensuring the container doesn't exceed its share.</p>","path":["KUBERNETES","RESOURCES","Pods","Resource Limits"],"tags":["Kubernetes","ResourcesLimits"]},{"location":"kubernetes/resourceLimits/#understanding-the-cost-implications-of-resource-limits","level":3,"title":"Understanding the Cost Implications of Resource Limits","text":"<p>Resource limits in Kubernetes allow you to specify the maximum amount of CPU and memory that a container can use. While this is crucial for preventing any single application from consuming excessive resources, it also directly influences your cloud billing.</p> <ul> <li>Overallocation: Setting resource limits higher than what an application actually needs can lead to underutilized resources. In cloud environments, where you pay for what you allocate, overallocation means paying for resources you don‚Äôt use.</li> <li>Underallocation: Conversely, setting limits too low can lead to throttled applications, potentially affecting performance and, indirectly, revenue, especially for customer-facing services.</li> </ul>","path":["KUBERNETES","RESOURCES","Pods","Resource Limits"],"tags":["Kubernetes","ResourcesLimits"]},{"location":"kubernetes/resourceLimits/#strategies-to-optimize-costs-through-resource-limits","level":3,"title":"Strategies to Optimize Costs Through Resource Limits","text":"<ol> <li>Baseline and Monitor: Before setting resource limits, it‚Äôs essential to understand your applications‚Äô typical resource usage. Tools like Kubernetes Metrics Server or third-party monitoring solutions can provide insights into each application‚Äôs CPU and memory usage over time.</li> <li>Rightsize Your Limits: Use the data collected to set resource limits that closely match your applications‚Äô needs, with a small buffer to handle unexpected spikes in demand. Regularly review and adjust these limits based on changes in application behavior and usage patterns.</li> <li>Leverage Horizontal Pod Autoscaling: Horizontal Pod Autoscaler (HPA) automatically adjusts the number of pods in a deployment based on observed CPU utilization or other selected metrics. By scaling the number of replicas rather than the resources of each pod, you can maintain performance while optimizing costs.</li> <li>Adopt Cluster Autoscaling: For workloads with variable resource demands, Cluster Autoscaler can adjust the size of your cluster dynamically, adding or removing nodes based on the needs of your pods and resource limits. This ensures you‚Äôre only paying for the resources you genuinely need at any given time.</li> <li>Implement Namespaces for Better Resource Management: Use Kubernetes namespaces to segregate resources by team, project, or environment. This not only helps in organizing resources but also in implementing granular policies and limits to control costs more effectively.</li> <li>Utilize Cost Management Tools: Tools specifically designed for cloud cost management can provide insights into how resource usage translates into costs. They can help identify overallocated resources and suggest optimizations to reduce expenses.</li> </ol> <p>To set the right resource limits in Kubernetes, begin with analyzing your applications‚Äô current resource usage, understanding their unique demands, and testing under various conditions to identify optimal performance thresholds. Start with initial limits based on this analysis, adding a buffer to manage spikes, and continuously monitor and adjust based on real-world performance. This iterative process, coupled with educating your team on best practices and implementing policies for regular review, ensures that your applications not only run efficiently but also cost-effectively. Finding this balance between resource allocation and application needs is key to leveraging Kubernetes‚Äô full potential while keeping a tight rein on your cloud expenses.</p> <p>I'll help you sail through the ocean of Kubernetes with minimal efforts</p> <p>In our journey through Kubernetes, today we are going to learn about Resource limits in Kubernetes. In this article we will cover what are Resource limits, why are they crucial and how to set those and as always we will be giving an example at the end. Let‚Äôs dive in</p> <p>Resource limits in K8's</p>","path":["KUBERNETES","RESOURCES","Pods","Resource Limits"],"tags":["Kubernetes","ResourcesLimits"]},{"location":"kubernetes/resourceLimits/#what-are-resource-limits","level":3,"title":"What Are Resource Limits?","text":"<p>Resource limits in Kubernetes are like the guardrails on a highway ‚Äî they prevent containers from hogging all the resources available on a node, which could otherwise lead to instability, poor performance, or even catastrophic crashes. These limits define the maximum amount of CPU and memory that a container can use, ensuring that resources are allocated fairly among different pods running on the same node.</p> <p>Resource limits consist of two key parameters:</p> <p>CPU Limits: These limits specify the maximum amount of CPU a container can consume. CPUs are divided into millicores (1/1000th of a CPU) in Kubernetes, which allows for precise control. For instance, if you set a CPU limit of 500m, the container can use up to half of a single CPU core.</p> <p>Memory Limits: Memory limits define the upper boundary for the amount of RAM a container can use. You can specify the limit in terms of bytes, megabytes, or gigabytes. For example, setting a memory limit of 256Mi means the container can use up to 256 megabytes of memory.</p>","path":["KUBERNETES","RESOURCES","Pods","Resource Limits"],"tags":["Kubernetes","ResourcesLimits"]},{"location":"kubernetes/resourceLimits/#why-are-resource-limits-crucial","level":3,"title":"Why Are Resource Limits Crucial?","text":"<p>Imagine you‚Äôre managing a Kubernetes cluster hosting multiple applications. Without resource limits, one misbehaving container could consume all available resources, leaving other containers starving for CPU and memory. This could lead to a domino effect, causing applications to slow down or crash.</p>","path":["KUBERNETES","RESOURCES","Pods","Resource Limits"],"tags":["Kubernetes","ResourcesLimits"]},{"location":"kubernetes/resourceLimits/#resource-limits-help-ensure","level":3,"title":"Resource limits help ensure:","text":"<p>1. Predictable Performance: Resource limits guarantee that each container has a predictable slice of the available resources. When an application exceeds its limits, Kubernetes takes action to mitigate the impact without affecting other pods.</p> <p>2. Efficient Resource Utilization: By setting limits appropriately, you optimize resource utilization, preventing over-provisioning (wasting resources) and under-provisioning (causing resource shortages).</p> <p>3. Reliability: Resource limits enhance the reliability of your applications. Even if a container goes rogue, it won‚Äôt bring down the entire cluster or degrade the performance of other pods.</p>","path":["KUBERNETES","RESOURCES","Pods","Resource Limits"],"tags":["Kubernetes","ResourcesLimits"]},{"location":"kubernetes/resourceLimits/#how-to-set-resource-limits","level":3,"title":"How to Set Resource Limits?","text":"<p>Setting resource limits in Kubernetes is straightforward. You define these limits within the configuration of your pods or deployments using the resources field. Here‚Äôs an example using a simple web application:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n name: web-app\nspec:\n containers:\n ‚Äî name: app-container\n image: my-web-app:v1\n resources:\n limits:\n cpu: 500m # Limits CPU usage to half a core\n memory: 256Mi # Limits memory usage to 256 megabytes\n</code></pre> <p>In this example, we have set CPU and memory limits for our web application. The CPU limit restricts the container to use a maximum of 500 millicores, while the memory limit caps memory usage at 256 megabytes.</p>","path":["KUBERNETES","RESOURCES","Pods","Resource Limits"],"tags":["Kubernetes","ResourcesLimits"]},{"location":"kubernetes/resourceLimits/#a-practical-example","level":3,"title":"A Practical Example","text":"<p>Let‚Äôs bring it all together with a practical example. Imagine you have a Kubernetes cluster running a database, a web server, and a background job worker. Without resource limits, a misbehaving job in the worker pod could consume all CPU resources, causing your web server and database to become sluggish.</p> <p>By setting resource limits, you can ensure that the worker job doesn‚Äôt monopolize resources, like so:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n name: background-worker\nspec:\n containers:\n ‚Äî name: worker-container\n image: my-worker-app:v1\n resources:\n limits:\n cpu: 500m # Limits CPU usage to half a core\n memory: 256Mi # Limits memory usage to 256 megabytes\n</code></pre> <p>Now, even if the worker job misbehaves, it won‚Äôt disrupt the stability and performance of other pods running in your cluster.</p> <p>In conclusion, Kubernetes resource limits are the safety nets that ensure the stability and performance of your containerized applications. By setting these limits judiciously, you can create a resilient and efficient containerized environment that keeps your applications running smoothly, no matter what challenges they may face.</p>","path":["KUBERNETES","RESOURCES","Pods","Resource Limits"],"tags":["Kubernetes","ResourcesLimits"]},{"location":"kubernetes/serviceAccounts/","level":1,"title":"Service Accounts","text":"<p>Let‚Äôs start of with a simple explanation, Think Kubernetes service accounts as special crew members aboard your ship, each with a specific role, ensuring everything runs smoothly and securely. In this guide, we‚Äôre going to understand Kubernetes Service Accounts inside and out. Let‚Äôs begin..</p> <p>Understanding service accounts in Kubernetes</p> <p>Kubernetes Service Accounts are specialised accounts used by applications and services running on Kubernetes to interact with the Kubernetes API. Unlike user accounts managed outside Kubernetes, Service Accounts are managed within the Kubernetes ecosystem, offering a more granular and controlled approach to accessing cluster resources and performing operations.</p>","path":["Service Accounts"],"tags":["Kubernetes","ServiceAccounts"]},{"location":"kubernetes/serviceAccounts/#why-service-accounts-matter","level":3,"title":"Why Service Accounts Matter?","text":"<p>Service Accounts are vital for several reasons:</p> <ul> <li>Security: They enable the principle of least privilege, ensuring applications have only the access they need.</li> <li>Automation: Service Accounts are essential for automated processes like CI/CD pipelines, where automated tools need to interact with the Kubernetes API.</li> <li>Auditability: By assigning specific accounts to applications, it‚Äôs easier to monitor and audit their actions within the cluster.</li> </ul>","path":["Service Accounts"],"tags":["Kubernetes","ServiceAccounts"]},{"location":"kubernetes/serviceAccounts/#creating-and-managing-service-accounts","level":3,"title":"Creating and Managing Service Accounts","text":"<p>Kubernetes automatically creates a default Service Account within each namespace. However, for enhanced security and control, it‚Äôs often necessary to create dedicated Service Accounts for different applications or services.</p>","path":["Service Accounts"],"tags":["Kubernetes","ServiceAccounts"]},{"location":"kubernetes/serviceAccounts/#creating-a-service-account","level":3,"title":"Creating a Service Account","text":"<p>To create a Service Account, you can use the following YAML manifest:</p> <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: my-service-account\n  namespace: my-namespace\n</code></pre> <p>Save this manifest as <code>service-account.yaml</code> and apply it using <code>kubectl apply -f service-account.yaml</code>. This command creates a Service Account named <code>my-service-account</code> in the <code>my-namespace</code> namespace.</p>","path":["Service Accounts"],"tags":["Kubernetes","ServiceAccounts"]},{"location":"kubernetes/serviceAccounts/#granting-permissions","level":3,"title":"Granting Permissions","text":"<p>Permissions are granted to a Service Account through Roles and RoleBindings (or ClusterRoles and ClusterRoleBindings for cluster-wide permissions):</p> <ul> <li>Role: Defines permissions within a namespace.</li> <li>RoleBinding: Binds a Role to a Service Account within a namespace.</li> <li>ClusterRole: Similar to Role but for cluster-wide permissions.</li> <li>ClusterRoleBinding: Binds a ClusterRole to a Service Account for cluster-wide permissions.</li> </ul> <p>Here‚Äôs an example of a Role and RoleBinding that grants a Service Account read-only access to Pods within a namespace:</p> <pre><code>kind: Role\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  namespace: my-namespace\n  name: pod-reader\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"get\", \"watch\", \"list\"]\n---\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: read-pods\n  namespace: my-namespace\nsubjects:\n- kind: ServiceAccount\n  name: my-service-account\n  namespace: my-namespace\nroleRef:\n  kind: Role\n  name: pod-reader\n  apiGroup: rbac.authorization.k8s.io\n</code></pre>","path":["Service Accounts"],"tags":["Kubernetes","ServiceAccounts"]},{"location":"kubernetes/serviceAccounts/#using-service-accounts-in-pods","level":2,"title":"Using Service Accounts in Pods","text":"<p>To use a Service Account in a Pod, specify the account name in the Pod‚Äôs YAML:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\n  namespace: my-namespace\nspec:\n  serviceAccountName: my-service-account\n  containers:\n  - name: my-container\n    image: my-image\n</code></pre> <p>This configuration ensures that the Pod operates under the permissions granted to the <code>my-service-account</code> Service Account.</p>","path":["Service Accounts"],"tags":["Kubernetes","ServiceAccounts"]},{"location":"kubernetes/serviceAccounts/#practical-applications-and-best-practices","level":3,"title":"Practical Applications and Best Practices","text":"","path":["Service Accounts"],"tags":["Kubernetes","ServiceAccounts"]},{"location":"kubernetes/serviceAccounts/#cicd-integration","level":3,"title":"CI/CD Integration","text":"<p>Service Accounts are instrumental in CI/CD workflows, allowing automation tools to deploy applications, manage configurations, and execute other Kubernetes operations securely.</p>","path":["Service Accounts"],"tags":["Kubernetes","ServiceAccounts"]},{"location":"kubernetes/serviceAccounts/#security-best-practices","level":3,"title":"Security Best Practices","text":"<ul> <li>Principle of Least Privilege: Always grant Service Accounts the minimum necessary permissions to perform their tasks.</li> <li>Regular Audits: Regularly review Service Account permissions and usage to ensure compliance with security policies.</li> <li>Use Namespaces: Leverage namespaces to isolate Service Accounts and limit their scope of influence.</li> </ul>","path":["Service Accounts"],"tags":["Kubernetes","ServiceAccounts"]},{"location":"kubernetes/serviceAccounts/#conclusion","level":3,"title":"Conclusion","text":"<p>By understanding their purpose, how to manage them, and best practices for their use, you can enhance the security and efficiency of your Kubernetes deployments.</p> <p>In our journey through Kubernetes security series, we have seen about basic security principles in our previous article, from this article we will be exploring each and every security concept in detail. Here in this article, we‚Äôll explore everything you need to know about Kubernetes Service Accounts, from the basics to advanced management techniques.</p> <p>kubernetes service accounts</p>","path":["Service Accounts"],"tags":["Kubernetes","ServiceAccounts"]},{"location":"kubernetes/serviceAccounts/#what-are-kubernetes-service-accounts","level":3,"title":"What Are Kubernetes Service Accounts?","text":"<p>Kubernetes Service Accounts are a fundamental component for managing authentication and authorization within your cluster. They allow your applications to interact securely with the Kubernetes API server and other resources.</p> <p>Here are some key aspects of Kubernetes Service Accounts:</p> <ol> <li>Automated Credentials: Service Accounts provide a way for pods to automatically obtain credentials for authentication.</li> <li>Granular Permissions: You can define RBAC (Role-Based Access Control) policies to grant different levels of access to Service Accounts.</li> <li>Secrets Management: Service Accounts can be associated with secrets, allowing your applications to access sensitive data securely.</li> </ol>","path":["Service Accounts"],"tags":["Kubernetes","ServiceAccounts"]},{"location":"kubernetes/serviceAccounts/#why-are-kubernetes-service-accounts-important","level":3,"title":"Why Are Kubernetes Service Accounts Important?","text":"<p>Service Accounts are crucial for several reasons:</p> <ol> <li>Security: By using Service Accounts, you can ensure that only authorized applications can interact with the Kubernetes API server.</li> <li>Isolation: Service Accounts help in isolating workloads within the cluster, preventing unauthorized access.</li> <li>Secrets Management: They simplify the management of secrets and credentials, reducing the risk of data breaches.</li> </ol>","path":["Service Accounts"],"tags":["Kubernetes","ServiceAccounts"]},{"location":"kubernetes/serviceAccounts/#managing-kubernetes-service-accounts","level":3,"title":"Managing Kubernetes Service Accounts","text":"<p>Managing Service Accounts effectively is essential for a secure and well-functioning Kubernetes environment. Here are some best practices:</p> <ol> <li>Namespace Segregation: Create separate namespaces for different projects or teams, each with its own set of Service Accounts.</li> <li>Least Privilege: Follow the principle of least privilege, granting only the necessary permissions to Service Accounts.</li> <li>Regular Rotation: Rotate secrets associated with Service Accounts regularly to enhance security.</li> <li>Audit Logging: Enable audit logs to track Service Account activity and detect any suspicious behaviour.</li> </ol>","path":["Service Accounts"],"tags":["Kubernetes","ServiceAccounts"]},{"location":"kubernetes/serviceAccounts/#faqs","level":3,"title":"FAQs","text":"<ol> <li>Can a Pod Use Multiple Service Accounts?</li> </ol> <p>No, a pod can only use one Service Account. However, you can create custom roles and role bindings to control permissions.</p> <p>2. How Do I Rotate Secrets for Service Accounts?</p> <p>You can use Kubernetes‚Äô built-in mechanisms to automate secret rotation.</p> <p>3. What Happens If a Service Account Is Compromised?</p> <p>If a Service Account is compromised, the attacker gains the permissions associated with it. It‚Äôs crucial to follow security best practices to prevent this.</p> <p>Kubernetes Service Accounts are a vital tool in securing your Kubernetes cluster. By understanding their role and following best practices, you can enhance the security of your containerized applications and ensure smooth operations. Embrace Kubernetes Service Accounts, and unlock the full potential of container orchestration.</p>","path":["Service Accounts"],"tags":["Kubernetes","ServiceAccounts"]},{"location":"kubernetes/helm/helmCheatSheet/","level":1,"title":"Helm cheat sheet","text":"<p>Helm cheatsheet featuring all the necessary commands required to manage an application through Helm.</p>","path":["Helm CheatSheet"],"tags":["CheatSheet","Helm"]},{"location":"kubernetes/helm/helmCheatSheet/#basic-interpretationscontext","level":2,"title":"Basic interpretations/context:","text":"<ul> <li> <p>Chart:</p> <ul> <li>It is the name of your chart in case it has been pulled and untarred.</li> <li>It is / in case the repository has been added but chart not pulled. <li>It is the URL/Absolute path to the chart.</li> <li> <p>Name: It is the name you want to give to your current helm chart installation.</p> </li> <li> <p>Release: Is the name you assigned to an installation instance.</p> </li> <li> <p>Revision: Is the value from the Helm history command.</p> </li> <li> <p>Repo-name: The name of a repository.</p> </li> <li> <p>DIR: Directory name/path.</p> </li>","path":["Helm CheatSheet"],"tags":["CheatSheet","Helm"]},{"location":"kubernetes/helm/helmCheatSheet/#commands","level":2,"title":"Commands","text":"","path":["Helm CheatSheet"],"tags":["CheatSheet","Helm"]},{"location":"kubernetes/helm/helmCheatSheet/#chart-management","level":3,"title":"Chart Management","text":"<pre><code>helm create &lt;name&gt;                        # Creates a chart directory along with the common files and directories used in a chart.\nhelm package &lt;chart-path&gt;                 # Packages a chart into a versioned chart archive file.\nhelm lint &lt;chart&gt;                         # Run tests to examine a chart and identify possible issues:\nhelm show all &lt;chart&gt;                     # Inspect a chart and list its contents:\nhelm show values &lt;repo-name&gt;/&lt;chart-path&gt; # Displays the contents of the values.yaml file\nhelm pull &lt;chart&gt;                         # Download/pull chart \nhelm pull &lt;chart&gt; --untar=true            # If set to true, will untar the chart after downloading it\nhelm pull &lt;chart&gt; --verify                # Verify the package before using it\nhelm pull &lt;chart&gt; --version &lt;number&gt;      # Default-latest is used, specify a version constraint for the chart version to use\nhelm dependency list &lt;chart&gt;              # Display a list of a chart‚Äôs dependencies:\n</code></pre>","path":["Helm CheatSheet"],"tags":["CheatSheet","Helm"]},{"location":"kubernetes/helm/helmCheatSheet/#install-and-uninstall-apps","level":3,"title":"Install and Uninstall Apps","text":"<pre><code>helm install &lt;name&gt; &lt;chart&gt;                           # Install the chart with a name\nhelm install &lt;name&gt; &lt;chart&gt; --namespace &lt;namespace&gt;   # Install the chart in a specific namespace\nhelm install &lt;name&gt; &lt;chart&gt; --set key1=val1,key2=val2 # Set values on the command line (can specify multiple or separate values with commas)\nhelm install &lt;name&gt; &lt;chart&gt; --values &lt;yaml-file/url&gt;  # Install the chart with your specified values\nhelm install &lt;name&gt; &lt;chart&gt; --dry-run --debug         # Run a test installation to validate chart (p)\nhelm install &lt;name&gt; &lt;chart&gt; --verify                  # Verify the package before using it \nhelm install &lt;name&gt; &lt;chart&gt; --dependency-update       # update dependencies if they are missing before installing the chart\nhelm uninstall &lt;name&gt;                                 # Uninstall a release\n</code></pre>","path":["Helm CheatSheet"],"tags":["CheatSheet","Helm"]},{"location":"kubernetes/helm/helmCheatSheet/#perform-app-upgrade-and-rollback","level":3,"title":"Perform App Upgrade and Rollback","text":"<pre><code>helm upgrade &lt;release&gt; &lt;chart&gt;                            # Upgrade a release\nhelm upgrade &lt;release&gt; &lt;chart&gt; --atomic                   # If set, upgrade process rolls back changes made in case of failed upgrade.\nhelm upgrade &lt;release&gt; &lt;chart&gt; --dependency-update        # update dependencies if they are missing before installing the chart\nhelm upgrade &lt;release&gt; &lt;chart&gt; --version &lt;version_number&gt; # specify a version constraint for the chart version to use\nhelm upgrade &lt;release&gt; &lt;chart&gt; --values                   # specify values in a YAML file or a URL (can specify multiple)\nhelm upgrade &lt;release&gt; &lt;chart&gt; --set key1=val1,key2=val2  # Set values on the command line (can specify multiple or separate valuese)\nhelm upgrade &lt;release&gt; &lt;chart&gt; --force                    # Force resource updates through a replacement strategy\nhelm rollback &lt;release&gt; &lt;revision&gt;                        # Roll back a release to a specific revision\nhelm rollback &lt;release&gt; &lt;revision&gt;  --cleanup-on-fail     # Allow deletion of new resources created in this rollback when rollback fails\n</code></pre>","path":["Helm CheatSheet"],"tags":["CheatSheet","Helm"]},{"location":"kubernetes/helm/helmCheatSheet/#list-add-remove-and-update-repositories","level":3,"title":"List, Add, Remove, and Update Repositories","text":"<pre><code>helm repo add &lt;repo-name&gt; &lt;url&gt; # Add a repository from the internet:\nhelm repo list                  # List added chart repositories\nhelm repo update                # Update information of available charts locally from chart repositories\nhelm repo remove &lt;repo_name&gt;    # Remove one or more chart repositories\nhelm repo index &lt;DIR&gt;           # Read the current directory and generate an index file based on the charts found.\nhelm repo index &lt;DIR&gt; --merge   # Merge the generated index with an existing index file\nhelm search repo &lt;keyword&gt;      # Search repositories for a keyword in charts\nhelm search hub &lt;keyword&gt;       # Search for charts in the Artifact Hub or your own hub instance\n</code></pre>","path":["Helm CheatSheet"],"tags":["CheatSheet","Helm"]},{"location":"kubernetes/helm/helmCheatSheet/#helm-release-monitoring","level":3,"title":"Helm Release monitoring","text":"<pre><code>helm list                                 # Lists all of the releases for a specified namespace, uses current namespace context if namespace not specified\nhelm list --all                           # Show all releases without any filter applied, can use -a\nhelm list --all-namespaces                # List releases across all namespaces, we can use -A\nhelm list -l key1=value1,key2=value2      # Selector (label query) to filter on, supports '=', '==', and '!='\nhelm list --date                          # Sort by release date\nhelm list --deployed                      # Show deployed releases. If no other is specified, this will be automatically enabled\nhelm list --pending                       # Show pending releases\nhelm list --failed                        # Show failed releases\nhelm list --uninstalled                   # Show uninstalled releases (if 'helm uninstall --keep-history' was used)\nhelm list --superseded                    # Show superseded releases\nhelm list -o yaml                         # Prints the output in the specified format. Allowed values: table, json, yaml (default table)\nhelm status &lt;release&gt;                     # This command shows the status of a named release.\nhelm status &lt;release&gt; --revision &lt;number&gt; # if set, display the status of the named release with revision\nhelm history &lt;release&gt;                    # Historical revisions for a given release.\nhelm env                                  # Env prints out all the environment information in use by Helm.\n</code></pre>","path":["Helm CheatSheet"],"tags":["CheatSheet","Helm"]},{"location":"kubernetes/helm/helmCheatSheet/#download-release-information","level":3,"title":"Download Release Information","text":"<pre><code>helm get all &lt;release&gt;      # A human readable collection of information about the notes, hooks, supplied values, and generated manifest file of the given release.\nhelm get hooks &lt;release&gt;    # This command downloads hooks for a given release. Hooks are formatted in YAML and separated by the YAML '---\\n' separator.\nhelm get manifest &lt;release&gt; # A manifest is a YAML-encoded representation of the Kubernetes resources that were generated from this release's chart(s). If a chart is dependent on other charts, those resources will also be included in the manifest.\nhelm get notes &lt;release&gt;    # Shows notes provided by the chart of a named release.\nhelm get values &lt;release&gt;   # Downloads a values file for a given release. use -o to format output\n</code></pre>","path":["Helm CheatSheet"],"tags":["CheatSheet","Helm"]},{"location":"kubernetes/helm/helmCheatSheet/#plugin-management","level":3,"title":"Plugin Management","text":"<pre><code>helm plugin install &lt;path/url&gt; # Install plugins\nhelm plugin list               # View a list of all installed plugins\nhelm plugin update &lt;plugin&gt;    # Update plugins\nhelm plugin uninstall &lt;plugin&gt; # Uninstall a plugin\n</code></pre> ABBR <pre><code>abbr \"h\"=\"helm\"\nabbr \"hd\"=\"helm delete\"\nabbr \"hi\"=\"helm install\"\nabbr \"hs\"=\"helm status\"\nabbr \"hl\"=\"helm list\"\nabbr \"hla\"=\"helm list -A\"\nabbr \"hu\"=\"helm upgrade\"\nabbr \"hui\"=\"helm upgrade --install\"\nabbr \"hun\"=\"helm uninstall\"\n</code></pre>","path":["Helm CheatSheet"],"tags":["CheatSheet","Helm"]},{"location":"kubernetes/helm/helmCheatSheet/#recuperer-un-fichier-valuesyaml-dune-charts-helm","level":2,"title":"R√©cup√©rer un fichier values.yaml d'une charts helm","text":"<pre><code>helm show values cilium/cilium &gt; cilium-values.yaml\n</code></pre> <p>helm show values cilium &gt; test-values.yaml</p>","path":["Helm CheatSheet"],"tags":["CheatSheet","Helm"]},{"location":"kubernetes/helm/wrapper/","level":1,"title":"Wrapper","text":"Chart.yaml<pre><code>apiVersion: v2 #(1)!\nname: reloader-wrapper #(2)!\ndescription: Enterprise wrapper for Stakater Reloader with monitoring and security #(3)!\ntype: application #(4)!\nversion: 1.0.0 #(5)!\nappVersion: \"v1.4.11\" #(6)! \nmaintainers: #(7)!\n  - name: Mathias FELIX\n    email: mathias.felix@mathod.io\n\ndependencies:\n  - name: reloader #(8)!\n    version: 2.2.6 #(9)!\n    repository: https://stakater.github.io/stakater-charts #(10)!\n\nsources: \n  - https://github.com/stakater/Reloader #(11)!\n</code></pre> <ol> <li> <p>Obligatoire     Version de l'API Helm utilis√©e</p> <ul> <li>v2 = Helm 3 (introduit en 2019)</li> <li>v1 = Helm 2 (d√©pr√©ci√©)</li> </ul> </li> <li> <p>Obligatoire     Nom unique de ton chart</p> <ul> <li>Utilis√© pour identifier le chart dans les repos</li> <li>Doit √™tre en minuscules, avec tirets autoris√©s</li> </ul> </li> <li> <p>Optionnel     Description courte du chart</p> <ul> <li>Affich√©e dans helm search</li> <li>Explique ce que fait le chart</li> </ul> </li> <li> <p>Optionnel (d√©faut = application)     Type de chart</p> <ul> <li>application : D√©ploie une application</li> <li>library : Chart r√©utilisable (templates partag√©s, pas de d√©ploiement direct)</li> </ul> </li> <li> <p>Obligatoire     Version du chart (pas de l'application)     Suit le SemVer : MAJOR.MINOR.PATCH     Incr√©mente quand tu modifies le chart:</p> <ul> <li>MAJOR : Changements incompatibles (breaking changes)</li> <li>MINOR : Nouvelles fonctionnalit√©s (compatibles)</li> <li>PATCH : Corrections de bugs</li> </ul> <p>Exemple : 1.0.0 ‚Üí 1.1.0 (ajout ServiceMonitor) ‚Üí 2.0.0 (changement structure values)</p> </li> <li> <p>Optionnel     Version de l'application d√©ploy√©e</p> <ul> <li>Indique quelle version de Reloader sera install√©e</li> <li>Entre guillemets car peut contenir des lettres (v1.0.69)</li> <li>Informatif uniquement</li> </ul> </li> <li> <p>Optionnel     Liste des responsables du chart</p> <ul> <li>Contact en cas de probl√®me</li> <li>Peut avoir plusieurs mainteneurs</li> </ul> </li> <li> <p>Obligatoire     Nom du chart d√©pendant</p> <ul> <li>Doit correspondre au nom dans le repo Helm</li> </ul> </li> <li> <p>Obligatoire     Version exacte du chart √† utiliser</p> </li> <li> <p>Obligatoire     URL du repo Helm o√π trouver le chart     Peut aussi √™tre :</p> <ul> <li>file://../local-chart : Chart local</li> <li>oci://registry.example.com/charts : Registry OCI</li> </ul> </li> <li> <p>Optionnel     URL du projet principal</p> </li> </ol> <p>INFORMATION</p> <p>Le fichier Chart.yaml suit le sch√©ma officiel Helm, qui d√©finit les champs autoris√©s.</p> <p>BEST PRACTICE</p> <p>Utiliser un wrapper Helm chart centralise les configurations et √©vite de modifier chaque chart individuellement. Vous pouvez d√©finir des valeurs par d√©faut propres √† votre organisation et synchroniser automatiquement tous les clusters, simplifiant la maintenance et r√©duisant les erreurs.</p>","path":["Wrapper"],"tags":[]},{"location":"kubernetes/kind/","level":1,"title":"KIND","text":"","path":["KIND"],"tags":["Kind"]},{"location":"kubernetes/kind/#introduction","level":2,"title":"Introduction","text":"<p>Kind (Kubernetes in Docker) est un outil qui facilitent l'ex√©cution de clusters Kubernetes locaux en utilisant des container Docker ! Cette outils √† √©t√© cr√©er principalement pour tester Kubernetes. Il existe de multiples solution pour d√©ployer un cluster Kubernetes. Kubeadm, Kops, Minikube pour ne citer qu'eux ! Cependant la plus part de ces solutions sont parfois assez limit√© en terme de configuration, voir complexe √† mettre en place... La documentation de kind est facile √† comprendre, pour plus de d√©tails, r√©f√©rez vous √† ce¬†lien.</p>","path":["KIND"],"tags":["Kind"]},{"location":"kubernetes/kind/#objectifs","level":3,"title":"Objectifs","text":"<ul> <li>Apprendre √† installer kind</li> <li>Cr√©er un cluster single ou multi node</li> <li>D√©ployer une version sp√©cifique de Kubernetes</li> <li>Supprimer un Cluster single ou multi node</li> <li>Exporter les logs d'un cluster kind</li> <li>D√©ployer une application</li> </ul>","path":["KIND"],"tags":["Kind"]},{"location":"kubernetes/kind/#prerequis","level":3,"title":"Pr√©requis","text":"<ul> <li>Docker/Docker Desktop</li> </ul>","path":["KIND"],"tags":["Kind"]},{"location":"kubernetes/kind/#ma-configuration","level":3,"title":"Ma configuration","text":"<ul> <li>Debian 12</li> <li>kubectl</li> <li>Docker desktop 4.28.0</li> </ul>","path":["KIND"],"tags":["Kind"]},{"location":"kubernetes/kind/#installation-de-kind","level":2,"title":"Installation de Kind","text":"","path":["KIND"],"tags":["Kind"]},{"location":"kubernetes/kind/#prerequis_1","level":3,"title":"Pr√©requis","text":"<p>Le seul pr√©requis pour utiliser Kind est d'avoir Docker d'installer. Dans notre cas nous utiliserons Docker-Desktop sous Windows et nous le lieront a un WSL Debian. Pour les autres syst√®mes je vous invite √† suivre¬†la documentation officiel</p> <p>Sur Debian pour l'installation nous utiliserons les commandes suivante:</p> BrewRelease Binaries <pre><code>brew install kind\n</code></pre> <p>Info</p> <p>The following are community supported efforts. The kind maintainers are not involved in the creation of these packages, and the upstream community makes no claims on the validity, safety, or content of them.</p> <pre><code>[ $(uname -m) = x86_64 ] &amp;&amp; curl -Lo ./kind https://kind.sigs.k8s.io/dl/latest/kind-linux-amd64\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100    97  100    97    0     0    822      0 --:--:-- --:--:-- --:--:--   829\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100 6304k  100 6304k    0     0  6132k      0  0:00:01  0:00:01 --:--:-- 6132k\nchmod +x ./kind\nsudo mv ./kind /usr/local/bin/kind\n</code></pre> <p>Afin de s'assurer que Kind est bien installer nous pouvons utiliser la commande¬†<code>kind version</code>¬†pour s'assurer de son bon fonctionnement.</p> <pre><code>kind version\nkind v0.30.0 go1.25.4 linux/amd64\n</code></pre>","path":["KIND"],"tags":["Kind"]},{"location":"kubernetes/kind/#creation-dun-cluster","level":2,"title":"Cr√©ation d'un cluster","text":"Single NodeMulti Node <p>Aucune configuration n√©c√©ssaire pour cr√©er un cluster single node juste la commande suivante:</p> <pre><code>kind create cluster\nCreating cluster \"kind\" ...\n ‚úì Ensuring node image (kindest/node:v1.34.0) üñº\n ‚úì Preparing nodes üì¶\n ‚úì Writing configuration üìú\n ‚úì Starting control-plane üïπ\n ‚úì Installing CNI üîå\n ‚úì Installing StorageClass üíæ\nSet kubectl context to \"kind-kind\"\nYou can now use your cluster with:\n\nkubectl cluster-info --context kind-kind\n\nHave a question, bug, or feature request? Let us know! https://kind.sigs.k8s.io/#community üôÇ\n</code></pre> <p>La cr√©ation du cluster ce feras avec une image pr√©-built h√©berger sur¬†Docker Hub</p> <p>Vous switcher automatiqment sur le context du cluster que vous venez de cr√©er. Vous pouvez utiliser la commande¬†<code>kubectl get nodes</code>¬†pour v√©rifier que votre cluster fonctionne correctement.</p> <pre><code>kubectl get nodes\nNAME                 STATUS   ROLES           AGE     VERSION\nkind-control-plane   Ready    control-plane   5m28s   v1.34.0\n</code></pre> <p>Pour un cluster avec plusieurs nodes un fichier de configuration comme suit seras n√©c√©ssaire</p> multiNode.yaml<pre><code>apiVersion: kind.x-k8s.io/v1alpha4\nkind: Cluster\nnodes:\n- role: control-plane\n- role: control-plane\n- role: control-plane\n- role: worker\n- role: worker\n- role: worker\n</code></pre> <p>Dans ce fichier de configuration, nous cr√©ons un cluster multi-nodes avec 3 control-plane et 3 worker</p> <p>Pour la commande on ajoute juste le flag <code>--config</code> et le chemin de notre configuration</p> <pre><code>kind create cluster --config multiNode.yaml\nCreating cluster \"kind\" ...\n ‚úì Ensuring node image (kindest/node:v1.34.0) üñº\n ‚úì Preparing nodes üì¶ üì¶ üì¶ üì¶ üì¶ üì¶\n ‚úì Configuring the external load balancer ‚öñ\n ‚úì Writing configuration üìú\n ‚úì Starting control-plane üïπ\n ‚úì Installing CNI üîå\n ‚úì Installing StorageClass üíæ\n ‚úì Joining more control-plane nodes üéÆ\n ‚úì Joining worker nodes üöú\nSet kubectl context to \"kind-kind\"\nYou can now use your cluster with:\n\nkubectl cluster-info --context kind-kind\n\nNot sure what to do next? üòÖ  Check out https://kind.sigs.k8s.io/docs/user/quick-start/\n</code></pre> <p>La cr√©ation du cluster ce feras avec une image pr√©-built h√©berger sur¬†Docker Hub</p> <p>Vous switcher automatiqment sur le context du cluster que vous venez de cr√©er. Vous pouvez utiliser la commande¬†<code>kubectl get nodes</code>¬†pour v√©rifier que votre cluster fonctionne correctement.</p> <pre><code>kubectl get nodes\nNAME                  STATUS     ROLES           AGE   VERSION\nkind-control-plane    Ready      control-plane   65s   v1.34.0\nkind-control-plane2   Ready      control-plane   65s   v1.34.0\nkind-control-plane3   Ready      control-plane   65s   v1.34.0\nkind-worker           Ready      &lt;none&gt;          65s   v1.34.0\nkind-worker2          Ready      &lt;none&gt;          65s   v1.34.0\nkind-worker3          Ready      &lt;none&gt;          65s   v1.34.0\n</code></pre>","path":["KIND"],"tags":["Kind"]},{"location":"kubernetes/kind/#cluster-configurations","level":2,"title":"Cluster Configurations","text":"<p>Par d√©faut le fichier de configuration de votre cluster se trouveras dans ${HOME}/.kube/config</p> <pre><code>cat ${HOME}/.kube/config\napiVersion: v1\nclusters:\n- cluster:\n    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUMvakNDQWVhZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJME1ESXhNREUwTXpjd09Wb1hEVE0wTURJd056RTBNemN3T1Zvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTWZmCjk5YmxnV1FFN1RBWXhSNmd5Yk9PbG0rRFRvdjBMTklUSG5ydDdkTzVydTljK3FvLzNqQzZWcTVseTByZ21odUIKTXgvVVlvd09MWWJrZ21kWkpNS0NENjh6bENFSVRNSHB4Q2xlaXYrdVdkVCtTYWJ2MkkyM2wvVjdsUjFVTzRQRwpJdWNLaFZNT0JGQ1Axci8yMDVPQVJlWDNtSjg0SER2UW03K2RKaWdtVUhVYk9DallidXIzeW9xcldxdXZ5d2hFCmNPaW1aN1hzMVc4WjBpTFF2L2pkYXhzMThqYWMzci9VbUJRa3FCV2ozSUJ2S3o3NHVPWVYxUWNoa0ZEMWJMVkkKSkZKMWc3UmUxazNHWkpUUzRHeEo0RWpoR3NyV01vRmlRT21uWTZneFA5TlpvZ2k2bkNRaWQ1Q0ZteUJnM1ZocAovMkRWcHUrVGNtUktxTlhPMEEwQ0F3RUFBYU5aTUZjd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0hRWURWUjBPQkJZRUZCcmVlaHFVOHpkNjlTSXdYaC9FQ1Jwc05HcURNQlVHQTFVZEVRUU8KTUF5Q0NtdDFZbVZ5Ym1WMFpYTXdEUVlKS29aSWh2Y05BUUVMQlFBRGdnRUJBQ3hrT2J0L3hXayt2dTdGMTk1UQpNTVpYUWR0alUvNjVidHp1MDZWVU9icEtOZVA0Qzkwb0VNWDBpbzJKbVhlelE2TTJnUGYyb2Z4MEdEOUE1dWxtCjYxWXFmR0ViQ1FzNytRaVFyc1ppbmhDMXI3L3Q1OUpwbkp4VCtVV1NwVkhUSmNnSlFQbXI3QlRDd2tkR3ZscEYKdFgxQTBTUGNPdVJUM2RRM3EvVkJ0b3BGMWhiWUN5S2cyM0t1YUZJZjZmSlY1K2hSNWgreENTNGpUT3Z6RVNqWgprZVhxTmxaM0ZWS0JTQzVZeWhORmhMelNKT3U1ZUJhTU9YVGl2aVg5ckVRcUFRYmlNcWMyVGZlekZWV3VUL0M3Cnc5R0FhamJNTnY4UVZQNU9jc0RDTGRIS1BRTDRpSEUxMHpEVllvRkwyb2JHQXFJMVNJamt0RTVTMFlNTS9uYlcKSFlrPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==\n    server: https://127.0.0.1:40299\n  name: kind-kind\ncontexts:\n- context:\n    cluster: kind-kind\n    user: kind-kind\n  name: kind-kind\ncurrent-context: kind-kind\nkind: Config\npreferences: {}\nusers:\n- name: kind-kind\n  user:\n    client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURJVENDQWdtZ0F3SUJBZ0lJRnpiYUxLcDl6MlV3RFFZSktvWklodmNOQVFFTEJRQXdGVEVUTUJFR0ExVUUKQXhNS2EzVmlaWEp1WlhSbGN6QWVGdzB5TkRBeU1UQXhORE0zTURsYUZ3MHlOVEF5TURreE5ETTNNVEZhTURReApGekFWQmdOVkJBb1REbk41YzNSbGJUcHRZWE4wWlhKek1Sa3dGd1lEVlFRREV4QnJkV0psY201bGRHVnpMV0ZrCmJXbHVNSUlCSWpBTkJna3Foa2lHOXcwQkFRRUZBQU9DQVE4QU1JSUJDZ0tDQVFFQXRHU2R2V0liQ2Y0SkpqVXoKNTM2VEh0WGRSeFN6bWVmYWo5L2kvU0xBQ3B0dCtISTBLV2ZOby9SQjB6UmxyMWFaN2UxclRLQUpHNFZLRDhydgpkak9CdDZPZUMwdElUVUQxZndJUGlTbUVRVURxMUhuRURHMnI2a0J3ek82Z0ZEcGxjbFpyV3p0WGRmdXZweXJICkVqOTVLZ0pDS0tmbUIzZFBlcjk5cTVRdU1URGw0MXBZNUhJWUswL3FNTFRsVVFUSnVlQVdqOHViaGVURDBUZDAKdFRBNElIZlA2UkJGWmtGUlJWeUh3N3BsdU4yNlJqOW9QYXVPckNUcWhTdGxtb2NKM1o2dXV4a1BPZkg1YXJ5TwpIbFNNWnJZWDdLVGk0Wmx6bTl1Ymg0b1V2NXhuZHJOajNabWV1S1hubWx3SVNsY0lWSnZTNE5SYWhBOC92TEF2CklPbk5Jd0lEQVFBQm8xWXdWREFPQmdOVkhROEJBZjhFQkFNQ0JhQXdFd1lEVlIwbEJBd3dDZ1lJS3dZQkJRVUgKQXdJd0RBWURWUjBUQVFIL0JBSXdBREFmQmdOVkhTTUVHREFXZ0JRYTNub2FsUE0zZXZVaU1GNGZ4QWthYkRScQpnekFOQmdrcWhraUc5dzBCQVFzRkFBT0NBUUVBVFhJM3BtenZvZDFTR1BhcGdCTWsyQ3g0OU4vaE92OUgvanNICkc5MUV0VCtYR2FMM1I3WWhIMG8vMm41MVcrTncxazdiY3l4NkJtT0IzNW9OQlJJNHpZS3lOYWZuN0x3UjRCSVAKb1QvZzZaZUJ2bTNhWC9JeDNCTUt1eHFHVmowZHZXREFWRDB1WGdiVE0zSHpOcXc1dnVNZTNyNXNWRmxoWDFoZgpPa2ttTFZXeTZuYXVvZzUxb0hadHNnVk9jNkl4M3lDTEJkeis0RDRSQVVtSTYwdlc4NmN3SUE3UE4yQlNhZlNiClJWMzZrQWduNGFTMmlmQXZCRDNDeFF1N0swYzYzNUx3aXYvR08vMkRSakx0NzV0ZVROdGQ4QXBIKzRsMFQzelMKRFh1VW1qcVZLRHZQL0lYbXVUTStZWXBkdzVZdzg1QXQ4UlhBY012dGQ1c2xSSUo2a3c9PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==\n    client-key-data: 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\n</code></pre>","path":["KIND"],"tags":["Kind"]},{"location":"kubernetes/kind/#utiliser-une-image-node-differente","level":2,"title":"Utiliser une image node diff√©rente","text":"<p>L'utilisation d'une image diff√©rente vous permet de changer la version du cluster Kubernetes cr√©√©. Chaque version de Kind supporte une liste sp√©cifique des versions de Kubernetes, vous pouvez voir la liste des versions support√©es de Kubernetes depuis cette¬†page.</p> <pre><code>Images pre-built for this release:\n\nv1.34.0: kindest/node:v1.34.0@sha256:7416a61b42b1662ca6ca89f02028ac133a309a2a30ba309614e8ec94d976dc5a\nv1.33.4: kindest/node:v1.33.4@sha256:25a6018e48dfcaee478f4a59af81157a437f15e6e140bf103f85a2e7cd0cbbf2\nv1.32.8: kindest/node:v1.32.8@sha256:abd489f042d2b644e2d033f5c2d900bc707798d075e8186cb65e3f1367a9d5a1\nv1.31.12: kindest/node:v1.31.12@sha256:0f5cc49c5e73c0c2bb6e2df56e7df189240d83cf94edfa30946482eb08ec57d2\n</code></pre> <p>Pour sp√©cifier une autre image, utilisez l'option¬†<code>--image</code></p> <pre><code>kind create cluster --image kindest/node:v1.33.4@sha256:25a6018e48dfcaee478f4a59af81157a437f15e6e140bf103f85a2e7cd0cbbf2\n</code></pre>","path":["KIND"],"tags":["Kind"]},{"location":"kubernetes/kind/#changer-le-nom-context-de-votre-cluster","level":2,"title":"Changer le nom context de votre cluster","text":"<p>Par d√©faut le context de votre cluster sera nomm√©¬†<code>kind</code>. Vous pouvez utiliser le flag¬†<code>--name</code>¬†pour assigner un nom diff√©rent.</p> <pre><code>kind create cluster --name mathod\nCreating cluster \"mathod\" ...\n ‚úì Ensuring node image (kindest/node:v1.34.0) üñº\n ‚úì Preparing nodes üì¶\n ‚úì Writing configuration üìú\n ‚úì Starting control-plane üïπ\n ‚úì Installing CNI üîå\n ‚úì Installing StorageClass üíæ\nSet kubectl context to \"kind-mathod\"\nYou can now use your cluster with:\n\nkubectl cluster-info --context kind-mathod\n\nThanks for using kind! üòä\n</code></pre> <p>La command¬†<code>kubectl config get-contexts</code>¬†permet de lister les clusters et indiquer le context en cours d'utilisation</p> <pre><code>‚ùØ kubectl config get-contexts\nCURRENT   NAME           CLUSTER        AUTHINFO       NAMESPACE\n          kind-kind      kind-kind      kind-kind      default\n*         kind-mathod    kind-mathod    kind-mathod\n</code></pre> <p>Pour passer d'un cluster √† un autre, vous pouvez utiliser¬†<code>kubectl config use-context &lt;cluster-name&gt;</code>.</p> Kubectx <ul> <li>kubectx permet de changer rapidement de contexte.</li> <li>kubens permet de basculer facilement entre les namespaces d'un cluster. (int√©grer √† kubectx)</li> </ul> <pre><code>brew install kubectx\n</code></pre>","path":["KIND"],"tags":["Kind"]},{"location":"kubernetes/kind/#interagir-avec-le-cluster","level":2,"title":"Interagir avec le cluster","text":"<p>Apr√®s avoir cr√©√© un cluster, vous pouvez utiliser kubectl pour interagir avec ce dernier.</p> <pre><code>kubectl cluster-info\nKubernetes control plane is running at https://127.0.0.1:32991\nCoreDNS is running at https://127.0.0.1:32991/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n</code></pre> <p>Vous pouvez √©galement lister tous les containers Kind avec la commande suivante:</p> <pre><code>docker ps\nCONTAINER ID   IMAGE                                COMMAND                  CREATED         STATUS         PORTS                       NAMES\n973259b15c3b   kindest/haproxy:v20230606-42a2262b   \"haproxy -W -db -f /‚Ä¶\"   8 minutes ago   Up 8 minutes   127.0.0.1:39103-&gt;6443/tcp   kind-external-load-balancer\n6120719d8b42   kindest/node:v1.34.0                 \"/usr/local/bin/entr‚Ä¶\"   8 minutes ago   Up 8 minutes   127.0.0.1:45665-&gt;6443/tcp   kind-control-plane2\nfd2be64055ab   kindest/node:v1.34.0                 \"/usr/local/bin/entr‚Ä¶\"   8 minutes ago   Up 8 minutes   127.0.0.1:33845-&gt;6443/tcp   kind-control-plane\nf14296cc3f77   kindest/node:v1.34.0                 \"/usr/local/bin/entr‚Ä¶\"   8 minutes ago   Up 8 minutes   127.0.0.1:40977-&gt;6443/tcp   kind-control-plane3\n63ba6aa113b6   kindest/node:v1.34.0                 \"/usr/local/bin/entr‚Ä¶\"   8 minutes ago   Up 8 minutes                               kind-worker3\n0098259cc8ce   kindest/node:v1.34.0                 \"/usr/local/bin/entr‚Ä¶\"   8 minutes ago   Up 8 minutes                               kind-worker2\na4d8e4971eab   kindest/node:v1.34.0                 \"/usr/local/bin/entr‚Ä¶\"   8 minutes ago   Up 8 minutes                               kind-worker\n</code></pre>","path":["KIND"],"tags":["Kind"]},{"location":"kubernetes/kind/#suppression-dun-cluster","level":2,"title":"Suppression d'un cluster","text":"<p>Pour supprimer un cluster nous utiliserons la commande¬†<code>kind delete clusters &lt;clusterName&gt;</code>.  </p> <pre><code>‚ùØ kind delete clusters kind\nDeleted nodes: [\"kind-external-load-balancer\" \"kind-control-plane2\" \"kind-control-plane\" \"kind-control-plane3\" \"kind-worker3\" \"kind-worker2\" \"kind-worker\"]\nDeleted clusters: [\"kind\"]\n</code></pre>","path":["KIND"],"tags":["Kind"]},{"location":"kubernetes/kind/#dynamic-volume-provisioning","level":2,"title":"Dynamic Volume Provisioning","text":"<p>Le provisionnement dynamique des volumes dans Kubernetes est un m√©canisme qui permet de cr√©er des volumes de stockage √† la demande. Pour ce faire, le cluster Kubernetes utilise le concept de classe de stockage, qui fait abstraction des d√©tails du stockage sous-jacent. Les administrateurs du cluster doivent appeler manuellement leur fournisseur de cloud ou de stockage, puis cr√©er des objets Persistent Volume dans Kubernetes sans provisionnement dynamique.</p> <p>Une Storage Class est par d√©faut pr√©configur√©e lorsque vous cr√©ez le cluster Kind. Pour voir la liste des classes de stockage disponibles, utilisez la commande kubectl get sc.</p> <pre><code>NAME                 PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE\nstandard (default)   rancher.io/local-path   Delete          WaitForFirstConsumer   false                  11d\n</code></pre> <p>WaitforFirstConsumer indique que le¬†pvc¬†(persistent volume claim) ne sera pas li√© tant qu'il ne sera pas attach√© √† un pod.</p> <p>Nous allons maintenant cr√©ez un fichier PVC en utilisant le code ci-dessous.</p> pvc.yaml<pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: pvc-test\nspec:\n  storageClassName: standard\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 500Mi\n</code></pre> <p>Enregistrez ce fichier sous le nom de¬†<code>pvc.yaml</code>¬†et ex√©cutez la commande suivante pour cr√©er une¬†demande de volume persistant¬†√† partir de ce fichier pvc. yaml :</p> <pre><code>kubectl create -f pvc.yaml\n</code></pre> <p>Create another yaml file for the busybox pod by using the given below code and save it as¬†<code>busybox.yaml</code>.</p> pod.yaml<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\nspec:\n  volumes:\n  - name: host-volume\n    persistentVolumeClaim:\n      claimName: pvc-test\n  containers:\n  - image: busybox\n    name: busybox\n    command: [\"/bin/sh\"]\n    args: [\"-c\", \"sleep 600\"]\n    volumeMounts:\n    - name: host-volume\n      mountPath: /mydata\n</code></pre> <p>Run the following command to create a pod:</p> <pre><code>kubectl create-f busybox.yaml\n</code></pre> <p>Run the following commands to validate the persistent volume or persistent volume claim created, and to check whether the pod is running or not:</p> <pre><code>kubectl get pv,pvc\n\nkubectl get pods\n</code></pre> <p></p> <p></p> <p>Now we've actually built a multi-node pvc-backed cluster and mounted it on busybox.</p> <p>You must expose your service after it has been deployed to Kubernetes so that your users can access it. The cluster can be accessed from outside in three ways: ingress, load balancer, and node port.</p>","path":["KIND"],"tags":["Kind"]},{"location":"kubernetes/kind/#exporter-les-logs-dun-cluster","level":2,"title":"Exporter les logs d'un cluster","text":"<p>Vous pouvez exporter les logs de Kind au besoin avec la commande suivante¬†<code>kind export logs</code>. Encore une fois vous pouvez utiliser l'option¬†<code>--name &lt;clusterName&gt;</code>.</p> <pre><code># Par d√©faut si aucun nom na √©t√© sp√©cifi√©\nkind export logs\nExporting logs for cluster \"kind\" to:\n/tmp/606883079\n\nOU\n\n# Si un nom √† √©t√© sp√©cifi√© avec la commande \"kind create cluster --name my-little-kubernetes\"\nkind export logs --name my-little-kubernetes\nExporting logs for cluster \"my-little-kubernetes\" to:\n/tmp/248121554\n</code></pre> <p>La structure des logs ressemblera plus ou moins √† ceci:</p> <pre><code>.\n‚îú‚îÄ‚îÄ docker-info.txt\n‚îú‚îÄ‚îÄ kind-version.txt\n‚îî‚îÄ‚îÄ my-little-kubernetes-control-plane\n    ‚îú‚îÄ‚îÄ alternatives.log\n    ‚îú‚îÄ‚îÄ containerd.log\n    ‚îú‚îÄ‚îÄ containers\n    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ coredns-5d78c9869d-m6zfl_kube-system_coredns-86169c2cf303ff450b9c634428486aad040bbc78c7ba386f15c9c0206e81cf2c.log\n    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ coredns-5d78c9869d-vlv59_kube-system_coredns-d66e4fa61d209f45a0b8d6394a363722058b98d62e7fb3fb08ec9300b399b8f8.log\n    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ etcd-my-little-kubernetes-control-plane_kube-system_etcd-656a77b115a3659616def413134537a25de8cdc1bebc804361136ad86036bf51.log\n    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ kindnet-n6jll_kube-system_kindnet-cni-32a1650c6e6ec313957bfab06a1f782c48ca2d21093a4f9cfa92d75955442ab7.log\n    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ kube-apiserver-my-little-kubernetes-control-plane_kube-system_kube-apiserver-20e26af6ee88986ab750a3d11fdf3cca10e40763e9c6a489156a672f9603983a.log\n    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ kube-controller-manager-my-little-kubernetes-control-plane_kube-system_kube-controller-manager-9c0f721140920ae0475a8dd52ca95240d764209f7509ff86b0901a623c801686.log\n    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ kube-proxy-wmvm5_kube-system_kube-proxy-13f3024cf7ef0eb521cc643a263120520570ac2c2b153d7c285942e17fbff396.log\n    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ kube-scheduler-my-little-kubernetes-control-plane_kube-system_kube-scheduler-70e72a8b0441293f5851125efa9392b2a376415565a05231450fc03afecc2668.log\n    ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ local-path-provisioner-6bc4bddd6b-km6m4_local-path-storage_local-path-provisioner-e4bf976fb7fcf3c1982817ea5b5de661790dbbe329692fe8b088e8fb316c137d.log\n    ‚îú‚îÄ‚îÄ images.log\n    ‚îú‚îÄ‚îÄ inspect.json\n    ‚îú‚îÄ‚îÄ journal.log\n    ‚îú‚îÄ‚îÄ kubelet.log\n    ‚îú‚îÄ‚îÄ kubernetes-version.txt\n    ‚îú‚îÄ‚îÄ pods\n    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ kube-system_coredns-5d78c9869d-m6zfl_db77db26-4fd0-48f1-bbe7-2d3fe81a76d6\n    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ coredns\n    ‚îÇ¬†¬† ‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ 0.log\n    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ kube-system_coredns-5d78c9869d-vlv59_9cc23a37-f541-4791-af13-8d27716617e1\n    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ coredns\n    ‚îÇ¬†¬† ‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ 0.log\n    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ kube-system_etcd-my-little-kubernetes-control-plane_f5d05561aae79e56e905c4d1a193c022\n    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ etcd\n    ‚îÇ¬†¬† ‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ 0.log\n    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ kube-system_kindnet-n6jll_c9569b3f-c56e-4f5f-9715-9fb1031a215a\n    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ kindnet-cni\n    ‚îÇ¬†¬† ‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ 0.log\n    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ kube-system_kube-apiserver-my-little-kubernetes-control-plane_ac1c887e1485d5b8bc5ca97574152ce8\n    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ kube-apiserver\n    ‚îÇ¬†¬† ‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ 0.log\n    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ kube-system_kube-controller-manager-my-little-kubernetes-control-plane_e154e425b891c8f0922cb29cbb3f2257\n    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ kube-controller-manager\n    ‚îÇ¬†¬† ‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ 0.log\n    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ kube-system_kube-proxy-wmvm5_8ce1cae9-0755-423a-a73c-045d5d4745ff\n    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ kube-proxy\n    ‚îÇ¬†¬† ‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ 0.log\n    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ kube-system_kube-scheduler-my-little-kubernetes-control-plane_bf4f69d46c370c6dd8446303e1ed7194\n    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ kube-scheduler\n    ‚îÇ¬†¬† ‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ 0.log\n    ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ local-path-storage_local-path-provisioner-6bc4bddd6b-km6m4_63c30f86-17b3-4278-ba5d-df1bf83a6d8f\n    ‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ local-path-provisioner\n    ‚îÇ¬†¬†         ‚îî‚îÄ‚îÄ 0.log\n    ‚îî‚îÄ‚îÄ serial.log\n\n21 directories, 28 files\n</code></pre>","path":["KIND"],"tags":["Kind"]},{"location":"kubernetes/kind/#deploiement-dapplication","level":2,"title":"D√©ploiement d'application","text":"<p>You can use the kubectl command-line tool to deploy an application to your Kind cluster. Create a deployment definition file that contains the specifics of your application. An example deployment definition file for a simple Nginx web server is provided below:</p> deployment.yaml<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hello-world-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hello-world\n  template:\n    metadata:\n      labels:\n        app: hello-world\n    spec:\n      containers:\n      - name: hello-world-container\n        image: nginx:latest\n        ports:\n        - containerPort: 80\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 80\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 80\n        startupProbe:\n          httpGet:\n            path: /\n            port: 80\n          failureThreshold: 30\n          periodSeconds: 10\n        resources:\n          limits:\n            cpu: 200m\n            memory: 256Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n</code></pre> <p>Save this file as¬†<code>nginx-deployment.yaml</code>¬†and then run the following command to create the deployment:</p> <pre><code>kubectl apply -f nginx-deployment.yaml\n</code></pre> <p>This command will create a deployment with three Nginx web server replicas.</p> <p>To connect to the web server, you must first create a service that exposes the deployment. The following service definition file can be used to create a service:</p> service.yaml<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-service\nspec:\n  selector:\n    app: nginx\n  ports:\n  - name: http\n    port: 80\n    targetPort: 80\n  type: ClusterIP\n</code></pre> <p>Save this file as¬†<code>nginx-service.yaml</code>¬†and then run the following command to create the service:</p> <pre><code>kubectl apply -f nginx-service.yaml\n</code></pre> <p>This command will create a ClusterIP service, which will expose the Nginx web server deployment.</p> <p>To obtain the IP address of the service, use the¬†<code>kubectl get services</code>¬†command. You can access the Nginx web server once you have the IP address by opening a web browser and navigating to¬†<code>http://&lt;service-ip&gt;:80</code>.</p>","path":["KIND"],"tags":["Kind"]},{"location":"kubernetes/kind/#conclusion","level":2,"title":"Conclusion","text":"<p>Plus d'excuse possible, vous savez maintenant d√©ployer et configurer un cluster Kubernetes single ou multi-nodes via Kind afin d'y d√©ployer toutes sorte d'applications ! Je ne peux que vous conseiller de poursuivre avec l'installation de Helm pour d√©ployer Crossplane, Argo CD et Gitlab.</p>","path":["KIND"],"tags":["Kind"]},{"location":"kubernetes/kind/kindCheatSheet/","level":1,"title":"Kind cheat sheet","text":"<p>Kind cheatsheet featuring all the necessary commands required to deploy a kubernetes cluster through Kind.</p> Create a cluster<pre><code>kind create cluster\n</code></pre> Use a custom name for the cluster<pre><code>kind create cluster --name=clusterName\n</code></pre> Create a cluster with a custom node image<pre><code>kind create cluster --image=image\n</code></pre> Use a custom configuration for the cluster<pre><code>kind create cluster --config=cluster.yaml\n</code></pre> When creating a cluster, assign a waiting time for the control plane to go up<pre><code>kind create cluster --wait &lt;timeInterval&gt;\n</code></pre> View the list of the running clusters<pre><code>kind get clusters\n</code></pre> View information about a cluster<pre><code>kubectl cluster-info --context kind-clusterName\n</code></pre> Export logs<pre><code>kind export logs\n</code></pre> Delete a cluster<pre><code>kind delete cluster --name clusterName\n</code></pre> ABBR <pre><code>abbr \"kicc\"=\"kind create cluster\"\nabbr \"kiccn\"=\"kind create cluster --name %\"\nabbr \"kiccc\"=\"kind create cluster --config %\"\nabbr \"kigc\"=\"kind get clusters\"\nabbr \"kign\"=\"kind get nodes\"\nabbr \"kidc\"=\"kind delete cluster\"\nabbr \"kidcn\"=\"kind delete cluster --name %\"\nabbr \"kidca\"=\"kind delete clusters -A\"\nabbr \"kigk\"=\"kind get kubeconfig\"\n</code></pre>","path":["Kind CheatSheet"],"tags":["CheatSheet","Kind"]},{"location":"kubernetes/resources/namespaces/","level":1,"title":"Namespaces","text":"<p>Kubernetes Namespaces are essentially labels that partition a cluster into smaller, distinct segments. They allow you to organize your resources into groups that reflect different projects, teams, or environments within the same cluster. Imagine a cluster as a big office building: without any signs or room numbers, finding the right office would be a challenge. Namespaces act as these signs, guiding you to the right room ‚Äî the right resource ‚Äî quickly and efficiently.</p>","path":["KUBERNETES","RESOURCES","Namespaces"],"tags":["Kubernetes"]},{"location":"kubernetes/resources/namespaces/#why-use-namespaces","level":2,"title":"Why Use Namespaces?","text":"<ol> <li>Organization: Namespaces keep your cluster resources well-organized and manageable. This is particularly useful in environments where multiple teams or projects share the same Kubernetes cluster.</li> <li>Resource Management: They enable fine-grained control over resources. For example, you can set quotas on CPU and memory usage on a per-Namespace basis, preventing one part of your cluster from hogging all the resources.</li> <li>Access Control: Namespaces work hand in hand with Kubernetes‚Äô Role-Based Access Control (RBAC) system, allowing administrators to restrict user permissions within specific Namespaces.</li> </ol>","path":["KUBERNETES","RESOURCES","Namespaces"],"tags":["Kubernetes"]},{"location":"kubernetes/resources/namespaces/#a-closer-look-at-namespaces","level":2,"title":"A Closer Look at Namespaces","text":"<p>Kubernetes starts with four initial Namespaces:</p> <ul> <li>Default: The starting point for objects with no other Namespace.</li> <li>Kube-system: This Namespace contains objects created by the Kubernetes system itself, such as system processes.</li> <li>Kube-public: This is where public information resides. It‚Äôs readable by all users and used for special purposes, like the cluster discovery.</li> <li>Kube-node-lease: It holds lease objects that ensure node heartbeats. This helps the Kubernetes scheduler make better decisions.</li> </ul>","path":["KUBERNETES","RESOURCES","Namespaces"],"tags":["Kubernetes"]},{"location":"kubernetes/resources/namespaces/#creating-your-own-namespace","level":2,"title":"Creating Your Own Namespace","text":"<p>Creating a Namespace is straightforward.</p> ImperativeDeclarative <p>You can do it with a simple command like: <pre><code>kubectl create namespace my-namespace\n</code></pre></p> <p>Or, you can create it using a YAML file, which might look something like this: namespace.yaml<pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: my-namespace\n</code></pre></p> <p>Then, apply it with:</p> <pre><code>kubectl apply -f my-namespace.yaml\n</code></pre>","path":["KUBERNETES","RESOURCES","Namespaces"],"tags":["Kubernetes"]},{"location":"kubernetes/resources/namespaces/#example-a-multi-team-cluster","level":2,"title":"Example: A Multi-Team Cluster","text":"<p>Let‚Äôs say you‚Äôre managing a cluster for an organization with three teams: Development, Staging, and Production. Without Namespaces, managing access and resources for each team would be chaotic. By creating a Namespace for each team, you can easily control who has access to what and set resource limits to prevent any team from using more than their fair share.</p> <ul> <li>Development Team: Works on new features and bug fixes.</li> <li>Staging Team: Handles testing and quality assurance of new releases.</li> <li>Production Team: Manages the live application serving real users.</li> </ul>","path":["KUBERNETES","RESOURCES","Namespaces"],"tags":["Kubernetes"]},{"location":"kubernetes/resources/namespaces/#step-1-creating-namespaces-for-each-team","level":3,"title":"Step 1: Creating Namespaces for Each Team","text":"<p>First, we create a Namespace for each environment. This separation allows each team to work independently within the cluster, without interfering with each other‚Äôs resources.</p> <pre><code>kubectl create namespace development\nkubectl create namespace staging\nkubectl create namespace production\n</code></pre> <p>These commands set up the basic organisational structure within your Kubernetes cluster, mirroring the organisation‚Äôs workflow.</p>","path":["KUBERNETES","RESOURCES","Namespaces"],"tags":["Kubernetes"]},{"location":"kubernetes/resources/namespaces/#step-2-deploying-applications-in-their-respective-namespaces","level":3,"title":"Step 2: Deploying Applications in Their Respective Namespaces","text":"<p>Each team deploys their version of the application within their designated Namespace. For instance, the Development team deploys the latest version of the app for testing new features:</p> <pre><code>kubectl apply -f development-app.yaml -n development\n</code></pre> <p>Similarly, the Staging and Production teams deploy their versions in the <code>staging</code> and <code>production</code> Namespaces, respectively. This ensures that the same application, at different stages of its lifecycle, can coexist without conflict in the same cluster.</p>","path":["KUBERNETES","RESOURCES","Namespaces"],"tags":["Kubernetes"]},{"location":"kubernetes/resources/namespaces/#step-3-setting-resource-quotas","level":3,"title":"Step 3: Setting Resource Quotas","text":"<p>To prevent any team from overconsuming resources and affecting the others, you set resource quotas for each Namespace. For example, you might allocate more resources to Production since it‚Äôs critical to keep the live application running smoothly:</p> <pre><code>apiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: production-quota\n  namespace: production\nspec:\n  hard:\n    requests.cpu: \"4\"\n    requests.memory: 8Gi\n    limits.cpu: \"8\"\n    limits.memory: 16Gi\n</code></pre> <p>This YAML file defines a quota for the Production Namespace, limiting it to specific CPU and memory usage. Similar quotas can be defined for Development and Staging, perhaps with lower resource allocations.</p>","path":["KUBERNETES","RESOURCES","Namespaces"],"tags":["Kubernetes"]},{"location":"kubernetes/resources/namespaces/#step-4-implementing-access-control","level":3,"title":"Step 4: Implementing Access Control","text":"<p>Finally, you use Kubernetes‚Äô Role-Based Access Control (RBAC) to give each team access only to their respective Namespace. This prevents unauthorized access to sensitive environments, especially Production. For instance, you might create roles that allow the Development team to deploy and manage resources in the <code>development</code> Namespace but not in <code>staging</code> or <code>production</code>.</p>","path":["KUBERNETES","RESOURCES","Namespaces"],"tags":["Kubernetes"]},{"location":"kubernetes/resources/namespaces/#tips-for-managing-namespaces","level":2,"title":"Tips for Managing Namespaces","text":"<ul> <li>Naming Conventions: Establish clear naming conventions for your Namespaces. This will make them easier to manage as your cluster grows.</li> <li>Resource Quotas: Use resource quotas to prevent any one Namespace from consuming too much of the cluster‚Äôs resources.</li> <li>Labels and Annotations: Use labels and annotations to add metadata to your Namespaces. This can help with organizing and managing resources at a granular level.</li> </ul> <p>By effectively utilizing Namespaces, you can ensure your cluster remains tidy, just like a well-organized library, making it easier for teams to find and utilize the resources they need. Whether you‚Äôre managing a cluster for a small team or a large enterprise, mastering Namespaces is a key step toward efficient Kubernetes management.</p> <p>Imagine you‚Äôre the manager of a big office building. You‚Äôve got multiple departments (like Finance, HR, and IT), and each department has its own team leaders and sub-teams. You wouldn‚Äôt just let everyone roam around wherever they please, right? You‚Äôd set up some boundaries and give permissions according to roles. That‚Äôs exactly what Kubernetes does with namespaces ‚Äî but what happens when you need namespaces inside namespaces? Enter Hierarchical Namespaces.</p> <p>Hierarchical Namespaces will allow you to create a tree-like structure of namespaces, making management much easier when dealing with complex setups.</p> <p>Let‚Äôs break it down from the basics and then get into how to actually implement and use them.</p>","path":["KUBERNETES","RESOURCES","Namespaces"],"tags":["Kubernetes"]},{"location":"kubernetes/resources/namespaces/#how-did-hierarchical-namespaces-come-into-the-picture","level":3,"title":"How Did Hierarchical Namespaces Come into the Picture?","text":"<p>Kubernetes namespaces were originally designed to divide cluster resources between different teams or environments. However, as organizations grew and their clusters became more complex, managing flat namespaces became increasingly challenging. Teams wanted more structure, such as grouping related namespaces or inheriting policies from one namespace to another. That‚Äôs where Hierarchical Namespaces came into the picture ‚Äî to simplify management by introducing parent-child relationships between namespaces.</p>","path":["KUBERNETES","RESOURCES","Namespaces"],"tags":["Kubernetes"]},{"location":"kubernetes/resources/namespaces/#example","level":3,"title":"Example:","text":"<p>Think of your laptop‚Äôs file system. You have folders (namespaces) and subfolders (hierarchical namespaces). Let‚Äôs say you have a main folder called ‚ÄúProjects,‚Äù and inside it, you have subfolders like ‚ÄúFrontend,‚Äù ‚ÄúBackend,‚Äù and ‚ÄúDevOps.‚Äù Each subfolder inherits the properties and permissions of the parent folder unless specified otherwise. That‚Äôs the gist of Hierarchical Namespaces!</p>","path":["KUBERNETES","RESOURCES","Namespaces"],"tags":["Kubernetes"]},{"location":"kubernetes/resources/namespaces/#why-use-hierarchical-namespaces","level":3,"title":"Why Use Hierarchical Namespaces?","text":"<ol> <li>Organization: When dealing with multi-tenant environments, you can group related namespaces under a single parent.</li> <li>Policy Inheritance: Apply policies at the parent level, and they get inherited by all child namespaces.</li> <li>Separation of Concerns: Maintain cleaner separation between different environments or applications.</li> <li>Efficient Resource Management: Group related resources under a single namespace tree.</li> </ol>","path":["KUBERNETES","RESOURCES","Namespaces"],"tags":["Kubernetes"]},{"location":"kubernetes/resources/namespaces/#creating-hierarchical-namespaces","level":3,"title":"Creating Hierarchical Namespaces","text":"<p>To create hierarchical namespaces, you‚Äôll need the Hierarchical Namespace Controller (HNC) installed in your cluster. Let‚Äôs walk through the basic setup:</p> <p>Step 1: Install HNC</p> <pre><code>kubectl apply -f https://github.com/kubernetes-sigs/multi-tenancy/releases/download/hnc-v0.9.0/hnc-manager.yaml\n</code></pre> <p>Step 2: Create a Parent Namespace</p> <pre><code>kubectl create namespace team-a\n</code></pre> <p>Step 3: Create a Child Namespace</p> <pre><code>kubectl hns create dev -n team-a\n</code></pre> <p>Step 4: Verify the Hierarchy</p> <pre><code>kubectl hns tree team-a\n</code></pre> <p>You should see an output similar to this:</p> <pre><code>team-a\n‚îî‚îÄ‚îÄ dev\n</code></pre> <p>Step 5: Inherit Roles and Policies</p> <p>Roles and policies applied to the parent namespace will automatically cascade down to child namespaces unless explicitly overridden.</p>","path":["KUBERNETES","RESOURCES","Namespaces"],"tags":["Kubernetes"]},{"location":"kubernetes/resources/namespaces/#real-world-use-case-multi-tenant-environment","level":3,"title":"Real-World Use Case: Multi-Tenant Environment","text":"<p>Imagine you‚Äôre managing a Kubernetes cluster for a SaaS product with multiple clients. Each client has different environments like dev, test, and prod. You can create a parent namespace for each client and have hierarchical namespaces for each environment under it. This way, policies applied to the client namespace will trickle down to all its environments, maintaining consistency without redundancy.</p>","path":["KUBERNETES","RESOURCES","Namespaces"],"tags":["Kubernetes"]},{"location":"kubernetes/resources/namespaces/#limitations-and-caveats","level":3,"title":"Limitations and Caveats","text":"<ul> <li>Complexity: Deeply nested namespaces can become hard to manage.</li> <li>Performance: Although minimal, there can be slight performance overhead with large hierarchy trees.</li> <li>Limited Support: Not every Kubernetes feature respects hierarchical namespace boundaries.</li> </ul>","path":["KUBERNETES","RESOURCES","Namespaces"],"tags":["Kubernetes"]},{"location":"kubernetes/resources/namespaces/#conclusion","level":3,"title":"Conclusion","text":"<p>Hierarchical Namespaces in Kubernetes are like a Russian nesting doll of resource management ‚Äî open one, and there‚Äôs another inside. They help organize complex environments efficiently and maintain a clear separation between tenants and environments. Just remember, with great power comes great responsibility (and potential complexity). So, plan your namespace hierarchy carefully!</p>","path":["KUBERNETES","RESOURCES","Namespaces"],"tags":["Kubernetes"]},{"location":"kubernetes/resources/pods/","level":1,"title":"Pods","text":"<p>These pods are like the tiny, essential building blocks that make the magic of container orchestration happen. If you‚Äôre new to Kubernetes or just curious, join us as we explore the significance of pods and how they seamlessly group containers in a way that makes managing applications feel like a breeze.</p>","path":["KUBERNETES","RESOURCES","Pods"],"tags":["Kubernetes"]},{"location":"kubernetes/resources/pods/#what-exactly-is-a-pod","level":2,"title":"What exactly is a pod?","text":"<p>At its core, a Kubernetes pod is like a cosy, shared flat for your containers. Think of it as a little universe where your containers live together. Now, why would you want containers to share space? Well, here‚Äôs the magic: Containers in the same pod can communicate with each other easily, almost like they‚Äôre in the same room having a conversation.</p> <p>In simpler terms, a pod is the smallest, most fundamental unit in Kubernetes. It‚Äôs the tiniest deployable package that can hold one or more containers. But why group containers together? Let‚Äôs find out!</p>","path":["KUBERNETES","RESOURCES","Pods"],"tags":["Kubernetes"]},{"location":"kubernetes/resources/pods/#why-pods-group-containers","level":2,"title":"Why Pods Group Containers","text":"<p>1. Buddies Stick Together: Imagine you‚Äôre moving into a new city and need a roommate to share expenses. In the Kubernetes world, containers are like your roommates, and pods are the flats you all share. When containers live together in a pod, they can chat and work together smoothly because they share the same network and storage space.</p> <p>2. Collaboration Made Easy: Sometimes, applications require multiple containers to work together, like a chef and a waiter teaming up to serve delicious meals. Pods make this collaboration a breeze. Containers within the same pod can share data and resources effortlessly.</p> <p>3. Isolation when Needed: Pods also offer a level of isolation. You can have containers in one pod that don‚Äôt need to interact with containers in another pod. This separation helps keep things tidy and organized.</p>","path":["KUBERNETES","RESOURCES","Pods"],"tags":["Kubernetes"]},{"location":"kubernetes/resources/pods/#things-inside-a-pod","level":2,"title":"Things inside a pod","text":"<p>Now, picture a pod as a little box that can hold one or more containers. These containers inside the pod share certain things:</p> <p>1. Network: Containers in a pod share the same IP address and port space. This means they can talk to each other using ‚Äúlocalhost,‚Äù just like you can chat with your roommate across the room.</p> <p>2. Storage: Pods can also share storage resources. So, if one container in a pod needs access to a file, another container in the same pod can provide it. They can share storage, like sharing a bookshelf.</p>","path":["KUBERNETES","RESOURCES","Pods"],"tags":["Kubernetes"]},{"location":"kubernetes/resources/pods/#need-for-multiple-containers-in-a-pod","level":2,"title":"Need for Multiple Containers in a Pod?","text":"<p>Good question! Here are some scenarios when you might want multiple containers in a pod:</p> <p>1. Sidecar Containers: Imagine a delivery truck that not only carries packages but also has a GPS tracker. In a pod, you might have your main application container (the delivery truck) and a sidecar container (the GPS tracker) that helps with tasks like logging or monitoring.</p> <p>2. Helper Containers: Sometimes, you require a little helper. In a pod, you can have your main container doing the heavy lifting and another container that helps with tasks like setting up configurations.</p>","path":["KUBERNETES","RESOURCES","Pods"],"tags":["Kubernetes"]},{"location":"kubernetes/resources/pods/#conclusion","level":2,"title":"Conclusion","text":"<p>In the enchanting world of Kubernetes, pods are the heartbeat of containerization. They bring containers together, fostering collaboration and communication while ensuring isolation when required. Think of pods as the cosy flats where your containers reside, working in harmony to deliver your applications flawlessly.</p> <p>So, the next time you see a pod in Kubernetes, remember that it‚Äôs like a tiny ecosystem, where containers are neighbours sharing resources and making your applications run like a well-orchestrated symphony.</p> <p>Welcome to this in-depth guide on Kubernetes Pods. In this article, we assume your application is already developed, built into Docker images, and hosted on a Docker repository (such as Docker Hub). We also assume that your Kubernetes cluster is configured and operational‚Äîwhether it is a single-node or multi-node cluster. With Kubernetes, the goal is to run containers on worker nodes, but rather than deploying containers directly, Kubernetes encapsulates them within an object called a pod. A pod represents a single instance of an application and is the smallest deployable unit in Kubernetes.</p> <p>In the simplest scenario, a single-node Kubernetes cluster may run one instance of your application inside a Docker container encapsulated by a pod.</p> <p>When user load increases, you can scale your application by spinning up additional instances‚Äîeach running in its own pod. This approach isolates each instance, allowing Kubernetes to distribute the pods across available nodes as needed.</p> <p>The image illustrates a Kubernetes cluster with a pod containing a Python application, showing user interaction and node structure.</p> <p>Instead of adding more containers to the same pod, additional pods are created. For instance, running two instances in separate pods allows the load to be shared across the node or even across multiple nodes if the demand escalates and additional cluster capacity is required.</p> <p>The image illustrates a Kubernetes cluster with multiple nodes, each containing pods running Python applications, and one pod marked with an error.</p> <p>Scaling Pods</p> <p>Remember, scaling an application in Kubernetes involves increasing or decreasing the number of pods, not the number of containers within a single pod.</p> <p>Typically, each pod hosts a single container running your main application. However, a pod can also contain multiple containers, which are usually complementary rather than redundant. For example, you might include a helper container alongside your main application container to support tasks like data processing or file uploads. Both containers in the pod share the same network namespace (allowing direct communication via localhost), storage volumes, and lifecycle events, ensuring they start and stop together.</p> <p>The image illustrates a Kubernetes multi-container pod setup, showing two containers within a pod on a node, labeled as \"Helper Containers.\"</p> <p>To better understand the concept, consider a basic Docker example. Suppose you initially deploy your application with a simple command:</p> <p>docker run python-app When the load increases, you may launch additional instances manually:</p> <p>docker run python-app docker run python-app docker run python-app docker run python-app Now, if your application needs a helper container that communicates with each instance, managing links, custom networks, and shared volumes manually becomes complex. You‚Äôd have to run commands like:</p> <p>docker run helper --link app1 docker run helper --link app2 docker run helper --link app3 docker run helper --link app4 With Kubernetes pods, these challenges are resolved automatically. When a pod is defined with multiple containers, they share storage, the network namespace, and lifecycle events‚Äîensuring seamless coordination and simplifying management.</p> <p>Even if your current application design uses one container per pod, Kubernetes enforces the pod abstraction. This design prepares your application for future scaling and architectural changes, even though multi-container pods remain less common. This article primarily focuses on single-container pods for clarity.</p>","path":["KUBERNETES","RESOURCES","Pods"],"tags":["Kubernetes"]},{"location":"kubernetes/resources/pods/#deploying-pods","level":2,"title":"Deploying Pods","text":"<p>A common method to deploy pods is using the kubectl run command. For example, the following command creates a pod that deploys an instance of the nginx Docker image, pulling it from a Docker repository:</p> <p><pre><code>kubectl run nginx --image nginx\n</code></pre> Once deployed, you can verify the pod's status with the kubectl get pods command. Initially, the pod might be in a \"ContainerCreating\" state, followed by a transition to the \"Running\" state as the application container becomes active. Below is an example session:</p> <pre><code>kubectl get pods\nNAME                   READY   STATUS    RESTARTS   AGE\nnginx-8586cf59-whssr   1/1     Running   0          8s\n</code></pre> <p>At this stage, note that external access to the nginx web server has not been configured. The service is accessible only within the node. In a future article, we will explore configuring external access through Kubernetes networking and services.</p> <p>Pods with YAML Welcome to this lesson on creating a Pod in Kubernetes using a YAML configuration file. In this guide, you'll learn how to structure your YAML file, create the Pod with kubectl, and verify its status. Kubernetes leverages YAML files to define objects such as Pods, ReplicaSets, Deployments, and Services. These definitions adhere to a consistent structure, with four essential top-level properties: apiVersion, kind, metadata, and spec.</p>","path":["KUBERNETES","RESOURCES","Pods"],"tags":["Kubernetes"]},{"location":"kubernetes/resources/pods/#top-level-fields-in-a-kubernetes-yaml-file","level":2,"title":"Top-Level Fields in a Kubernetes YAML File","text":"<p>Every Kubernetes definition file must include the following four fields:</p> <pre><code>apiVersion:\nkind:\nmetadata:\nspec:\n</code></pre> <ul> <li>apiVersion: This field indicates the version of the Kubernetes API you are using. For a Pod, set apiVersion: v1. Depending on the object you define, you might need different versions such as apps/v1, extensions/v1beta1, etc.</li> <li>kind: This specifies the type of object being created. In this lesson, since we're creating a Pod, you'll define it as kind: Pod. Other objects might include ReplicaSet, Deployment, or Service.</li> <li>metadata: The metadata section provides details about the object, including its name and labels. It is represented as a dictionary. It is essential to maintain consistent indentation for sibling keys to ensure proper YAML nesting. For example:</li> </ul> <p>apiVersion: v1 kind: Pod metadata:   name: myapp-pod   labels:     app: myapp spec: Indentation Reminder</p> <p>Make sure that the properties under metadata (like name and labels) are indented to the same level. This is crucial for correct YAML parsing.</p> <p>spec: The spec section provides specific configuration details for the object. For a Pod, this is where you define its containers. Since a Pod can run multiple containers, the containers field is an array. In our example, with a single container, the array has just one item. The dash (-) indicates a list item, and each container must be defined with at least name and image keys.</p> <p>Below is the complete YAML configuration for our Pod:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp-pod\n  labels:\n    app: myapp\n    type: front-end\nspec:\n  containers:\n    - name: nginx-container\n      image: nginx\n</code></pre> <p>Creating and Verifying the Pod After you have saved your configuration (for example, as pod-definition.yaml), use the following command to create the Pod:</p> <p>kubectl create -f pod-definition.yaml Once the Pod is created, you can verify its status by listing all Pods:</p> <p>kubectl get pods You should see output similar to this:</p> <p>NAME         READY   STATUS    RESTARTS   AGE myapp-pod    1/1     Running   0          20s To view detailed information about the Pod, run:</p> <p>kubectl describe pod myapp-pod This command provides extensive details, including metadata, node assignment, container specifics, and event history such as scheduling, volume mounting, and container start-up. Here is an example output:</p> <p>Name:         myapp-pod Namespace:    default Node:         minikube/192.168.99.100 Start Time:   Sat, 03 Mar 2018 14:26:14 +0800 Labels:       app=myapp Annotations:   Status:       Running IP:           172.17.0.24 Containers:   nginx:     Container ID:   docker://830bb56c8c42a860b4b70e9c1488faelbc38663e49186bc2f5a78e7688b8c9d     Image:          nginx     Image ID:       docker-pullable://nginx@sha256:4771d09578c7ca65299e110b3ee1c0a2592f5ea2618d32e4ffe7a4cab1c5de     Port:                State:          Running       Started:      Sat, 03 Mar 2018 14:26:21 +0800     Ready:          True     Restart Count:  0     Environment:         Mounts:       /var/run/secrets/kubernetes.io/serviceaccount from default-token-x95w7 (ro) <p>Conditions:   Type              Status   Initialized       True   Ready             True   PodScheduled      True</p> <p>Events:   Type    Reason                 Age   From               Message   ----    ------                 ----  ----               -------   Normal  Scheduled              27s   default-scheduler  Successfully assigned myapp-pod to minikube   Normal  SuccessfulMountVolume  27s   minikube           MountVolume.SetUp succeeded for volume \"default-token-x95w7\"   Normal  Pulling                27s   minikube           pulling image \"nginx\"   Normal  Pulled                 27s   minikube           Successfully pulled image \"nginx\"   Normal  Created                27s   minikube           Created container   Normal  Started                27s   minikube           Started container Tip</p> <p>Using kubectl describe helps you gain detailed insights into the internal state of your Pod, which can be invaluable for troubleshooting.</p> <p>Conclusion In this lesson, you learned how to structure a Kubernetes YAML file for a Pod, create it using kubectl, and verify its status. This hands-on approach equips you to manage and troubleshoot your Kubernetes resources effectively. Happy Kubernetes-ing!</p> <p>For more information, refer to the following resources:</p> <p>Demo Pods with YAML In this lesson, we will create a Kubernetes Pod using a YAML definition file instead of the \"kubectl run\" command. This method offers more control by allowing you to define pod specifications explicitly in a file. You can choose any text editor for this task; for instance, Windows users may prefer Notepad++ over Notepad, while Linux users might opt for vim. In future sections, we will explore additional IDEs and tools to streamline YAML editing, but we will stick with the basics for now.</p> <p>Step 1: Creating the YAML File Open your terminal and use vim to create a file named pod.yaml:</p> <p>vim pod.yaml Inside the file, define the following key elements:</p> <p>apiVersion: Should be set to v1 for a Pod. kind: Must be Pod (case-sensitive). metadata: A dictionary that includes the pod's name and any labels used for grouping. spec: Contains the pod specifications, including a list of containers. Note</p> <p>Be sure to follow proper indentation rules. Use two spaces per level (avoid tabs), as misalignment can lead to errors.</p> <p>Below is a complete example configuration for a single-container Pod using the nginx image:</p> <p>apiVersion: v1 kind: Pod metadata:   name: nginx   labels:     app: nginx     tier: frontend spec:   containers:     - name: nginx       image: nginx Tip</p> <p>To add additional containers, insert another block within the containers list with the appropriate name and image.</p> <p>Step 2: Saving and Verifying the YAML File After editing the file, exit vim and save your changes by typing:</p> <p>:wq Verify the contents of your YAML file with:</p> <p>cat pod.yaml The output should match the YAML configuration shown above.</p> <p>Step 3: Creating the Pod in the Cluster Create the Pod on your Kubernetes cluster using your YAML file. You can use either the kubectl create or kubectl apply command. Here‚Äôs an example with kubectl apply:</p> <p>kubectl apply -f pod.yaml</p>","path":["KUBERNETES","RESOURCES","Pods"],"tags":["Kubernetes"]},{"location":"kubernetes/resources/pods/#output","level":1,"title":"Output:","text":"","path":["KUBERNETES","RESOURCES","Pods"],"tags":["Kubernetes"]},{"location":"kubernetes/resources/pods/#podnginx-created","level":1,"title":"pod/nginx created","text":"<p>To check the status of your Pod, run:</p> <p>kubectl get pods Initially, you might see an output similar to this:</p> <p>NAME    READY   STATUS              RESTARTS   AGE nginx   0/1     ContainerCreating   0          7s After a short while, re-running the command should show the Pod in a running state:</p> <p>kubectl get pods Example output:</p> <p>NAME    READY   STATUS    RESTARTS   AGE nginx   1/1     Running   0          9s Step 4: Inspecting the Pod Details For a detailed overview of your Pod, use the kubectl describe command:</p> <p>kubectl describe pod nginx This command provides comprehensive details about the Pod, including container statuses, event logs, volumes, and node assignments. Below is an example of typical output:</p> <p>Initialized              True Ready                    True ContainersReady          True PodScheduled             True Volumes:   default-token-f5ntk:     Type:        Secret (a volume populated by a Secret)     SecretName:  default-token-f5ntk     Optional:    false QoS Class:   BestEffort Node-Selectors:  Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s              node.kubernetes.io/unreachable:NoExecute for 300s Events:   Type     Reason        Age   From                Message   ----     ------        ----  ----                -------   Normal   Scheduled     21s   default-scheduler   Successfully assigned default/nginx to minikube   Normal   Pulling       20s   kubelet, minikube   Pulling image \"nginx\"   Normal   Pulled        14s   kubelet, minikube   Successfully pulled image \"nginx\"   Normal   Created       14s   kubelet, minikube   Created container nginx   Normal   Started       14s   kubelet, minikube   Started container nginx Conclusion This demonstration has guided you through creating a Kubernetes Pod using a YAML configuration file. This approach not only reinforces good configuration practices but also provides enhanced flexibility compared to command-based object creation. In our next lesson, we will cover advanced IDEs and tools to further ease YAML file management.","path":["KUBERNETES","RESOURCES","Pods"],"tags":["Kubernetes"]},{"location":"kubernetes/resources/pods/probes/","level":1,"title":"Probes","text":"<p>Liveness, Readiness, and Startup Probes stand out as essential mechanisms for maintaining application health and optimizing service performance. This article delves into these probes, elucidating their functionalities, differences, and best practices for their implementation.</p> <p>Liveness, Readiness, and Startup Probes</p>","path":["KUBERNETES","RESOURCES","Pods","Probes"],"tags":["Kubernetes"]},{"location":"kubernetes/resources/pods/probes/#the-importance-of-probes-in-kubernetes","level":3,"title":"The Importance of Probes in Kubernetes","text":"<p>In a Kubernetes environment, applications are deployed in containers across multiple pods and nodes. This distributed nature can complicate monitoring and managing the health of applications. Probes are Kubernetes‚Äô solution to this challenge, providing a way to automatically check the health of an application and take necessary actions to maintain its reliability.</p>","path":["KUBERNETES","RESOURCES","Pods","Probes"],"tags":["Kubernetes"]},{"location":"kubernetes/resources/pods/probes/#liveness-probes-keeping-containers-alive","level":3,"title":"Liveness Probes: Keeping Containers Alive","text":"<p>Liveness probes determine if a container is running as expected. Kubernetes uses these probes to decide when to restart a container. For instance, if a liveness probe fails, indicating that an application is unresponsive or deadlocked, Kubernetes will restart the container, attempting to restore its normal operation. This automated intervention helps maintain service availability even in the face of software faults.</p> <p>Example: Consider a web server running inside a container, serving traffic on port 8080. A liveness probe could be configured to perform an HTTP GET request to <code>/healthz</code>, an endpoint that returns a 200 OK status code if the server is healthy. If this endpoint fails to respond correctly, the liveness probe fails, signaling Kubernetes to restart the container.</p> <pre><code>livenessProbe:\n httpGet:\n path: /healthz\n port: 8080\n initialDelaySeconds: 15\n periodSeconds: 10\n</code></pre>","path":["KUBERNETES","RESOURCES","Pods","Probes"],"tags":["Kubernetes"]},{"location":"kubernetes/resources/pods/probes/#readiness-probes-managing-traffic-flow","level":3,"title":"Readiness Probes: Managing Traffic Flow","text":"<p>Readiness probes assess whether a container is ready to serve traffic. Unlike liveness probes, if a readiness probe fails, Kubernetes will not kill the container; instead, it removes the pod from the list of service endpoints, preventing it from receiving traffic. This is particularly useful during startup or when an application is temporarily unable to serve traffic due to loading large data sets or performing other initialization tasks.</p> <p>Example: Imagine an application that needs to load a large configuration file or establish a database connection before it can start serving traffic. A readiness probe can be configured to check an endpoint, such as <code>/ready</code>, which only returns a 200 OK status code once the initialization process is complete.</p> <pre><code>readinessProbe:\n  httpGet:\n    path: /ready\n    port: 8080\n  initialDelaySeconds: 5\n  periodSeconds: 5\n</code></pre>","path":["KUBERNETES","RESOURCES","Pods","Probes"],"tags":["Kubernetes"]},{"location":"kubernetes/resources/pods/probes/#startup-probes-the-initial-health-check","level":3,"title":"Startup Probes: The Initial Health Check","text":"<p>Startup probes are designed to check the health of a container that might take a long time to start. They provide a way to delay liveness and readiness checks, giving an application sufficient time to initialize without being restarted by Kubernetes due to slow startup times. Once a startup probe succeeds for the first time, it is disabled, and liveness and/or readiness probes take over.</p> <p>Example: For an application that requires a significant amount of time to start, such as initializing a large dataset or waiting for external services, a startup probe can ensure that Kubernetes does not prematurely restart the container. This probe might execute a command inside the container that checks for the existence of a file indicating readiness.</p> <pre><code>startupProbe:\n exec:\n command:\n ‚Äî cat\n ‚Äî /app/is_ready\n initialDelaySeconds: 10\n periodSeconds: 20\n</code></pre>","path":["KUBERNETES","RESOURCES","Pods","Probes"],"tags":["Kubernetes"]},{"location":"kubernetes/resources/pods/probes/#best-practices-for-implementing-probes","level":3,"title":"Best Practices for Implementing Probes","text":"<ol> <li>Customize Probe Parameters: Adjust the <code>initialDelaySeconds</code>, <code>periodSeconds</code>, <code>timeoutSeconds</code>, <code>successThreshold</code>, and <code>failureThreshold</code> according to the specific needs of your application to avoid unnecessary restarts or downtime.</li> <li>Use Probes Judiciously: Not every container needs a liveness or startup probe. Use them when containers are prone to deadlocks or have lengthy startup times.</li> <li>Optimize Probe Effectiveness: Ensure that the checks performed by probes are reliable indicators of application health and readiness. Avoid heavy operations that could impact performance.</li> <li>Monitor and Adjust: Continuously monitor the effectiveness of your probes and adjust their configurations as needed. This is especially important as your application evolves.</li> <li>Leverage Probe Specifics: Each type of probe (HTTP, TCP, Exec) has its own benefits. Choose the one that best fits the health indicators of your application.</li> </ol>","path":["KUBERNETES","RESOURCES","Pods","Probes"],"tags":["Kubernetes"]},{"location":"kubernetes/resources/services/","level":1,"title":"Kubernetes Services","text":"<p>Navigating Kubernetes: Unveiling the Power of Services</p> <p>In our exploration of Kubernetes, we‚Äôve already discussed pods and how they provide the fundamental building blocks for running containerized applications. Now, it‚Äôs time to take the next step in our Kubernetes journey by introducing Kubernetes services. These services play a crucial role in enabling seamless communication between different parts of your application within a Kubernetes cluster. In this blog, we‚Äôll dive into what Kubernetes services are, explore their various types (Cluster IP Node Port, Load Balancer), and understand how they help maintain connectivity within your cluster.</p> <p>Kubernetes services ‚Äî Explained</p>","path":["KUBERNETES","RESOURCES","Services","Kubernetes Services"],"tags":["Kubernetes"]},{"location":"kubernetes/resources/services/#understanding-kubernetes-services","level":3,"title":"Understanding Kubernetes Services","text":"<p>Kubernetes' services are a vital component of Kubernetes networking, serving as the glue that connects various pods and enables communication between them. Imagine services as a bridge that allows your application‚Äôs components, spread across different pods, to talk to each other, all while maintaining scalability and flexibility.</p>","path":["KUBERNETES","RESOURCES","Services","Kubernetes Services"],"tags":["Kubernetes"]},{"location":"kubernetes/resources/services/#types-of-kubernetes-services","level":3,"title":"Types of Kubernetes Services","text":"<p>Kubernetes offers several types of services, each tailored to specific networking requirements. Let‚Äôs explore the three primary types:</p>","path":["KUBERNETES","RESOURCES","Services","Kubernetes Services"],"tags":["Kubernetes"]},{"location":"kubernetes/resources/services/#cluster-ip-service","level":3,"title":"Cluster IP Service","text":"<p>ClusterIP is the default service type in Kubernetes. It provides an internal IP address that is accessible only from within the cluster. This type is perfect for services that need to communicate exclusively within the Kubernetes cluster, like a database service that multiple application components rely on. ClusterIP services ensure that communication remains private and secure within the cluster.</p>","path":["KUBERNETES","RESOURCES","Services","Kubernetes Services"],"tags":["Kubernetes"]},{"location":"kubernetes/resources/services/#node-port-service","level":3,"title":"Node Port Service","text":"<p>NodePort services open up communication to the outside world. They allocate a specific port on every node in the cluster, allowing external traffic to reach the service. NodePort services are often used when you need to expose your application to external clients, such as web applications or APIs. While they provide external access, NodePort services maintain the ability to route traffic internally within the cluster.</p>","path":["KUBERNETES","RESOURCES","Services","Kubernetes Services"],"tags":["Kubernetes"]},{"location":"kubernetes/resources/services/#load-balancer-service","level":3,"title":"Load Balancer Service","text":"<p>Load Balancer services are a powerful way to distribute incoming traffic across multiple pods. They work hand-in-hand with cloud providers to create and configure load balancers that evenly distribute external traffic to the pods in your service. This type of service is especially useful for applications that need high availability and scalability, like a web application that experiences varying levels of traffic throughout the day.</p>","path":["KUBERNETES","RESOURCES","Services","Kubernetes Services"],"tags":["Kubernetes"]},{"location":"kubernetes/resources/services/#how-kubernetes-services-maintain-connectivity","level":3,"title":"How Kubernetes Services Maintain Connectivity","text":"<p>Now that we understand the types, let‚Äôs delve into how Kubernetes services help maintain connectivity within your cluster:</p>","path":["KUBERNETES","RESOURCES","Services","Kubernetes Services"],"tags":["Kubernetes"]},{"location":"kubernetes/resources/services/#labels-and-selectors","level":3,"title":"Labels and Selectors","text":"<p>Kubernetes services rely on labels and selectors to determine which pods to route traffic to. Labels are key-value pairs that you attach to pods, and selectors are rules that services use to identify the pods they should target. This labelling system provides flexibility and allows services to dynamically discover pods as they are created or terminated.</p>","path":["KUBERNETES","RESOURCES","Services","Kubernetes Services"],"tags":["Kubernetes"]},{"location":"kubernetes/resources/services/#service-discovery","level":3,"title":"Service Discovery","text":"<p>Services maintain a DNS record within the cluster, making it easy for pods to discover and communicate with each other. This DNS record allows you to reference services by their names rather than hardcoding IP addresses, simplifying communication and ensuring that it remains robust as your application scales.</p>","path":["KUBERNETES","RESOURCES","Services","Kubernetes Services"],"tags":["Kubernetes"]},{"location":"kubernetes/resources/services/#load-balancing","level":3,"title":"Load Balancing","text":"<p>Load Balancer services distribute incoming traffic across multiple pods, ensuring high availability and reliability. By evenly distributing the load, they prevent any one pod from becoming a bottleneck and help your application handle increased traffic seamlessly.</p> <p>Other than the above-mentioned there are few other services like headless service, External name service etc‚Ä¶ We will write detailed articles about defining a service and how to access a service in our later posts‚Ä¶</p> <p>As you continue your Kubernetes journey, remember that services are the key to building robust, scalable, and resilient containerized applications.</p>","path":["KUBERNETES","RESOURCES","Services","Kubernetes Services"],"tags":["Kubernetes"]},{"location":"kubernetes/resources/services/clusterIP/","level":1,"title":"Cluster IP allocation","text":"<p>In our journey learning networking in Kubernetes, today we are going to learn about cluster IP allocation. Here in this blog we‚Äôll see what it is, how cluster IPs were assigned to services and their significance.</p> <p>Kubernetes cluster IP allocation</p>","path":["KUBERNETES","RESOURCES","Services","Cluster IP allocation"],"tags":["Kubernetes","Services","ClusterIP"]},{"location":"kubernetes/resources/services/clusterIP/#what-is-a-cluster-ip","level":2,"title":"What Is a Cluster IP?","text":"<p>A Cluster IP is a type of service in Kubernetes designed for internal communication. It provides a stable, virtual IP address that routes traffic to one or more pods that belong to a specific service. In simpler terms, think of it as a front door to a group of pods that work together to deliver a specific service or function within your cluster.</p> <p>Cluster IPs are excellent for services that need to talk to each other internally, like a web application connecting to a database or microservices communicating with each other. They are accessible only within the Kubernetes cluster, making them a secure and reliable choice for internal network traffic.</p>","path":["KUBERNETES","RESOURCES","Services","Cluster IP allocation"],"tags":["Kubernetes","Services","ClusterIP"]},{"location":"kubernetes/resources/services/clusterIP/#how-are-cluster-ips-assigned-to-services","level":2,"title":"How Are Cluster IPs Assigned to Services?","text":"<p>Cluster IPs are assigned to services through Kubernetes service objects. Let‚Äôs break down the steps to understand how this works:</p> <ol> <li>Create a Service Object: To assign a Cluster IP to a service, you create a Kubernetes Service object. This object includes the service type (in this case, Cluster IP), a selector, and a list of pods that the service should target. Here‚Äôs an example in YAML: <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\nspec:\n  selector:\n    app: my-app\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 80\n</code></pre></li> </ol> <p>In this example, we‚Äôre creating a Cluster IP service called ‚Äúmy-service‚Äù that routes traffic to pods with the label ‚Äúapp: my-app‚Äù on port 80.</p> <ol> <li>Kubernetes Controller: Once you create the service object, Kubernetes takes over. A Kubernetes controller watches for new services and ensures that the Cluster IP is properly assigned.</li> <li>IP Allocation: The Kubernetes controller assigns a Cluster IP to the service, and it becomes accessible through that IP within the cluster.</li> <li>Service Discovery: Other parts of your application can now discover and connect to the service using the Cluster IP. For example, if another pod wants to communicate with ‚Äúmy-service,‚Äù it simply makes a request to the Cluster IP (e.g., <code>http://my-service</code>) without needing to know the individual pod IP addresses.</li> </ol>","path":["KUBERNETES","RESOURCES","Services","Cluster IP allocation"],"tags":["Kubernetes","Services","ClusterIP"]},{"location":"kubernetes/resources/services/clusterIP/#significance-of-cluster-ips-in-managing-internal-communication","level":2,"title":"Significance of Cluster IPs in Managing Internal Communication","text":"<p>Cluster IPs play a crucial role in managing internal communication within a Kubernetes cluster for several reasons:</p> <ol> <li>Stability: Cluster IPs provide a stable point of contact for services. Even if the pods they route traffic to change or scale, the Cluster IP remains consistent. This stability is essential for ensuring your application can rely on consistent communication.</li> <li>Security: By design, Cluster IPs are accessible only within the Kubernetes cluster. This helps isolate and secure your internal communication from external access. It‚Äôs an effective way to prevent unauthorized external access to your services.</li> <li>Load Balancing: When multiple pods are part of a service, the Cluster IP automatically load-balances traffic among them. This load balancing ensures efficient distribution of requests and optimal resource utilization.</li> <li>Service Discovery: With Cluster IPs, you don‚Äôt need to keep track of individual pod IP addresses or worry about how many pods are running. Service discovery is simplified, as you can access services by their Cluster IP, abstracting the complexity of pod management.</li> </ol> <p>In conclusion, Cluster IPs offer stability, security, load balancing, and simplified service discovery. By creating and using Cluster IPs through Kubernetes service objects, you can enhance the reliability and security of your containerized applications.</p>","path":["KUBERNETES","RESOURCES","Services","Cluster IP allocation"],"tags":["Kubernetes","Services","ClusterIP"]},{"location":"kubernetes/yaml/annotations/","level":1,"title":"Annotations","text":"<p>Welcome to Kubernetes adventure series, In this blog I came up with a feature that often flies under the radar, yet holds substantial power in customizing and managing Kubernetes resources, is annotations. This blog post is designed to unravel the complexities of Kubernetes annotations, with a comprehensive understanding of their purpose, use cases, and best practices.</p>","path":["KUBERNETES","YAML","Annotations"],"tags":["Kubernetes"]},{"location":"kubernetes/yaml/annotations/#understanding-kubernetes-annotations","level":3,"title":"Understanding Kubernetes Annotations","text":"<p>Imagine you have a bunch of containers and services running in Kubernetes. You want to keep track of extra details about them, like notes or reminders, that don‚Äôt fit into their basic descriptions. That‚Äôs where annotations come in. They‚Äôre like labels but more detailed. You can use them to add all sorts of information to your Kubernetes objects that don‚Äôt directly affect how things run.</p>","path":["KUBERNETES","YAML","Annotations"],"tags":["Kubernetes"]},{"location":"kubernetes/yaml/annotations/#the-anatomy-of-annotations","level":2,"title":"The Anatomy of Annotations","text":"<p>Annotations, like labels, are key-value pairs attached to Kubernetes objects. They are defined in the metadata section of object definitions, allowing you to store additional information that can be leveraged by tools and libraries working with these objects. Here is a simple example of an annotation within a Pod manifest:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: example-pod\n  annotations:\n    example.com/log-level: \"debug\"\n    example.com/update-policy: \"auto\"\nspec:\n  containers:\n  - name: example-container\n    image: nginx\n</code></pre> <p>In this example, two annotations are defined: <code>example.com/log-level</code> and <code>example.com/update-policy</code>. These annotations can be used to convey configuration information to utilities that process Pods but are not directly involved in their creation or management.</p>","path":["KUBERNETES","YAML","Annotations"],"tags":["Kubernetes"]},{"location":"kubernetes/yaml/annotations/#why-use-annotations","level":2,"title":"Why Use Annotations?","text":"<p>Annotations provide a mechanism for storing additional information about Kubernetes objects, enabling a variety of use cases:</p> <ul> <li>Storing build/release information: Annotations can hold information such as build/release IDs, PR numbers, git branch, docker image hashes, etc., that can be useful for debugging and tracing.</li> <li>Configuration and management: Tools and controllers can use annotations to manage and configure objects in a more flexible way than labels.</li> <li>Signaling between tools: Annotations can act as a signal between different tools controlling the same object, conveying information like whether an object should be auto-scaled, whether a service requires an external load balancer, etc.</li> </ul>","path":["KUBERNETES","YAML","Annotations"],"tags":["Kubernetes"]},{"location":"kubernetes/yaml/annotations/#use-cases-and-examples","level":3,"title":"Use Cases and Examples","text":"<p>Let‚Äôs explore some practical use cases where annotations can be particularly useful:</p>","path":["KUBERNETES","YAML","Annotations"],"tags":["Kubernetes"]},{"location":"kubernetes/yaml/annotations/#1-tracking-deployment-information","level":3,"title":"1. Tracking Deployment Information","text":"<p>Annotations are ideal for tracking information like deployment version, repository URL, or commit hashes. This can be invaluable for debugging purposes and understanding the provenance of deployed resources.</p>","path":["KUBERNETES","YAML","Annotations"],"tags":["Kubernetes"]},{"location":"kubernetes/yaml/annotations/#2-signaling-to-ingress-controllers","level":3,"title":"2. Signaling to Ingress Controllers","text":"<p>Ingress controllers can use annotations to customize behavior. For example, you might use an annotation to specify a custom rate limit or to enable client certificate authentication for a specific Ingress resource.</p>","path":["KUBERNETES","YAML","Annotations"],"tags":["Kubernetes"]},{"location":"kubernetes/yaml/annotations/#3-configuring-pod-behavior","level":3,"title":"3. Configuring Pod Behavior","text":"<p>Annotations can influence the behavior of Pods or their containers. For instance, you could use annotations to specify a sidecar container should be injected into a Pod, with the annotation holding configuration data for the sidecar.</p>","path":["KUBERNETES","YAML","Annotations"],"tags":["Kubernetes"]},{"location":"kubernetes/yaml/annotations/#best-practices-for-using-annotations","level":3,"title":"Best Practices for Using Annotations","text":"<p>While annotations offer great flexibility, it‚Äôs important to use them judiciously. Here are some best practices to keep in mind:</p> <ul> <li>Use domain-prefixed names: To avoid conflicts, use domain-prefixed names (e.g., <code>example.com/my-annotation</code>) for your annotations.</li> <li>Keep them small: Annotations are not designed to hold large amounts of data. If you find yourself needing to store large blobs of information, consider using a ConfigMap or another storage mechanism.</li> <li>Document your annotations: Since annotations can be used freely, it‚Äôs crucial to document the annotations you use, their purposes, and their expected values. This is especially important for annotations used by tools and applications across your team or organization.</li> <li>Use them for non-identifying information: Remember, annotations are not meant to be used for identifying and selecting Kubernetes objects. Use labels for that purpose.</li> </ul> <p>In conclusion, while Kubernetes labels might get the spotlight for their role in organizing and selecting resources, annotations work quietly in the background, offering a flexible mechanism for storing metadata, conveying configuration, and enabling integrations. By understanding and leveraging annotations wisely, you can unlock new possibilities for managing your Kubernetes resources more effectively and efficiently.</p> <p>I'll help you sail through the ocean of Kubernetes with minimal efforts</p>","path":["KUBERNETES","YAML","Annotations"],"tags":["Kubernetes"]},{"location":"linux/debian/zsh/","level":1,"title":"Index","text":"","path":["Index"],"tags":[]},{"location":"linux/debian/zsh/#configuration","level":2,"title":"Configuration","text":"<p>Zshrc</p> .zshrc<pre><code># PATH\neval \"$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)\"\nexport PATH=\"/home/linuxbrew/.linuxbrew/bin:/usr/local/bin:/usr/bin:/bin\"\nsource $HOMEBREW_PREFIX/opt/zinit/zinit.zsh\n#source \"$(brew --prefix)\"/opt/fzf/shell/key-bindings.zsh\n#source \"$(brew --prefix)\"/opt/fzf/shell/completion.zsh\n\n# BIND\nbindkey -e                       # Use Emacs-style keybindings in the shell\nbindkey \"^[[3~\" delete-char      # Map Delete key to delete-char\nbindkey \"^[[H\" beginning-of-line # Map Home key to move to beginning of line\nbindkey \"^[[F\" end-of-line       # Map End key to move to end of line\n\n# OPTIONS\nsetopt AUTO_CD                   # Allow changing directories by typing the directory name without 'cd'\nsetopt INTERACTIVE_COMMENTS      # Enable comments in interactive shell commands\nsetopt HIST_IGNORE_DUPS          # Ignore duplicate commands in history\nsetopt HIST_IGNORE_SPACE         # Ignore commands starting with a space\nsetopt INC_APPEND_HISTORY        # Append commands to history immediately, not at shell exit\nsetopt SHARE_HISTORY             # Share command history across all sessions\nsetopt APPEND_HISTORY            # Append to the history file rather than overwrite it\nsetopt BANG_HIST                 # Enable history expansion using '!' syntax\nsetopt EXTENDED_HISTORY          # Save timestamps and durations in history file\nsetopt HIST_REDUCE_BLANKS        # Remove superfluous blanks before saving to history\nsetopt HIST_VERIFY               # Show history-expanded command before running it\n\n# OTHER\nHISTFILE=~/.histfile\nHISTSIZE=1000\nSAVEHIST=100000\n\n# ALIASES\nalias kubectl=\"kubecolor\"\nalias szshrc=\"source ~/.zshrc\"\nalias zshrc=\"vim ~/.zshrc\"\n\n# COMPLETION\nautoload -Uz compinit           # Disable if zsh-autocomplete\ncompinit                        # Disable if zsh-autocomplete\ncompdef kubecolor=kubectl\n\n# ZSTYLE\nzstyle ':fzf-tab:complete:cd:*' fzf-preview 'eza -1 --color=always $realpath'\n\n# ZINIT PLUGINS\nzinit ice wait\"0\" lucid\nzinit load Aloxaf/fzf-tab\n#zinit light marlonrichert/zsh-autocomplete\nzinit ice wait lucid\nzinit load zdharma-continuum/fast-syntax-highlighting\nzinit ice wait lucid\nzinit load zsh-users/zsh-autosuggestions\nzinit ice depth=1; zinit light romkatv/powerlevel10k\nzinit light olets/zsh-abbr\nzinit light hlissner/zsh-autopair\nzinit light unixorn/fzf-zsh-plugin\n\n# CONFIG ###################################################################################################\n\n## ZELLIJ\nif [[ -z \"$ZELLIJ\" ]]; then\n    zellij attach -c MATHOD\nfi\n\n# POWERLEVEL10K\n[[ ! -f ~/.p10k.zsh ]] || source ~/.p10k.zsh\n\n# ABBR\nABBR_EXPAND_PUSH_ABBREVIATION_TO_HISTORY=0\nABBR_SET_EXPANSION_CURSOR=1\nABBR_EXPANSION_CURSOR_MARKER=%\nABBR_SET_LINE_CURSOR=1\nABBR_GET_AVAILABLE_ABBREVIATION=1\nABBR_LOG_AVAILABLE_ABBREVIATION=0\n\n# FUNCTION\nautoload -U add-zsh-hook\nadd-zsh-hook preexec my_abbreviation_reminder\n\nmy_abbreviation_reminder() {\n  if [[ -n \"$ABBR_UNUSED_ABBREVIATION\" ]]; then\n    # Set color codes\n    local green=\"\\e[32m\"\n    local red=\"\\e[31m\"\n    local reset=\"\\e[0m\"  # Reset color back to normal\n\n    # Display the custom reminder with proper colors\n    echo -e \"üí° Reminder: You could have used the abbreviation '${green}$ABBR_UNUSED_ABBREVIATION${reset}' for '${red}$ABBR_UNUSED_ABBREVIATION_EXPANSION${reset}'.\"\n  fi\n}\n\nif [[ -z \"$DISABLE_AUTO_EZA\" ]]; then\n  function auto_eza_on_cd() {\n    eza --icons --group-directories-first --git\n  }\n\n  # Override the default cd function to run auto_eza_on_cd\n  autoload -Uz add-zsh-hook\n  add-zsh-hook chpwd auto_eza_on_cd\nfi\n</code></pre>","path":["Index"],"tags":[]},{"location":"linux/debian/zsh/#plugins","level":2,"title":"Plugins","text":"<ul> <li>https://github.com/romkatv/powerlevel10k</li> <li>https://github.com/olets/zsh-abbr</li> <li>https://github.com/hlissner/zsh-autopair</li> <li>https://github.com/zdharma-continuum/fast-syntax-highlighting</li> <li>https://github.com/Aloxaf/fzf-tab</li> <li>https://github.com/marlonrichert/zsh-autocomplete</li> <li>https://github.com/zsh-users/zsh-autosuggestions</li> <li>https://github.com/unixorn/fzf-zsh-plugin</li> <li>https://github.com/junegunn/fzf</li> </ul> <p>Documentation officielle</p> <p>https://zsh.sourceforge.io/Doc/Release/index.html</p>","path":["Index"],"tags":[]},{"location":"rust/","level":1,"title":"Home","text":"<p>Course Introduction:</p> <ul> <li>Introduction</li> </ul> <p>Getting Started with Rust:</p> <ul> <li>Introduction to Rust</li> <li>Overview of Rusts Key Features</li> <li>Setting up the Development Environment</li> <li>Introduction to Cargo</li> </ul> <p>Rust Basics:</p> <ul> <li>Writing and Running Your First Rust program</li> <li>Comments</li> <li>Variables and Mutability</li> <li>Scalar and Compound Data Types</li> <li>Constants</li> <li>Shadows</li> <li>Macros</li> <li>Control Flow if else if else</li> <li>Control Flow loop and labels</li> <li>Control Flow for and while</li> <li>Control Flow Pattern matching with match</li> </ul> <p>Functions:</p> <ul> <li>What are Functions</li> <li>Parameters</li> <li>Return Values</li> <li>Statements and Expressions</li> </ul> <p>Ownership:</p> <ul> <li>Introduction</li> <li>Stack and Heap</li> <li>The String Type and Ownership</li> <li>Scope and Ownership</li> <li>Variables and Data Interacting with Move</li> <li>Variables and Data Interacting with Clone</li> <li>Ownership and Functions</li> <li>References and Borrowing</li> <li>Immutable and Mutable References</li> <li>Rules of References</li> <li>The Slice Type</li> <li>Rules for Slices</li> </ul> <p>Collections Error Handling:</p> <ul> <li>Structs</li> <li>Methods</li> <li>Enums</li> <li>Vectors</li> <li>Hashmaps</li> <li>Unrecoverable Errors</li> <li>Recoverable Errors</li> <li>Creating Custom Error Types</li> <li>Logging Libraries</li> <li>Logging Best Practices</li> </ul> <p>Packages Modules and Crates:</p> <ul> <li>Introduction to Crates</li> <li>Introduction to Packages</li> <li>Introduction to Modules</li> <li>Bringing Paths into Scope</li> <li>Separating Modules into Files</li> </ul> <p>Building Command Line Tools:</p> <ul> <li>Introduction to CLI Tools and Importance in DevOps</li> <li>Parsing command line args with clap</li> <li>Project Manage Docker Containers using Docker Clients in Rust</li> </ul> <p>Debugging in Rust:</p> <ul> <li>Debugging Tools</li> <li>Using println and logging</li> <li>Rust specific tools rust gdb rust lldb</li> </ul> <p>Advanced Rust Concepts:</p> <ul> <li>Generic Types</li> <li>Traits Shared Behavior</li> <li>Validating References with Lifetimes</li> <li>Introduction to Smart Pointers</li> <li>Box Single Ownership and Heap Allocation</li> <li>Rc Reference Counting and Shared Ownership</li> <li>RefCell and Interior Mutability</li> <li>Weak Breaking Reference Cycles</li> <li>Implementing Your Own Smart Pointer</li> </ul> <p>Closures and Iterators:</p> <ul> <li>Closures</li> <li>Rusts Iterator Ecosystem Custom Iterators</li> </ul> <p>Fearless Concurrency:</p> <ul> <li>Threads</li> <li>Message passing</li> <li>Shared state concurrency Mutex Arc</li> <li>Send and Sync traits</li> </ul> <p>Asynchronous Programming:</p> <ul> <li>Concurrent Programming</li> <li>Async Await and Futures</li> <li>Building a Concurrent URL Pinger</li> </ul> <p>Testing Continuous Integration:</p> <ul> <li>Introduction to Testing in Rust</li> <li>Organizing Tests in Rust</li> <li>Mocking Dependencies in Rust</li> <li>Managing and Running Tests in Rust</li> <li>Building and Deploying with CICD</li> </ul> <p>Advanced Features:</p> <ul> <li>Advanced Traits</li> <li>Default Generic Type Parameters</li> <li>Fully qualified syntax</li> <li>Advanced Types</li> <li>Advanced Functions and Closures</li> </ul> <p>Network Programming and File Handling:</p> <ul> <li>Introduction to Network Programming in Rust</li> <li>Building TCP Clients and Servers</li> <li>Building UDP Clients and Servers</li> <li>Introduction to File Handling in Rust</li> <li>Performing Common Filesystem Operations</li> <li>Project Networked File Transfer Application</li> </ul> <p>WebAssembly with Rust:</p> <ul> <li>Introduction to WebAssembly and Why Rust</li> <li>Setting Up Your Rust and WebAssembly Development Environment</li> <li>Interacting with WebAssembly from JavaScript</li> <li>Accessing WebAssembly Memory from JavaScript</li> <li>Building a Mini Project Log Analyzer</li> </ul> <p>Final Capstone Project:</p> <ul> <li>Capstone Project</li> </ul>","path":["RUST","Home"],"tags":["Rust"]},{"location":"rust/project/DevCardBattle/","level":1,"title":"Dev Card Battle","text":"<p>Phase d‚Äôune partie:Dans ce jeu pas de tours mais des jours ! D√©but de partie piocher 10 cartesPremi√®re phase pioche une carte resource ou incidentDeuxieme phase utiliser ces d√©ployment pour utiliser les resource ou incident acquis Derni√®re phase fin de phase au tours de l‚Äôadversaire</p> <p></p> <p></p> <p></p> <p></p>","path":["RUST","Dev Card Battle"],"tags":[]}]}